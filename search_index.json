[
["index.html", "Community contributions for EDAV Fall 2019 Chapter 1 Instructions 1.1 Background 1.2 Preparing your .Rmd file 1.3 Submission steps 1.4 Optional tweaks 1.5 FAQ", " Community contributions for EDAV Fall 2019 2019-12-13 Chapter 1 Instructions This chapter gives you all the information you need to upload your community contribution. Please read this entire document carefully before making your submission. Of particular note is the fact that bookdown requires a different .Rmd format than you’re used to, so you must make changes to the beginning of the file as described below before submitting. 1.1 Background This web site makes use of the bookdown package to render a collection of .Rmd files into a nicely formatted online book with chapters and subchapters. Your job will be to submit a slightly modified version of your community contribution .Rmd file to the GitHub repository where the source files for this web site are stored. On the backend, the admins will divide the chapters into book sections and order them. We use Travis CI to render the book and push the rendered .html files to our gh-pages branch–you can view our builds here–and GitHub Pages to host the site. (If you’re thinking about using CI, you may want to consider GitHub Actions instead. It was released in November 2019 and is getting a lot of attention.) If your community contribution is in a different format, then create a short .Rmd file that explains what you did, and includes links to any relevant files, such as slides, etc. which you can post on your GitHub repo (or another online site.) 1.2 Preparing your .Rmd file You should only submit ONE Rmd file. After completing these modifications, your .Rmd should look like this sample bookdown .Rmd. Create a concise, descriptive name for your project. For instance, name it base_r_ggplot_graph or something similar if your work is about contrasting/working with base R graphics and ggplot2 graphics. Check the .Rmd filenames in the project repo to make sure your name isn’t already taken. Your project name should be words only and joined with underscores, i.e. Do not include whitespace in the name. Create a copy of your .Rmd file with the new name. Completely delete the YAML header (the section at the top of the .Rmd that includes name, title, date, output, etc.) including the --- line. Choose a short, descriptive, human readable title for your project as your title will show up in the table of contents – look at examples in the rendered book. Capitalize the first letter only (“sentence case”). On the first line of your document, enter a single hashtag, followed by a single whitespace, and then your title. It is important to follow this format so that bookdown renders your title as a header. Do not use single # headers anywhere else in the document. The second line should be blank, followed by your name(s): # Base R vs. ggplot2 Aaron Burr and Alexander Hamilton Your content starts here. If your project requires data, please use a built-in dataset or read directly from a URL, such as: df &lt;- readr::read_csv(&quot;https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv&quot;) If you absolutely must include a data file, please use a small one, as for many reasons it is desirable to keep the repository size as small as possible. If you have included a setup chunk in your Rmd file, please remember to remove the label setup in the chunk, ie., use : {r, include=FALSE} instead of {r setup, include=FALSE} Want to get fancy? See the optional tweaks section below. 1.3 Submission steps To submit your work, we will be following the instructions in this tutorial, which are provided in abbreviated form below, with specific instructions on naming conventions, content information, and other important details. Fork cc19 repo (this repo) to your GitHub account. Clone/download the forked repo to your local computer. Create a new branch and name it with your project name, in our case sample_project. If you forget to do so, check this tutorial to fix. Copy your modified .Rmd file with the same name into the root directory on the branch. In our example, it is sample_project.Rmd. Do not include an .html file. (In order for the bookdown package to work, all .Rmd files will be rendered behind the scenes.) [OPTIONAL] If you have other resources (such as images) included in your project, create a folder under resources/. In our example, it is resources/sample_project/. Put the resources files there. When you are ready to submit your project, push your branch to your remote repo. Follow this tutorial to create a pull request. If you follow the steps, we will merge it to the master branch. After submitting your pull request, do not be concerned if you see an “All builds have failed” message from Travis CI. There are things that need to be done on the backend, such as adding the libraries you use to the project for the Travis CI build to pass. 1.4 Optional tweaks If you prefer for links from your chapter to open in new tabs, add {target=&quot;_blank&quot;} after the link, such as: [edav.info](edav.info){target=&quot;_blank&quot;} Note that your headers (##, ###, etc.) will be converted to numbered headings as such: ## –&gt; 3.1 ### –&gt; 3.1.1 These headings will appear as chapter subheadings and sub-subheadings in the navigation panel on the left. Think about a logical structure for users to navigate your chapter. We recommend using only ## and ### headings as subheadings such as 4.1.3.4 are generally not necessary and look messy. Unfortunately, there’s no simple way to preview your chapter before it’s actually merged into the project. (bookdown has preview_chapter() option but it only works after the entire book has been rendered at least once and that will become more and more complex and require more and more packages as the project grows.) If you really want to preview it, fork and clone this minimal bookdown repo, add your .Rmd file, click the “Build book” button on the Build tab (next to Git), and then open any of the .html files in the _book folder in a web browser to see the rendered book. (Do not click the Knit button as it will not build a bookdown book.) If you’re interested in more bookdown options, see the official reference book. Have more useful tweaks to share? Submit an issue or PR. 1.5 FAQ 1.5.1 What should I expect after creating a pull request? Within a week after you create a pull request, we will apply a label to it and assign an administrater who will review all the files you submit to see if they meet the requirements. It will take some time before we can process all the pull requests, so as long as you see your pull request has been labeled and assigned to an administrater, don’t worry. However, if the admin contacts you regarding the pull request, that usually means your files fail to meet some requirements. The admin will clearly state what is wrong, so please fix them as soon as possible. 1.5.2 What if I catch mistakes after my pull request is merged? You may submit additional pull requests to fix material on the site. If the edits are small, such as fixing typos, it is easiest to make the edits directly on GitHub, following these instructions. We will merge first pull requests before edits, so please be patient. 1.5.3 Other questions If you encounter other problems, please submit an issue and we will look into it. Thank you for your contributions! "],
["sample-project.html", "Chapter 2 Sample project", " Chapter 2 Sample project Nancy Pelosi and Donald Trump This chapter gives a sample layout of your Rmd file. Test Photo "],
["basic-r.html", "Chapter 3 Basic R 3.1 Data types 3.2 data structure", " Chapter 3 Basic R Xin Guo, Aiden Zhang R is a tool for data processing, it is important for us to understand the basic operations. In this cheatsheat/tutorial we will cover some basic operations of data. We will start with the building blocks of data types then go to the data structures. We try to cover the create, update(delete included), select operations for each data object following the logic of basic database operations. 3.1 Data types For different data types, the attributes of an object is of most interest, we would use the following functions to explore objects. We will classify them into two types, one is the read-only functions, and the others are writable functions. If we want to change the attributes of an object, we might use some functions to implement, and we will also cover these operations. 3.1.1 (1) character temp_char = &quot;hellow world&quot; Let us explore the attributes of the character. The following are some of the general “read-only” functions. summary(temp_char) ## Length Class Mode ## 1 character character str function returns some information, but it is less detailed than the summary. str(temp_char) ## chr &quot;hellow world&quot; summary and str are efficient functions to know about an object, but it is worth noticed that function attributes does not give us information about the variable. attributes(temp_char) ## NULL Then let us explore more about the “read-only” functions and get to know the functions that can change their return values. length function will return the length of the object. length(temp_char) ## [1] 1 We can change the length of the object by appending another character to it. If we want append a number to it, the number will be converted into a character to be consistent. long_temp = append(temp_char,&quot;hahaha&quot;) longer_test = append(temp_char,1) length(longer_test) ## [1] 2 unique function returns the unique element in the object. unique(temp_char) ## [1] &quot;hellow world&quot; For each unique element in the object, table function returns their frequencies. range function returns the minimum and maximum value. table(temp_char) ## temp_char ## hellow world ## 1 range(temp_char) ## [1] &quot;hellow world&quot; &quot;hellow world&quot; We can change the value of the character by using the substr function. If we want to add more value to the end, we can use the paste function.It will return a longer size character object. If we add a number using paste, like the append function, it will convert the number into character. This is useful when we have a different data to print out and the print function cannot support multiple items. substr(temp_char,6,7) = &quot;? &quot; a = 2019 paste(temp_char,&quot;!&quot;,a,sep =&quot;&quot;) ## [1] &quot;hello? world!2019&quot; We can use the class function to find out the data type of this object. class(temp_char) ## [1] &quot;character&quot; If we want to change the class of this object, we would normally use the as function. We might try to turn a number into character sometimes. # To turn a number into character temp_num = 1 class(temp_num) ## [1] &quot;numeric&quot; temp_num_new = as.character(temp_num) class(temp_num_new) ## [1] &quot;character&quot; We should pay attention to that the as function does not modify the parameter passed into it. So we will have a new object. Now, let us look at some writable attributes of the object. Levels is the order for the items in the object, that is why a character object will not have order. We can set the levels though it does not have any meaning for this certain character. It will not change the data type of the object. levels(temp_char) ## NULL levels(temp_char) = 1 levels(temp_char) ## [1] 1 class(temp_char) ## [1] &quot;character&quot; names function will return the name of an object if it has, and we can assign value to it to name the object. The dimension and the length of the names we want to assign must match the dimension and length of the object, otherwise it will return error. names(temp_char) ## NULL names(temp_char) = c(&#39;1&#39;) Other attributes related to names are rownames, colnames and dimnames, since this object does not have dimension, so it will return values NULL and it will not allow us to make any changes to them. rownames(temp_char) ## NULL colnames(temp_char) ## NULL dimnames(temp_char) ## NULL 3.1.2 (2)numeric There are two types of numerical data in R, numeric and integer. We will use the numeric class as interpretation since integer can be regarded as a specific kind of numeric. temp_numer = 3.14 Let us explore the attributes of the numeric object. The following are some of the general “read-only” functions. Based on different object passed into the function, the summary will give us different details about the object. summary(temp_numer) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.14 3.14 3.14 3.14 3.14 3.14 str function returns the class and value information. str(temp_numer) ## num 3.14 attributes function still does not give us information about the object. attributes(temp_numer) ## NULL Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object. length(temp_numer) ## [1] 1 We can change the length of the object by appending another numeric number to it. long_temp = append(temp_numer,2.72) class(long_temp) ## [1] &quot;numeric&quot; length(long_temp) ## [1] 2 unique function returns the unique element in the object. unique(temp_numer) ## [1] 3.14 For each unique element in the object, table function returns their frequencies. range function returns the lowest and largest values in the input vector table(temp_numer) ## temp_numer ## 3.14 ## 1 range(temp_numer) ## [1] 3.14 3.14 To change the value of a numeric object, we usually reassign the value of it. temp_numer = 2.72 class function can find out the data type of this object. class(temp_numer) ## [1] &quot;numeric&quot; If we want to change the class of this object, we would normally use the as function. We might try to turn character into a numeric data. The function only works if it makes sense. We cannot turn a character of “hello world” into a number, the function will return error. # To turn a number into character temp_ch = &quot;3.14&quot; temp_num_new = as.numeric(temp_ch) temp_num_new ## [1] 3.14 class(temp_num_new) ## [1] &quot;numeric&quot; We should pay attention to that the as function does not modify the parameter passed into it. So we will have a new object. And if we try to turn a character into a integer, we can see that if it is a real number in meaning, it will be converted into a integer at the output. as.integer(&quot;2.7&quot;) ## [1] 2 It will have problem when the character is not in a good format, for example with a comma. We need to fix the comma problem before we transform the class type. Here, gsub and sub does not have a functional difference, because the matching pattern is not a regular expression. gsub is more powerful and strict in matching pattern when regular expression is used. as.integer(&quot;1,234&quot;) ## [1] NA as.numeric(&quot;1,234&quot;) ## [1] NA as.numeric(gsub(&quot;,&quot;,&quot;&quot;,&quot;1,234&quot;)) ## [1] 1234 as.numeric(sub(&quot;,&quot;,&quot;&quot;,&quot;1,234&quot;)) ## [1] 1234 Now, let us look at some writable attributes of the object. Levels can be modified but still does not effect the class type of the numeric object. levels(temp_numer) ## NULL levels(temp_numer) = 1 levels(temp_numer) ## [1] 1 class(temp_numer) ## [1] &quot;numeric&quot; names function will return the name of an object if it has, and we can assign value to it to name the object. names(temp_numer) ## NULL names(temp_numer) = c(&#39;1&#39;) Other attributes related to names are rownames, colnames and dimnames, since this object does not have dimension, so it will return values NULL and it will not allow us to make any changes to them. rownames(temp_numer) ## NULL colnames(temp_numer) ## NULL dimnames(temp_numer) ## NULL 3.1.3 (3)Logical logical data is the data that contains TRUE and FALSE, or you can abbreviate them as T and F in R. We can easily convert between logicals and numericals with 1 and 0. temp_logical = c(TRUE, FALSE, T, T, F) as.numeric(temp_logical) ## [1] 1 0 1 1 0 as.logical(temp_logical) ## [1] TRUE FALSE TRUE TRUE FALSE Let us explore the attributes of the logicals. The following are some of the general “read-only” functions. Based on different object passed into the function, the summary will give us different details about the object. summary(temp_logical) ## Mode FALSE TRUE ## logical 2 3 str function returns the some information, it is less detailed than the summary. str(temp_logical) ## logi [1:5] TRUE FALSE TRUE TRUE FALSE For each unique element in the object, table function returns their frequencies. range function returns the lowest and largest values in the input vector. range(temp_logical) ## [1] 0 1 table(temp_logical) ## temp_logical ## FALSE TRUE ## 2 3 There are many useful functions related to logicals. We will illustrate a few here: ifelse(&lt;logical&gt;, value1, value2) returns value1 if logical is true, return value2 if logical is false. It is especially useful when we need to inspect or modify data based on its value. Note that this function takes a vector. temp &lt;- c(1,NA,1,NA,2,2,3,4) is.na(temp) ## [1] FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE temp &lt;- ifelse(is.na(temp), 0, temp) temp ## [1] 1 0 1 0 2 2 3 4 Now we have all NA values changed to 0. all(&lt;logical vector&gt;) returns if all the logical values are true. temp &lt;- c(1,NA,1,NA,2,2,3,4) all(is.na(temp)) ## [1] FALSE Not all values of temp are NAs. any(&lt;logical vector&gt;) returns if any of the logical values are true temp &lt;- c(1,NA,1,NA,2,2,3,4) any(is.na(temp)) ## [1] TRUE There exists an NA value in temp. which(&lt;logical vector&gt;) return the index which the logical value is true temp &lt;- c(1,NA,1,NA,2,2,3,4) which(is.na(temp)) ## [1] 2 4 The element at indexes 2 and 4 of temp are NAs. The basic classes of R includes numeric, logical, character, integer, complex, since numeric, character, logical are the most common ones, so we illustrated these 3 classes. Complex is a broader range than the real number, in reality, the data we collect will rarely relate to it. Now we will head to different data structures. 3.2 data structure 3.2.1 (1) vector: A vector can only contain objects from the same class. We can use these two ways to create a vector. It will lead different operations when we want to update this vector object. There is no better between these two methods, they should be used according to our coding needs. temp_vector = c(1,2,3) temp_vector2 = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) num_vec = vector(&quot;numeric&quot;, length = 10) char_vec = vector(&quot;character&quot;,length = 10) Let us explore the attributes of the vector. The following are some of the general “read-only” functions. summary(temp_vector) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 1.5 2.0 2.0 2.5 3.0 str(temp_vector) ## num [1:3] 1 2 3 attributes(temp_vector) ## NULL Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object. length(temp_vector) ## [1] 3 We can change the length of the object by appending another character to it. If we want append a number to a character vector, the number will be converted into a character to be consistent. long_temp = append(temp_vector,2.72) class(long_temp) ## [1] &quot;numeric&quot; length(long_temp) ## [1] 4 unique function returns the unique element in the object. unique(temp_vector) ## [1] 1 2 3 For each unique element in the object, table function returns their frequencies. range function returns the maximum and minimum values. table(temp_vector) ## temp_vector ## 1 2 3 ## 1 1 1 range(temp_vector) ## [1] 1 3 range(temp_vector2) ## [1] &quot;a&quot; &quot;c&quot; To update the value of a vector object, we use the index to access and change that value. temp_vector[1] = 1.1 temp_vector ## [1] 1.1 2.0 3.0 match function can be used to find the corresponding item based on value and update its values. which function can be used to find all the matches. This is like the relationship between regexpr and gregexpr, where regexpr only gives you the first match of the string (reading left to right) and gregexpr will return all of the matches in a given string if there are is more than one match. For character vector, we might also use gregexpr if we look for certain items with some patterns. temp_vector[match(c(1.1,2,3),temp_vector)] ## [1] 1.1 2.0 3.0 temp_vector[which(temp_vector %in% c(1.1,2,3))] ## [1] 1.1 2.0 3.0 temp_vector2[match(c(&quot;a&quot;,&quot;b&quot;),temp_vector2)] ## [1] &quot;a&quot; &quot;b&quot; temp_vector2[which(temp_vector2 %in% c(&quot;a&quot;,&quot;b&quot;))] ## [1] &quot;a&quot; &quot;b&quot; If we want to change the values of some items within specific range of values, we can use the logical expression to subset them. temp_vector[temp_vector &lt;= 2] ## [1] 1.1 2.0 sapply(temp_vector[temp_vector &lt;= 2],FUN = function(x){x=1}) ## [1] 1 1 sapply(temp_vector[temp_vector &lt;= 2],FUN = function(x){x+1}) ## [1] 2.1 3.0 class function can find out the data type of this object. We can see that the class type for vector is not vector but the data type of the item inside the vector. class(temp_vector) ## [1] &quot;numeric&quot; class(num_vec) ## [1] &quot;numeric&quot; class(char_vec) ## [1] &quot;character&quot; If we want to change the class of this object, we would normally use the as function. We might try to use the sapply functions. We might try to turn character into a numeric data. The function only works if it makes sense. The sapply function behaves similarly to lapply; the only real difference is in the return value. sapply will try to simplify the result of lapply. vec_char = c(&quot;3.14&quot;,&quot;2.72&quot;) sapply(vec_char, as.numeric) ## 3.14 2.72 ## 3.14 2.72 lapply(vec_char,as.numeric) ## [[1]] ## [1] 3.14 ## ## [[2]] ## [1] 2.72 We should pay attention to that the as function does not modify the parameter passed into it. So we will have a new object. Now, let us look at some writable attributes of the object. Levels is NULL and can be modified but still does not effect the class type of the numeric object. levels(temp_vector) ## NULL levels(temp_vector) = 1 class(temp_vector) ## [1] &quot;numeric&quot; names function will return the name of an object if it has, and we can assign value to it to name the object. Then we will be able to access the items with their names. names(temp_vector) ## NULL names(temp_vector) = c(&#39;alpha&#39;,&#39;beta&#39;,&#39;ceta&#39;) temp_vector[&#39;alpha&#39;] ## alpha ## 1.1 If we want to find some items with certain names, we can find them in the following ways. temp_vector[names(temp_vector) %in% c(&quot;hello&quot;,&quot;world&quot;,&quot;alpha&quot;) ] ## alpha ## 1.1 Other attributes related to names are rownames, colnames and dimnames, since this object does not have dimension, so it will return values NULL and it will not allow us to make any changes to them. rownames(temp_numer) ## NULL colnames(temp_numer) ## NULL dimnames(temp_numer) ## NULL 3.2.2 (2)list: A list can contain objects from the different classes. We can use two ways to create a list. temp_list = list(&quot;a&quot; = c(1,2), 2.72,&quot;pi&quot; = 3.14, &quot;c&quot; = &quot;hello world&quot;) list_vec = vector(&quot;list&quot;, length = 5) Let us explore the attributes of the list. The following are some of the general “read-only” functions. summary(temp_list) ## Length Class Mode ## a 2 -none- numeric ## 1 -none- numeric ## pi 1 -none- numeric ## c 1 -none- character str(temp_list) ## List of 4 ## $ a : num [1:2] 1 2 ## $ : num 2.72 ## $ pi: num 3.14 ## $ c : chr &quot;hello world&quot; str(list_vec) ## List of 5 ## $ : NULL ## $ : NULL ## $ : NULL ## $ : NULL ## $ : NULL attributes(temp_list) ## $names ## [1] &quot;a&quot; &quot;&quot; &quot;pi&quot; &quot;c&quot; Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object. length(temp_list) ## [1] 4 We can change the length of the object by append function. long_temp = append(temp_list,0) class(long_temp) ## [1] &quot;list&quot; length(long_temp) ## [1] 5 unique function returns the unique element in the object. unique(temp_list) ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] 2.72 ## ## [[3]] ## [1] 3.14 ## ## [[4]] ## [1] &quot;hello world&quot; For each unique element in the object,we cannot use the table function directly but to apply it to each item of the the object. range function returns the maximum and minimum values. sapply(temp_list,table) ## $a ## ## 1 2 ## 1 1 ## ## [[2]] ## ## 2.72 ## 1 ## ## $pi ## ## 3.14 ## 1 ## ## $c ## ## hello world ## 1 range(temp_list) ## [1] &quot;1&quot; &quot;hello world&quot; To update the value of a vector object, we use the index/name to access and change that value. The following three ways can be used to acces and modify an object in the list. temp_list[[&quot;a&quot;]] ## [1] 1 2 temp_list$a = c(1,3) temp_list[[1]] ## [1] 1 3 The following indexings are used to access the first item, it is worth noticed that it returns the whole item including its value. temp_list[&quot;a&quot;] ## $a ## [1] 1 3 temp_list[1] ## $a ## [1] 1 3 temp_list[1][1] ## $a ## [1] 1 3 If we want to find the nested element in an item of a list, we will use the following ways to select. temp_list[[c(1,1)]] ## [1] 1 temp_list[[1]][[2]] = 2 temp_list[[1]][2] ## [1] 2 class function can find out the data type of this object. class(temp_list) ## [1] &quot;list&quot; Now, let us look at some writable attributes of the object. Levels is NULL and can be modified but still does not effect the class type of the numeric object. levels(temp_list) ## NULL levels(temp_list) = 1 class(temp_list) ## [1] &quot;list&quot; names function will return the name of an object if it has, and we can assign value to it to name the object. Recall previously that we cannot name the item when appending it, but we can name it later. names(temp_list) ## [1] &quot;a&quot; &quot;&quot; &quot;pi&quot; &quot;c&quot; names(temp_list)[2] = &quot;e&quot; If we want to find some items with certain names, we can find them in the following ways. temp_list[names(temp_list) %in% c(&quot;hello&quot;,&quot;world&quot;,&quot;e&quot;,&quot;c&quot;) ] ## $e ## [1] 2.72 ## ## $c ## [1] &quot;hello world&quot; Other attributes related to names are rownames, colnames and dimnames, since this object does not have dimension, so it will return values NULL and it will not allow us to make any changes to them. rownames(temp_list) ## NULL colnames(temp_list) ## NULL dimnames(temp_list) ## NULL 3.2.3 (3)factor Often factors will be automatically created when we read a dataset in using a function like read.table(). factor function will create a factor array. temp_factor = factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;)) Let us explore the attributes of the factor. The following are some of the general “read-only” functions. summary(temp_factor) ## no yes ## 2 3 str(temp_factor) ## Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 1 attributes function tells us the levels of the factor, it is set by default. And we can know from the str function that “no” corresponds to the level 1 and “yes” to 2. attributes(temp_factor) ## $levels ## [1] &quot;no&quot; &quot;yes&quot; ## ## $class ## [1] &quot;factor&quot; Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object. length(temp_factor) ## [1] 5 It is not common to add new item into a factor, we can try using the append function to do it. And we have found that during the append operation, the function takes the levels attribute to do the work. So if we append a character to it, the resulting object is a character vector and so does the numeric object. So this is not a proper operation and it would be better if we create a vector than turn it into a factor with as.factor or factor function. temp_factor ## [1] yes yes no yes no ## Levels: no yes class(temp_factor) ## [1] &quot;factor&quot; long_temp_char = append(temp_factor,&quot;no&quot;) long_temp_num = append(temp_factor,2019) long_temp_char ## [1] &quot;2&quot; &quot;2&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;no&quot; long_temp ## $a ## [1] 1 2 ## ## [[2]] ## [1] 2.72 ## ## $pi ## [1] 3.14 ## ## $c ## [1] &quot;hello world&quot; ## ## [[5]] ## [1] 0 unique function returns the unique element in the object. unique(temp_factor) ## [1] yes no ## Levels: no yes For each unique element in the object, table function returns their frequencies. range function does not have meanings under the level order, so it will return error. table(temp_factor) ## temp_factor ## no yes ## 2 3 #range(temp_factor) To update the value of a vector object, we use the index to access and change that value. We can only modify the value within the current levels, out of scope modification will return error, which means we cannot assign value like “neutral” to the item inside the factor. temp_factor[1] = &quot;no&quot; temp_factor ## [1] no yes no yes no ## Levels: no yes match function can be used to find the corresponding item based on value and update its values. which function can be used to find all the matches. Then we can use the sapply function to modify the value of the elements. temp_factor ## [1] no yes no yes no ## Levels: no yes temp_factor[match(c(&quot;yes&quot;,&quot;no&quot;),temp_factor)] ## [1] yes no ## Levels: no yes temp_factor[which(temp_factor %in% c(&quot;yes&quot;,&quot;no&quot;))] ## [1] no yes no yes no ## Levels: no yes class function can find out the data type of this object. class(temp_factor) ## [1] &quot;factor&quot; Now, let us look at some writable attributes of the object. levels function returns the default level, we would try to reset the order with the factor function. We should notice that though we can modify the levels directly, but this will not return the desired result, so we should not use this way to modify it. temp_factor = factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;)) levels(temp_factor) ## [1] &quot;no&quot; &quot;yes&quot; sort(temp_factor) ## [1] no no yes yes yes ## Levels: no yes # build a new factor sort(factor(temp_factor,levels = c(&quot;yes&quot;,&quot;no&quot;))) ## [1] yes yes yes no no ## Levels: yes no # directly change the level levels(temp_factor) = c(&quot;yes&quot;,&quot;no&quot;) sort(temp_factor) ## [1] yes yes no no no ## Levels: yes no # build a new factor sort(factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;), levels = c(&quot;yes&quot;, &quot;no&quot;)) ) ## [1] yes yes yes no no ## Levels: yes no names function will return the name of an object if it has, and we can assign value to it to name the object. Then we will be able to access the items with their names. temp_factor = factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;)) names(temp_factor) ## NULL names(temp_factor) = c(&#39;alpha&#39;,&#39;beta&#39;,&#39;ceta&#39;,&#39;gamma&#39;,&#39;omega&#39;) temp_factor[&#39;alpha&#39;] ## alpha ## yes ## Levels: no yes If we want to find some items with certain names, we can find them in the following ways. temp_factor[names(temp_factor) %in% c(&quot;hello&quot;,&quot;world&quot;,&quot;alpha&quot;) ] ## alpha ## yes ## Levels: no yes Other attributes related to names are rownames, colnames and dimnames, since this object does not have dimension, so it will return values NULL and it will not allow us to make any changes to them. rownames(temp_factor) ## NULL colnames(temp_factor) ## NULL dimnames(temp_factor) ## NULL 3.2.4 (4)matrix matrix is a data structure with dimensions. matrix function will create a matrix. temp_matrix &lt;- matrix(0, nrow = 2, ncol = 5) Let us explore the attributes of the factor. The following are some of the general “read-only” functions. summary(temp_matrix) ## V1 V2 V3 V4 V5 ## Min. :0 Min. :0 Min. :0 Min. :0 Min. :0 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :0 Median :0 Median :0 Median :0 Median :0 ## Mean :0 Mean :0 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :0 Max. :0 Max. :0 Max. :0 Max. :0 str(temp_matrix) ## num [1:2, 1:5] 0 0 0 0 0 0 0 0 0 0 attributes(temp_matrix) ## $dim ## [1] 2 5 Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object, the length is calculated by multiplying dimensions. length(temp_matrix) ## [1] 10 nrow(temp_matrix) ## [1] 2 ncol(temp_matrix) ## [1] 5 length(temp_matrix) ## [1] 10 If we want to add another matrix into the current matrix, we will use the rbind and cbind functions. rbind means binding along the row, so it will require the column sizes of two matrices are the same. cbind means binding along the column, so it will require the row sizes of two matrices are the same. append function does not work, because it would turn the matrix into a 1 dimensions data structure which is not we want. long_temp = append(temp_matrix,cbind(1,2,3,4,5)) attributes(long_temp) ## NULL temp_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 long_temp_row = rbind(temp_matrix,cbind(1,2,3,4,5)) long_temp_col = cbind(temp_matrix,rbind(1,2)) unique function returns the unique element in the object. temp_matrix[2,2] = 3 unique(temp_matrix) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 3 0 0 0 For each unique element in the object, table function returns their frequencies. range will return the minimum and maximum object. The operations are done across the matrix, not confined to certain row or column. table(temp_matrix) ## temp_matrix ## 0 3 ## 9 1 range(temp_matrix) ## [1] 0 3 To update the value of a matrix object, we use the index to access and change that value. We cannot update with a different data type. # this is not valid: # temp_matrix[1][2] temp_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 3 0 0 0 temp_matrix[1,2] = 1 #for a row temp_matrix[2,] ## [1] 0 3 0 0 0 #for a column temp_matrix[,2] ## [1] 1 3 Random access item like this will return value by spreading the matrix. temp_matrix[4] ## [1] 3 The in function can help us to determine if an value falls into our criteria, we directly use the TRUE and False result to subset the matrix we want. When using the which function, we need to be careful that we are selecting rows so that the subset format should be correct. Then we can use the sapply function to modify the value of the elements. temp_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 0 0 0 ## [2,] 0 3 0 0 0 temp_matrix[,2] %in% c(2,3,4) ## [1] FALSE TRUE temp_matrix[temp_matrix[,2] %in% c(2,3,4)] ## [1] 0 3 0 0 0 which(temp_matrix[,2] %in% c(2,3,4) ) ## [1] 2 temp_matrix[which(temp_matrix[,2] %in% c(2,3,4)),] ## [1] 0 3 0 0 0 class function can find out the data type of this object. class(temp_matrix) ## [1] &quot;matrix&quot; Now, let us look at some writable attributes of the object. levels function returns NULL because this is not a factor matrix. We create a factor matrix but it will lose the attributes as a factor when we apply matrix operations to it. So levels does not have significant meaning in matrix. levels(temp_matrix) ## NULL names function will return the name of an object if it has, and we can assign value to it to name the object. Then we will be able to access the items with their names. names(temp_matrix) = c(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;) temp_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 0 0 0 ## [2,] 0 3 0 0 0 ## attr(,&quot;names&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; NA NA temp_matrix[&#39;3&#39;] ## 3 ## 1 If we want to find some items with certain names, we can find them in the following ways. temp_matrix[names(temp_matrix) %in% c(&#39;4&#39;,&#39;5&#39;,&#39;6&#39;) ] ## 4 5 6 ## 3 0 0 But selecting a single item is not usually what we want, we might want to select a whole column/row based on its name. Other attributes related to names are rownames, colnames and dimnames, the dimensions of the assignment value must match the matrix size. rownames(temp_matrix) = c(&quot;1&quot;,&#39;2&#39;) colnames(temp_matrix) = c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;) temp_matrix ## a b c d e ## 1 0 1 0 0 0 ## 2 0 3 0 0 0 ## attr(,&quot;names&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; NA NA dimnames(temp_matrix) = list(c(&quot;1&quot;,&#39;2&#39;),c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;)) To access or update value with their names we use the following format. temp_matrix[&#39;1&#39;,] ## a b c d e ## 0 1 0 0 0 temp_matrix[,&#39;a&#39;] ## 1 2 ## 0 0 3.2.5 (5) dataframe Data frames are usually created by reading in a dataset using the read.table() or read.csv(). Each column can be viewed as a vector. And can be viewed as list with vectors of same length. We load a dataframe from the basic R data function data(&quot;women&quot;) Let us explore the attributes of the factor. The following are some of the general “read-only” functions. summary(women) ## height weight ## Min. :58.0 Min. :115.0 ## 1st Qu.:61.5 1st Qu.:124.5 ## Median :65.0 Median :135.0 ## Mean :65.0 Mean :136.7 ## 3rd Qu.:68.5 3rd Qu.:148.0 ## Max. :72.0 Max. :164.0 str(women) ## &#39;data.frame&#39;: 15 obs. of 2 variables: ## $ height: num 58 59 60 61 62 63 64 65 66 67 ... ## $ weight: num 115 117 120 123 126 129 132 135 139 142 ... attributes(women) ## $names ## [1] &quot;height&quot; &quot;weight&quot; ## ## $class ## [1] &quot;data.frame&quot; ## ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Then let us explore more about the “read-only” functions and get to know the functions that will change their return values. length function will return the length of the object, the length is calculated by multiplying dimensions. length(women) ## [1] 2 nrow(women) ## [1] 15 ncol(women) ## [1] 2 dim(women) ## [1] 15 2 If we want to add another dataframe into the current matrix, we will use the rbind method because this is a new observation. If we want to add a new variable then we will use cbind. long_temp_row = rbind(women,data.frame(height=c(110), weight=c(77))) dim(long_temp_row) ## [1] 16 2 unique function returns the unique element in the object. unique(women) ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 For each unique element in the object, table function returns their frequencies. range will return the minimum and maximum object. The operations are done across the matrix, not confined to certain row or column. table(women) ## weight ## height 115 117 120 123 126 129 132 135 139 142 146 150 154 159 164 ## 58 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 59 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 60 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 61 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 62 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 63 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## 64 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 65 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 68 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 70 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ## 71 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 72 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 range(women) ## [1] 58 164 To update the value of a dataframe item, we use the index to access and change that value. We cannot update with a different data type. women[3,2] ## [1] 120 #for a row women[2,] ## height weight ## 2 59 117 #for a column women[,2] ## [1] 115 117 120 123 126 129 132 135 139 142 146 150 154 159 164 Random access item like this will return the 1st column value of the dataframe which is different from the matrix operation. women[1] ## height ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 To change the values in a row or in a column, we can use sapply function or vectorized function to modify the data. And if we want to apply changes to all the rows/columns, we will use the apply function. To create new variable based on the other variables, we will use the mapply function. women[,2] = as.factor(women[,2]) women ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 data.frame(apply(women,2,as.numeric) ) # 1 for row statistics, 2 for column statistics ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 data(&quot;women&quot;) mapply(function(h,w){w/(h*h)},women[,1],women[,2]) ## [1] 0.03418549 0.03361103 0.03333333 0.03305563 0.03277836 0.03250189 ## [7] 0.03222656 0.03195266 0.03191001 0.03163288 0.03157439 0.03150599 ## [13] 0.03142857 0.03154136 0.03163580 To subset some dataframe, we use the in function and the which function. We need to be careful that we are selecting rows so that the subset format should be correct, unlike matrix, when in function can be easily converted to the result we want. And we can also use the split function and then access the part we want with corresponding index. #split(women,women[,2]) women[women[,2] %in% c(117,120),] ## height weight ## 2 59 117 ## 3 60 120 women[which(women [,2] %in% c(117,120)),] ## height weight ## 2 59 117 ## 3 60 120 class function can find out the data type of this object. class(women) ## [1] &quot;data.frame&quot; Now, let us look at some writable attributes of the object. levels function returns NULL even if the dataframe contains factor variables. levels(women) ## NULL names function will return the name of an object if it has, and we can assign value to it to name the object. Then we will be able to access the items with their names. Unlike matrix, in dataframe the names is for the column names, so by names, we will access a column. names(women) ## [1] &quot;height&quot; &quot;weight&quot; names(women) = c(&#39;1&#39;) women[&#39;1&#39;] ## 1 ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 Other attributes related to names are rownames, colnames and dimnames, the dimensions of the assignment value must match the matrix size. rownames(women) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; colnames(women) ## [1] &quot;1&quot; NA dimnames(women) ## [[1]] ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## ## [[2]] ## [1] &quot;1&quot; NA To access or update value with their names we use the following format. women[&#39;1&#39;,] ## 1 NA ## 1 58 115 women[,&#39;1&#39;] ## [1] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 "],
["data-structure-and-cleaning-101.html", "Chapter 4 Data structure and cleaning 101 4.1 Overview 4.2 Data Structure 4.3 Data Cleaning", " Chapter 4 Data structure and cleaning 101 Zijian Wang and Shuo Yang 4.1 Overview From our own experiences, many of the errors or difficulties arised during implementing charts or analysis are highly likely due to the data. In this tutorial, we will discuss and summarize some basic data structure knowledge and useful data cleaning skills. We hope it will be helpful by shareing some common mistakes and practical tips, especially for the beginners in R. 4.2 Data Structure 4.2.1 Basic Data Types character: strings “this is an example”. numeric: numbers 1, 1.1, 2.5. factor: categorical labels “red”, “green”, “blue”. Date: dates “2001-02-03”, “09-20-2019”. logical: Boolean True, Flase. integer : integer values 1, 2, 3. For these commonly used data types, we can check the data type by is.*() and convert to one another using as.*(). 4.2.2 Attributes R objects can have attributes to describe the data object. Commonly used attributes are names, dimnames, dimensions, class, length and other user-defined attibutes. 4.2.3 Vector Vector is a basic data structure in R. It cantains the element of the same type. Create Assign value directly by c() v&lt;-c(&#39;Alpha&#39;,&#39;Beta&#39;) #char vector v&lt;-c(TRUE, FALSE, TRUE) #logi vector v&lt;-c(1,6,4.5,3.2) #num vector Use : operator for numeric sequence with increment 1 v&lt;-1:4 Use functions that returns a vector seq(0,1,by=0.1) #increment by 0.1 ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 rep(c(&quot;A&quot;,&quot;B&quot;), each=2, times=3) #repeat each by 2 times and 3 complete replications ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; Access Use integer as index v&lt;-1:4 v[c(1,2)] #access 1st and 2nd element. ## [1] 1 2 Use logical vector as index v&gt;2 #a logical vector comparing elements in v to 2. ## [1] FALSE FALSE TRUE TRUE v[v&gt;2] #access elements that is larger than 2. ## [1] 3 4 Use element name as index names(v)&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;) #assign each element a name v[c(&#39;A&#39;,&#39;B&#39;)] # using name as an index ## A B ## 1 2 Operation Modify an element: use &lt;— to assign a value to overide the original one v&lt;-1:4 v[1]&lt;-5 v ## [1] 5 2 3 4 Add an element: use c() to add or combine two vectors c(v,c(1,2,3)) # combine two vectors together. ## [1] 5 2 3 4 1 2 3 Delete an element: use negative index to delete the element v[-3] #delete the 3rd element ## [1] 5 2 4 4.2.4 Matrix Matrix is similar to vector with additional dimension attribute. Matrix is 2-dimensional and requires the same type elements. Create Use matrix() to create a matrix. Dimension attribute can be set by nrow and/or ncol. Dimension names can also be set by argument. Matrix is filled by column by default. m&lt;-matrix(1:6, nrow=2, #byrow=FALSE by default dimnames=list(c(&quot;row1&quot;,&quot;row2&quot;), c(&quot;col1&quot;,&quot;col2&quot;,&quot;col3&quot;))) m ## col1 col2 col3 ## row1 1 3 5 ## row2 2 4 6 Filled by row: matrix(1:6, nrow=2, byrow=TRUE) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 Access Elements of matrix can be accessed by integer, logical and name as index, like what we do with vector. Index is needed for both row and column. Use integer as index m[1,3] # element in row 1 and column 3 ## [1] 5 m[,-1] # select all column except first column. ## col2 col3 ## row1 3 5 ## row2 4 6 Use logical vector as index m[c(TRUE,FALSE),] # select rows 1 ## col1 col2 col3 ## 1 3 5 m[m&gt;2] # select element that greater than 2 ## [1] 3 4 5 6 Use element name as index m[&#39;row2&#39;,c(&#39;col2&#39;,&#39;col3&#39;)] # select element in 2nd row and 2nd and 3rd column. ## col2 col3 ## 4 6 Operation Combine two matrices (same column) by row: rbind() x&lt;-matrix(c(1,2,3,4), nrow = 2) y&lt;-matrix(c(5,6,7,8), nrow = 2) rbind(x,y) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## [3,] 5 7 ## [4,] 6 8 Combine two matrices (same row) by column: cbind() cbind(x,y) ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 Change dimension using dim() dim(x)&lt;-c(1,4)# change the dimension to (1,4) x #Matrix is rearranged by column. ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 4.2.5 Array Arrays can store data in more than two dimensions. One-dimensional arrays are similar to vectors. A two-dimensional arrays are the same as a matrix. Array requires input of same type. Create A&lt;-array(1:12, dim=c(3,2,2)) # create a array of three dimension. A ## , , 1 ## ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 ## ## , , 2 ## ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 ## [3,] 9 12 Access Accessing elements is similar to vector and matrix. The index must be consistent with the dimension. For a three-dimensional array, index is in the format [ , , ]. Operation As array is made of matrices in multiple dimensions, the operation on elements are carried out by accessing elements of the matrices. matrix1&lt;-A[,,1] matrix2&lt;-A[,,2] matrix1+matrix2 # add element in the two matrix together accordingly. ## [,1] [,2] ## [1,] 8 14 ## [2,] 10 16 ## [3,] 12 18 We can also do operation on specific dimension of an array: apply(A,1,sum) # sum the rows value across all matrixis. ## [1] 22 26 30 apply(A,2,sum) # sum the columns value across all matrixis. ## [1] 30 48 apply(A,3,sum) # sum the matrix value across all matrixis ## [1] 21 57 4.2.6 List A vector with elements of different types is called a list. For example, a list can have a vector of character, a factor, a numeric matrix and a data frame as its four components. Great flexibility makes lists widely used. Create We can give names of the components to the list or use the index for reference. L&lt;-list(a=1:4,FALSE) # assign a name to the first component L ## $a ## [1] 1 2 3 4 ## ## [[2]] ## [1] FALSE Accesse We can access the contents in each component by either $ notation or [[]] L$a ## [1] 1 2 3 4 L[[2]] ## [1] FALSE We can obtain a sublist of the list by [] L[1] ## $a ## [1] 1 2 3 4 Note that L[[1]] returns the content of the first component, which is a numeric vector in our example. On the other hand, L[1] returns a list of one component, which is the first component of L. Operation Add components to a list: similar like vector, we can use c() to add components. The list structure is dynamic, which means we can also simply assign values using new tags. c(L,b=&quot;s&quot;) ## $a ## [1] 1 2 3 4 ## ## [[2]] ## [1] FALSE ## ## $b ## [1] &quot;s&quot; L$b&lt;-&quot;s&quot; # add a string component to the list and name it &#39;b&#39;. L ## $a ## [1] 1 2 3 4 ## ## [[2]] ## [1] FALSE ## ## $b ## [1] &quot;s&quot; Delete components from a list: we can delete a component either by negative index or assigning NULL to it. L[-1] # delete the first component of the list. ## [[1]] ## [1] FALSE ## ## $b ## [1] &quot;s&quot; L$b&lt;-NULL # delete the component which name is b. L ## $a ## [1] 1 2 3 4 ## ## [[2]] ## [1] FALSE Flatten lists: use unlist to produce a vector for all components in the list. The components will be converted into an approporiate unique mode. unlist(L) # FALSE is converted to 0 ## a1 a2 a3 a4 ## 1 2 3 4 0 4.2.7 Data Frame Data frame is a special case of list. Data frame is organized in a two-dimensional way, which has equal length for each column. Create Use data.frame(). If the name of a column is not provided, it will make a valid name with the component. Another thing to point out is argument stringsAsFactors. It is set to default setting, which is TRUE for most of the cases. We recommend to set it to FALSE if there is at least column meant to be character. df&lt;-data.frame(&#39;A&#39;=c(1,2,3),&#39;B&#39;=c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), stringsAsFactors = FALSE) df ## A B ## 1 1 a ## 2 2 b ## 3 3 c Access The content can be accessed like a list using $ operator and [[]] or like a matrix using the index. df$A #using name to select columns ## [1] 1 2 3 df[[1]] #like a list ## [1] 1 2 3 df[,1] #like a matrix ## [1] 1 2 3 Operation Here we only discuss basic operation of data frame from a data structure perspective. {dplyr} is highly recommended for efficient data frame operation. Add a column in data frame: just like list. df$&#39;m&#39;&lt;-c(TRUE,TRUE,FALSE) # add a column m df&lt;-cbind(df,n=2:4) # add a column n df ## A B m n ## 1 1 a TRUE 2 ## 2 2 b TRUE 3 ## 3 3 c FALSE 4 Add a row in data frame: add the new row as a data frame. df.new&lt;-data.frame(A=4, B=&quot;d&quot;, m=TRUE, n=5) rbind(df,df.new) # add a row ## A B m n ## 1 1 a TRUE 2 ## 2 2 b TRUE 3 ## 3 3 c FALSE 4 ## 4 4 d TRUE 5 Delete a column in data frame: we can do this by consider the data frame as a matrix or list df[[1]]&lt;-NULL # delete the first column df ## B m n ## 1 a TRUE 2 ## 2 b TRUE 3 ## 3 c FALSE 4 Delete a row in data frame: just like matrix, by negative the row index we want to delete df[-1,] # delete the first row ## B m n ## 2 b TRUE 3 ## 3 c FALSE 4 4.2.8 Data Structure Conversion Similar as data types, there are as.* functions to convert object into different structures. Pay attention to the differences in attributes when performing conversion. For example, A data frame converted to a matrix will lose its variety in column data types. 4.2.9 Functions to Check Data Structure Attributes Name Related names(): get or set the names of the object. For matrix-like objects (matrix and data frame), names() is dealing with the column names. colnames() and rownames(): only work for matrix-like objects. Size Related length(): get or set the length of object. vector&lt;-1:12 length(vector) # length of the vector is the number of elements. ## [1] 12 matrix&lt;-matrix(1:12,nrow = 3) length(matrix) # length of the matrix is the number of all elements. ## [1] 12 array&lt;-array(1:12,dim=c(2,3,2)) length(array) # length of the array is the number of all elements. ## [1] 12 list&lt;-list(&#39;a&#39;=c(1,2,3),&#39;b&#39;=c(&#39;r&#39;,&#39;b&#39;)) length(list) # length of the list is the number of components in the list. ## [1] 2 df&lt;-data.frame(&#39;a&#39;=c(1,2,3),&#39;b&#39;=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;)) length(df) # length of the data frame is the number of column. ## [1] 2 nrow() and ncol(): returns the number of rows or column for matrix-like objects. dim(): returns a vector c(nrow(), ncol()) for matrix-like objects. Type Related typeof(): object’s type (low-level). class(): object’s type (high-level). Others attributes(): present the attributes of the object. str(): structure of the object and information about the class, length and content of each column. str(df) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ a: num 1 2 3 ## $ b: Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 2 3 summary(): summary statistics summary(df) #summary for data frame by column ## a b ## Min. :1.0 A:1 ## 1st Qu.:1.5 B:1 ## Median :2.0 C:1 ## Mean :2.0 ## 3rd Qu.:2.5 ## Max. :3.0 summary(L) #summary for list ## Length Class Mode ## a 4 -none- numeric ## 1 -none- logical head() and tail(): return the first few part and last few part of the object head(1:100) ## [1] 1 2 3 4 5 6 tail(1:100) ## [1] 95 96 97 98 99 100 Tips: Getting familiar with different data types and structures will help you spot simple mistakes. Be aware of each data structures’ different characteristics, like dimension and data types. Most of the functions require certain type of data input and produce certain type of outputs. Keeping a clear clue of what to input and what you get will help debug errors. Understanding the relationship and conversion among different data structures will provide more flexibility dealing with the analysis. 4.3 Data Cleaning Getting data ready for analysis can be quite tedious and time consuming. In this chapter, we will discuss some general workflow for the data cleaning process using {tidyverse}, including importing and tidying data. We will also summarise some common mistakes and provide useful tips. 4.3.1 Import Data Commonly used data file types are .txt, .csv, .xlsx and .xls. Import .txt and .csv using {readr package} The .txt and .csv files are flat files with certian character as delimiter to seperate fields with in a record. Normally .txt is tab (\\t) delimited and .csv is comma (,) delimited. General function to import a delimited file is read_delim(). Argument delim needs to be set to indicate delimiter. With commonly used delimiers “,” and “\\t”, read_csv() and read_tsv are the special caes of read_delim(). For example, read_csv(...) = read_delim(..., delim=&quot;,&quot;). By defualt, argument col_names is set to be TRUE for these read_* functions. It uses the first row as the column names. Remember to set it to FALSE if it starts with data directly without column names. col_names can also take a character vector for the known column names. By defualt, argument na is set to be c(&quot;&quot;,&quot;NA&quot;). If there is any other input interpreted as missing values, like &quot;N\\A&quot; or &quot;*&quot;, it can be pass to na as a character vector. library(tidyverse) df&lt;-read_delim(&quot;1|a|\\nNA|c|5.5\\n2|*|4.3&quot;, delim = &quot;|&quot;, col_names=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), na=c(&quot;&quot;,&quot;NA&quot;,&quot;*&quot;)) df ## # A tibble: 3 x 3 ## A B C ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 a NA ## 2 NA c 5.5 ## 3 2 &lt;NA&gt; 4.3 Import .xlsx and .xls using {readxl package} read_excel() is commonly used, without specifying file type. Otherwise, read_xls() can be used for .xls file and read_xlsx() can be used for .xlsx file. Argument sheet is for the sheet to read. It is either a string of the sheet name, or an integer \\(i\\) for the ith sheet. col_names is similar to {readr} functions na=&quot;&quot; by default. It can be modified similar as the above {readr} functions. Tips: It is always helpful to open the original files to observe the data first. This will help determine which delimiter is used, whether the first line is the column name, what should be interpreted as missing value, and any notable pattern for further cleaning (for example 50* for 50 with some notes). Sometimes it might not be possible to open the original file directly, for example the file size is too large. Then you need to go through a trial and error process to set the proper arguments. In this case, argument of n_max is very useful. You can start with the general functions reading the first 10 rows by setting n_max=10. Then you can decide whether the first row is the column name. For delimited files, you can try a few delimiter till you find the correct one. As for missing values, you can always find a way to modify them in the later cleaning process. There is an easy way to familiarize yourselve with the previous two sets of function. On the Environment tab, you can manually import dataset. Select “From Text (readr)…” and “From Excel…” from the dropdown list and play around with the “Import Options”. Then you can visualize the effects of different arguments directly from the “Data Preview” and learn the coding details from the “Code Preview”. The local file to read is usually put in the working directory. But if you want to access a file in some other folder and do not know the path to it, here is the trick. Function file.choose() will pop up a dialog box allowing you to choose a file interactively and return the path to this file. There are packages for other types of data to explore: {rvest} for html data, like the html_table. {haven} reads SPSS, Stata, and SAS files. {dbplyr} returns a data frame with SQL queries against a database, along with a backend package {DBI} to work with different databases (e.g. RMySQL, RSQLite, RPostgreSQL etc). 4.3.2 Tidy Data Before we perform any data manipulation with fancy dplyr pipes for further visualization or modeling, we must have tidy data at hand. However in the real world, the imported data is never as clean as the textbook one. Each data has its own adjustment needs, here we try to discuss some must-check steps and useful skills, along with some tips. Data Overview Use str() to get the structure of the data frame. It shows the size of the data frame, each column’s name and data types. This is a quick check of what the data looks like. Use head() to get first few rows of data. Use summary() to get a columnwise statistics summary. This will provide some initial ideas of the data range and distribution. It is also a good way to check NAs. These functions is useful whenever a data overview is needed in the data cleaning process, especially when there is any big change in the data frame. Column Names Majority of the functions for data import will return a data-frame like structure or something can be easily transformed into a data frame. An important feature of data frame is its column name. {readr} can preserve its column names as original with white space and special characters, but it is still recommended to keep a clear short name with &quot;_&quot; at most. Renaming the columns is basically string manipulation, which can be performed with {stringr} eg.colname&lt;-c(&quot;Number of Moons&quot;, &quot;Ring System?&quot;) #eg1: replace white space by &quot;_&quot; eg1&lt;-str_replace_all(eg.colname,&quot; &quot;,&quot;&quot;) eg1 ## [1] &quot;NumberofMoons&quot; &quot;RingSystem?&quot; #eg2: remove all white space and special characters eg2&lt;-str_replace_all(eg.colname,&quot;(\\\\W)&quot;,&quot;&quot;) #\\W is RegEx for any non char eg2 ## [1] &quot;NumberofMoons&quot; &quot;RingSystem&quot; #eg3: keep the first 3 char as the abbr. eg3&lt;-str_sub(eg.colname, 1, 3) eg3 ## [1] &quot;Num&quot; &quot;Rin&quot; #eg4: design your own way for names eg4&lt;-sapply(sapply(eg.colname,str_extract_all,&quot;([A-Z][a-z]{2})&quot;), paste0,collapse = &quot;&quot;) eg4 ## Number of Moons Ring System? ## &quot;NumMoo&quot; &quot;RinSys&quot; Then column names can be changed by colnames or rename_* functions, along with above string manipulation. The original informative long column names can be stored in a vector separately for future reference. Similar treatments can be applied to row names as needed. Transpose Sometimes we need to transpose data frame for a better organization of data. It is better to perform transposition before any further modification of data. Simpliest way is to treat data frame a matrix and use t(), after which as.data.frame() is needed to transfer data back to a data frame. The drawback is all entries will be forced into one data type. This can be modified in later stage. A better way to transpose data frame while maintaining each column’s data type is to use gather() and spread(). Columnwise Modification Use mutate_all() for general patterns applied to all collumns. For example, remove any unnecessary notation. df&lt;-data.frame(ID=c(&quot;test_1&quot;,&quot;test_2&quot;,&quot;test_3&quot;), Value=c(&quot;3&quot;,&quot;4.5*&quot;,&quot;2&quot;), Type=c(&quot;A&quot;,&quot;A&quot;,&quot;B*&quot;), stringsAsFactors=F) df&lt;-df%&gt;%mutate_all(function(x){str_remove_all(x,&quot;\\\\*&quot;)}) #df%&gt;%mutate_all(str_remove_all,&quot;\\\\*&quot;) #produce the same result df ## ID Value Type ## 1 test_1 3 A ## 2 test_2 4.5 A ## 3 test_3 2 B Use mutate_if() or mutate_at() to modify selected columns. For example, double the value of each numeric column. df&lt;-df%&gt;% mutate_at(vars(Type), as.factor)%&gt;% mutate_at(&quot;Value&quot;,as.numeric)%&gt;% mutate_if(is.numeric, ~.^2) df ## ID Value Type ## 1 test_1 9.00 A ## 2 test_2 20.25 A ## 3 test_3 4.00 B Use separate() to separate information in one column into multiple columns. Combine with string manipulation to modify character columns as desired. df%&gt;%separate(ID,c(&quot;ID_Type&quot;,&quot;Index&quot;),sep=&quot;_&quot;) ## ID_Type Index Value Type ## 1 test 1 9.00 A ## 2 test 2 20.25 A ## 3 test 3 4.00 B df%&gt;%mutate(Index = str_extract(ID,&quot;\\\\d&quot;)) ## ID Value Type Index ## 1 test_1 9.00 A 1 ## 2 test_2 20.25 A 2 ## 3 test_3 4.00 B 3 NAs and Outliers Missing values and extreme values are also very important for future analysis. Depending on the purposes, there are different strategies dealing with them. We will not cover them in this tutorial, but we need to highlight their importance. We would rather to include NA and outlier processing in the later stages of data analysis, as the strategies might be adjusted along the way. Tips: It always worths studying the documentations about the original data in more details. This will help you get a better understand of the data: what the number unit is, what the possible values of the data are, any natural relation presents and so on. Store the data as different objects from time to time. This helps to keep tracks for future reference and debug. Column modification is basically mutate_* functions with some functions to modify the columns. Simple functions without arguments are easy to use within the mutate_* functions. For complex modification involving multiple functions, it is recommended to define a new function which only requires the column values as the input. Try to make your code more generic, so that it can be applied to similar data with minimal modification. Reference: [R for Data Science] https://r4ds.had.co.nz/ [edav.info/] https://edav.info/index.html "],
["all-about-dataframes.html", "Chapter 5 All About Dataframes 5.1 Create Data Frames 5.2 Get information on the dataframe 5.3 Concatenate dataframes 5.4 Order dataframes 5.5 Subset of data tables 5.6 Change dataframe shape 5.7 Transforming data 5.8 Dealing with duplicates and missing values 5.9 group_by function", " Chapter 5 All About Dataframes Thomas Causero The purpose of this cheatsheet is to present functions to deal with dataframes. To do so, I’ll use 2 different ways: one that doesn’t require any additional libraries and one using two libraries (dplyr and tidyr). Note that the tidyverse library contains those two libraries among others. So, one can decide to load dplyr and tidyr sperately or load all of them at the same time using tidyverse. #library(tidyverse) library(dplyr) library(tidyr) 5.1 Create Data Frames There are several ways to create a dataframe using R. The first one is to create a dataframe with vectors as inputs for every column, to do so, you need to use the following syntax: data.frame(x = vector_1, y = vector_2,… ,col_name = vector_n). To avoid writing too long vectors, you might also use random data or random sampling: rnorm(k) will return a k random samples from a standard normal distribution runif(k) will return a k random samples from a [0,1] uniform distribution rpois(k, lambda) for a poison distribution rbinom(k, n, p) for a binomial distribution Another way to create a dataframe is to use external data, either from a csv file, or using web scrapping techniques. To do so, there is a very useful package rvest, which is rather easy to use and which has a function html_table() which convert the html table into a dataframe. In the same way, the “read_csv” function takes a csv file as input and ouput a dataframe. Note that for these functions, there are a lot of parameters such as header which enables to control which rows or columns we would like to keep. To have more information on an R function, you can just tap ?function_name. Another useful function is write_csv(), which is rather explicit. Here are a few other functions that can be useful to deal with data: to create a vector: 1:10 or c(1:10) to repeat 5 10 times: rep(5,10) it also works with strings: rep(‘hey’,10) repeat hey 10 times rep(c(1,2,3),2) will repeat the vector 1:3 twice rep(c(1,2,3), each = 2) will repeat each value of the vector twice #dataframe with random numbers df &lt;- data.frame(x = rnorm(5), y = rnorm(5), z = rnorm(5)) df ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 2 0.18470422 0.3790845 -0.25926828 ## 3 -0.03302085 -0.2270535 0.39218406 ## 4 1.33592831 0.7596725 -0.67016045 ## 5 0.28714408 -1.1602793 -0.09767826 5.2 Get information on the dataframe When dealing with dataframes, it is very useful to first look at its characteristic such as the shape or the type of data we are dealing with, here are the functions to do so. #No additional libraries required #To look at first rows (here only the first one) head(df,1) ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 #get number of columns ncol(df) ## [1] 3 #get number of rows nrow(df) ## [1] 5 #get dimensions dim(df) ## [1] 5 3 #get information on each column str(df) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ x: num -0.0654 0.1847 -0.033 1.3359 0.2871 ## $ y: num 0.226 0.379 -0.227 0.76 -1.16 ## $ z: num -0.1942 -0.2593 0.3922 -0.6702 -0.0977 #There are three main data types: numeric, string and factors (very similar to vectors #but very useful with nominal data because it also provides the set of possible values) factor(c(&#39;Tomato&#39;,&#39;Tomato&#39;,&#39;Tomato&#39;,&#39;Tomato&#39;,&#39;Tomato&#39;,&#39;Apple&#39;,&#39;Apple&#39;,&#39;Apple&#39;,&#39;Apple&#39;)) ## [1] Tomato Tomato Tomato Tomato Tomato Apple Apple Apple Apple ## Levels: Apple Tomato #get column names colnames(df) ## [1] &quot;x&quot; &quot;y&quot; &quot;z&quot; #get row names rownames(df) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; #to change row names or column names with a vector colnames(df) &lt;- c(&#39;a&#39;,&#39;y&#39;,&#39;z&#39;) #change only one column name without needing to write all the others colnames(df)[colnames(df)==&quot;a&quot;] &lt;- &#39;x&#39; #with the column name colnames(df)[1]&lt;-&quot;x&quot; #with the column number #Using dplyr and tidyr #you can rename variables with select but it will drop all the variables not explicitly mentioned select(df, new = x) ## new ## 1 -0.06539989 ## 2 0.18470422 ## 3 -0.03302085 ## 4 1.33592831 ## 5 0.28714408 #rename() is more useful rename(df, new = x) ## new y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 2 0.18470422 0.3790845 -0.25926828 ## 3 -0.03302085 -0.2270535 0.39218406 ## 4 1.33592831 0.7596725 -0.67016045 ## 5 0.28714408 -1.1602793 -0.09767826 5.3 Concatenate dataframes When dealing with dataframes, it can also be very useful to concatenate them. To do so, there are several functions such as rbind or cbind. In this section, I will also present functions to merge dataframe (depending on a specific key) #columns by columns cbind(1:3,1:3) ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 #rows by rows rbind(1:3,1:3) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 3 #apply multiple concatenations list &lt;- list(1:3, 1:3, 1:3, 1:3, 1:3) do.call(rbind, list) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 3 ## [3,] 1 2 3 ## [4,] 1 2 3 ## [5,] 1 2 3 df1 &lt;- data.frame(ID = 1:5, value = 5:1) df2 &lt;- data.frame(ID = 1:5, value = 6:10) #to merge with a single key merge(df1, df2, by = &#39;ID&#39;) ## ID value.x value.y ## 1 1 5 6 ## 2 2 4 7 ## 3 3 3 8 ## 4 4 2 9 ## 5 5 1 10 #can also merge with multiple ids: by = c(col1, col2 ...) #merge with a key that has a different name in datasets df3 &lt;- data.frame(ID3 = 1:5, value = 6:10) merge(df1, df3, by.x = &#39;ID&#39;, by.y = &#39;ID3&#39;) ## ID value.x value.y ## 1 1 5 6 ## 2 2 4 7 ## 3 3 3 8 ## 4 4 2 9 ## 5 5 1 10 5.4 Order dataframes Ordering a dataframe means that you want to reorder the rows depending on the value of a specific colum. You can also change the order of the columns. #No additional libraries required #order rows by variable x then y df[order(df$x, df$y),] ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 3 -0.03302085 -0.2270535 0.39218406 ## 2 0.18470422 0.3790845 -0.25926828 ## 5 0.28714408 -1.1602793 -0.09767826 ## 4 1.33592831 0.7596725 -0.67016045 df[order(df[&#39;x&#39;]),] ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 3 -0.03302085 -0.2270535 0.39218406 ## 2 0.18470422 0.3790845 -0.25926828 ## 5 0.28714408 -1.1602793 -0.09767826 ## 4 1.33592831 0.7596725 -0.67016045 df[order(&#39;x&#39;)] #doesn&#39;t return the whole dataframe (only the column x) and nothing is sorted ## x ## 1 -0.06539989 ## 2 0.18470422 ## 3 -0.03302085 ## 4 1.33592831 ## 5 0.28714408 df[order(&#39;x&#39;),] #return the first row of the dataframe and not sorted neither ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 #change columns order (use a vector) df[,c(1,3,2)] ## x z y ## 1 -0.06539989 -0.19421678 0.2256787 ## 2 0.18470422 -0.25926828 0.3790845 ## 3 -0.03302085 0.39218406 -0.2270535 ## 4 1.33592831 -0.67016045 0.7596725 ## 5 0.28714408 -0.09767826 -1.1602793 #Using libraries dplyr and tidyr #arrange() to reorder the dataframe arrange(df, -x,y,z) ## x y z ## 1 1.33592831 0.7596725 -0.67016045 ## 2 0.28714408 -1.1602793 -0.09767826 ## 3 0.18470422 0.3790845 -0.25926828 ## 4 -0.03302085 -0.2270535 0.39218406 ## 5 -0.06539989 0.2256787 -0.19421678 arrange(df, x,y,z) ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 2 -0.03302085 -0.2270535 0.39218406 ## 3 0.18470422 0.3790845 -0.25926828 ## 4 0.28714408 -1.1602793 -0.09767826 ## 5 1.33592831 0.7596725 -0.67016045 5.5 Subset of data tables Getting subset of a dataframe is very important. Those functions are like filters in a way that they will only return specific rows (or columns) of a dataframe depending on its value. #For all those functions, no additional libraries are required #Only return specific row and column numbers #df[row_numbers, col_numbers] df[1:4,1:3] ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 ## 2 0.18470422 0.3790845 -0.2592683 ## 3 -0.03302085 -0.2270535 0.3921841 ## 4 1.33592831 0.7596725 -0.6701605 #trake 2 random rows from df df[sample(1:nrow(df), 2, replace = F),] ## x y z ## 3 -0.03302085 -0.2270535 0.39218406 ## 5 0.28714408 -1.1602793 -0.09767826 #get 3 first rows df[1:3,] ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 ## 2 0.18470422 0.3790845 -0.2592683 ## 3 -0.03302085 -0.2270535 0.3921841 #remove rows 1 and 2 df[-c(1,2),] ## x y z ## 3 -0.03302085 -0.2270535 0.39218406 ## 4 1.33592831 0.7596725 -0.67016045 ## 5 0.28714408 -1.1602793 -0.09767826 #select columns df[,c(&#39;x&#39;,&#39;y&#39;)] ## x y ## 1 -0.06539989 0.2256787 ## 2 0.18470422 0.3790845 ## 3 -0.03302085 -0.2270535 ## 4 1.33592831 0.7596725 ## 5 0.28714408 -1.1602793 #select rows under condition of a certain column df[df$x&lt;0,] #df[df$col %in% c(&#39;var1&#39;,&#39;var2&#39;,&#39;var3&#39;)] ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 ## 3 -0.03302085 -0.2270535 0.3921841 subset(df, x&gt;0 &amp; y&gt;0) ## x y z ## 2 0.1847042 0.3790845 -0.2592683 ## 4 1.3359283 0.7596725 -0.6701605 #Now, I&#39;ll show similar functions using additional libraries (dplyr and tidyr) #sample_n() (fixed number) and sample_frac() to take random samples (fixed fraction) sample_n(df, 1) ## x y z ## 1 -0.03302085 -0.2270535 0.3921841 sample_frac(df, 0.5) ## x y z ## 1 -0.06539989 0.2256787 -0.1942168 ## 2 -0.03302085 -0.2270535 0.3921841 #filter() to select cases based on their values. filter(df, x &gt;0 &amp; y&gt;0) ## x y z ## 1 0.1847042 0.3790845 -0.2592683 ## 2 1.3359283 0.7596725 -0.6701605 df %&gt;% filter(x&gt;0, y&gt;0) ## x y z ## 1 0.1847042 0.3790845 -0.2592683 ## 2 1.3359283 0.7596725 -0.6701605 #select() to select variables based on their names. #select column x select(df, x) ## x ## 1 -0.06539989 ## 2 0.18470422 ## 3 -0.03302085 ## 4 1.33592831 ## 5 0.28714408 #select column x and y select(df, x, y) ## x y ## 1 -0.06539989 0.2256787 ## 2 0.18470422 0.3790845 ## 3 -0.03302085 -0.2270535 ## 4 1.33592831 0.7596725 ## 5 0.28714408 -1.1602793 #select all columns from x to z select(df, x:z) ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 2 0.18470422 0.3790845 -0.25926828 ## 3 -0.03302085 -0.2270535 0.39218406 ## 4 1.33592831 0.7596725 -0.67016045 ## 5 0.28714408 -1.1602793 -0.09767826 #drop colum x select(df, -x) ## y z ## 1 0.2256787 -0.19421678 ## 2 0.3790845 -0.25926828 ## 3 -0.2270535 0.39218406 ## 4 0.7596725 -0.67016045 ## 5 -1.1602793 -0.09767826 #drop columns x and y select(df, -x, -y) ## z ## 1 -0.19421678 ## 2 -0.25926828 ## 3 0.39218406 ## 4 -0.67016045 ## 5 -0.09767826 5.6 Change dataframe shape #Without additional libraries #to transpose a dataframe (rows become columns and inversely) t(df) ## [,1] [,2] [,3] [,4] [,5] ## x -0.06539989 0.1847042 -0.03302085 1.3359283 0.28714408 ## y 0.22567875 0.3790845 -0.22705346 0.7596725 -1.16027931 ## z -0.19421678 -0.2592683 0.39218406 -0.6701605 -0.09767826 #Using tidyr library #gather and spread #gathering the column names and turning them into a pair of new variables. #One variable represents the column names as values, and the other variable #contains the values previously associated with the column names tmp &lt;- data.frame(col1 = 1:3, col2 = 1:3, col3 = 1:3) df_gather &lt;- gather(tmp, key = &#39;col&#39;, value = &#39;value&#39;, -col1) df_gather ## col1 col value ## 1 1 col2 1 ## 2 2 col2 2 ## 3 3 col2 3 ## 4 1 col3 1 ## 5 2 col3 2 ## 6 3 col3 3 #spread to get initial dataset #spread is the inverse function of gather spread(df_gather, key = col, value = value) ## col1 col2 col3 ## 1 1 1 1 ## 2 2 2 2 ## 3 3 3 3 5.7 Transforming data Transforming data is also very important. Indeed, when one creates a new column depending on the value of others, or if one would like to change the values of a specific column, then it transforms the dataframe. #No additional libraries required #multiply by 100 the column x transform(df, x =100*x) ## x y z ## 1 -6.539989 0.2256787 -0.19421678 ## 2 18.470422 0.3790845 -0.25926828 ## 3 -3.302085 -0.2270535 0.39218406 ## 4 133.592831 0.7596725 -0.67016045 ## 5 28.714408 -1.1602793 -0.09767826 #multiply x by 100 if z&gt;0 transform(df, x = ifelse(z&gt;0, x*100, x)) ## x y z ## 1 -0.06539989 0.2256787 -0.19421678 ## 2 0.18470422 0.3790845 -0.25926828 ## 3 -3.30208519 -0.2270535 0.39218406 ## 4 1.33592831 0.7596725 -0.67016045 ## 5 0.28714408 -1.1602793 -0.09767826 #create a new column with a value of 1 or -1 depending on the sign of x transform(df, new_col = ifelse(x&gt;0, 1, -1)) ## x y z new_col ## 1 -0.06539989 0.2256787 -0.19421678 -1 ## 2 0.18470422 0.3790845 -0.25926828 1 ## 3 -0.03302085 -0.2270535 0.39218406 -1 ## 4 1.33592831 0.7596725 -0.67016045 1 ## 5 0.28714408 -1.1602793 -0.09767826 1 #create new colum (there are several ways to do so) df[&#39;w&#39;] = 2*df[&#39;x&#39;] #create a colum w whose values are twice the value of x df[,&#39;r&#39;] = ifelse(df[[&#39;x&#39;]]&gt;0, 1, -1) #create a column r whose values are 1 or -1 #depending on the value of x #note that there are [[]] and not []) - df[,&#39;r&#39;] = ifelse(df[&#39;x&#39;]&gt;0, 1, -1) doesn&#39;t work df[&#39;t&#39;] = ifelse(df[,&#39;x&#39;]&gt;0, 1, -1) df$o = ifelse(df[,&#39;x&#39;]&gt;0, 1, -1) df$i = ifelse(df$x&gt;0, 1, -1) df ## x y z w r t o i ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 #Note the following (there is a difference between [] and [[]]) df[&#39;x&#39;] #is a dataframe ## x ## 1 -0.06539989 ## 2 0.18470422 ## 3 -0.03302085 ## 4 1.33592831 ## 5 0.28714408 df[,&#39;x&#39;] #is a vector ## [1] -0.06539989 0.18470422 -0.03302085 1.33592831 0.28714408 df$x #is a vector ## [1] -0.06539989 0.18470422 -0.03302085 1.33592831 0.28714408 df[[&#39;x&#39;]] #is a vector ## [1] -0.06539989 0.18470422 -0.03302085 1.33592831 0.28714408 #Using dplyr and tidyr libraries #mutate() and transmute() to add new variables that are functions of existing variables. #dplyr::mutate() is similar to transform(), but allows you to refer to columns that you’ve just created mutate(df,a = 2*x,b = 2*y) ## x y z w r t o i a ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 -0.1307998 ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 0.3694084 ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 -0.0660417 ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 2.6718566 ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 0.5742882 ## b ## 1 0.4513575 ## 2 0.7581690 ## 3 -0.4541069 ## 4 1.5193451 ## 5 -2.3205586 mutate(df,a = 2*x,b = 2*a) ## x y z w r t o i a ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 -0.1307998 ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 0.3694084 ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 -0.0660417 ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 2.6718566 ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 0.5742882 ## b ## 1 -0.2615996 ## 2 0.7388169 ## 3 -0.1320834 ## 4 5.3437132 ## 5 1.1485763 #transform(df, a = 2*X, b = 2*a) #doesn&#39;t work #If you only want to keep the new variables, use transmute(): transmute(df,a = 2*x,b = 2*a) ## a b ## 1 -0.1307998 -0.2615996 ## 2 0.3694084 0.7388169 ## 3 -0.0660417 -0.1320834 ## 4 2.6718566 5.3437132 ## 5 0.5742882 1.1485763 5.8 Dealing with duplicates and missing values NA values or duplicate values can be very annoying in a dataframe. Hopefully, some functions exist to deal with them. #create NA values to show the different functions df[&#39;NA&#39;] = NA df ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 NA ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 NA ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 NA ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 NA ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 NA df[is.na(df)] = 0 #set NA values to 0 df ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 0 ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 0 ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 0 ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 0 ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 0 df[df==0] = NA #set 0 values to NA df ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 NA ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 NA ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 NA ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 NA ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 NA na.omit(df) #delete rows with NA values ## [1] x y z w r t o i NA ## &lt;0 rows&gt; (or 0-length row.names) df[!is.na(df$x),] #delete rows with missing values in column x ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 NA ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 NA ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 NA ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 NA ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 NA unique(df) #remove duplicates rows ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 NA ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 NA ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 NA ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 NA ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 NA df[duplicated(df),] #return duplicate rows ## [1] x y z w r t o i NA ## &lt;0 rows&gt; (or 0-length row.names) df[!duplicated(df[,c(&#39;x&#39;)]),] #remove all rows with duplicates X values (first is kept) ## x y z w r t o i NA ## 1 -0.06539989 0.2256787 -0.19421678 -0.1307998 -1 -1 -1 -1 NA ## 2 0.18470422 0.3790845 -0.25926828 0.3694084 1 1 1 1 NA ## 3 -0.03302085 -0.2270535 0.39218406 -0.0660417 -1 -1 -1 -1 NA ## 4 1.33592831 0.7596725 -0.67016045 2.6718566 1 1 1 1 NA ## 5 0.28714408 -1.1602793 -0.09767826 0.5742882 1 1 1 1 NA df[duplicated(df[,c(&#39;x&#39;)]),] #returns rows with duplicate X ## [1] x y z w r t o i NA ## &lt;0 rows&gt; (or 0-length row.names) 5.9 group_by function The group_by function is used to create groups of observations in order to apply functions to each group separately. It is equivalent to the GROUP BY function is SQL. You need the dplyr package for this section. library(nycflights13) #summarise() to condense multiple values to a single value #It collapses a data frame to a single row #very useful with group by #examples of functions: min(), max(), mean(), sum(), sd(), median() #n(): the number of observations in the current group or n_distinct(x):the number of unique values in x summarise(df,name_col = mean(x, na.rm = TRUE)) ## name_col ## 1 0.3418712 #group_by() + summarize() flights %&gt;% group_by(tailnum) %&gt;% summarise(count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE)) %&gt;% filter(delay, count &gt; 500, dist &lt; 800) ## # A tibble: 3 x 4 ## tailnum count dist delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 N722MQ 513 546. 4.91 ## 2 N723MQ 507 538. 6.42 ## 3 N725MQ 575 559. 4.67 "],
["dplyr-relational-databases.html", "Chapter 6 Dplyr Relational Databases 6.1 1.Overview 6.2 2.Definition of Relational Databases 6.3 3. R Packages 6.4 4. Data description for example 6.5 5. Types of joins", " Chapter 6 Dplyr Relational Databases Alberto Munguia and Chengyi Chen 6.1 1.Overview This project intends to illustrate the use of some libarries that are relevant to manipulate and transform relational databases. 6.2 2.Definition of Relational Databases A relational database is a type of database that stores and provides access to data points that are related to one another. Relational databases are based on the relational model, an intuitive, straightforward way of representing data in tables. The relational model organizes data into one or more tables (or “relations”) of columns and rows, with a unique key identifying each row. Key plays an important role in relational database. Primary Key a.. A primary is a column or set of columns in a table that uniquely identifies tuples (rows) in that table. Super Key a.. A super key is a set of one or more columns (attributes) to uniquely identify rows in a table. Candidate Key a.. A super key with no redundant attribute is known as a candidate key. Alternate Key a.. Out of all candidate keys, only one gets selected as primary key, remaining keys are known as alternate or secondary keys. Composite Key a.. A key that consists of more than one attribute to uniquely identify rows (also known as records &amp; tuples) in a table is called composite key. Foreign Key a.. Foreign keys are the columns of a table that points to the primary key of another table. They act as a cross-reference between tables. Standard relational databases enable users to manage predefined data relationships across multiple databases. Popular examples of relational databases include Microsoft SQL Server, Oracle Database, MySQL and IBM DB2. 6.3 3. R Packages R offers flexibility in the manipulation of relational of databases through some specific functions embedded in the packages like: dplyr base sqldf Nevertheless, the data manipulation in R is easier with dplyr because the package is oriented towards the data analysis. Furthemore, dplyr offers some advantages in the join functions in comaprison with base and sqldf: For large amounts of data joining tables is faster. Rows are kept in existing order. Tells users what keys you are merging by. Work with database tables. For our example you need to install the next packages: For the dplyr manipulation. install.packages('dplyr') For the SQL manipulation. install.packages('sqldf') install.packages('gsubfn') install.packages('proto') install.packages('RSQLite') For getting the data for the example install.packages('BIS') Load the libraries library(dplyr) library(sqldf) library(gsubfn) library(proto) library(RSQLite) library(BIS) 6.4 4. Data description for example 6.4.1 4.1 BIS Library We will explore relational data from the recent data package BIS that provides an interface to data provided by the Bank for International Settlements https://www.bis.org, allowing for programmatic retrieval of a large quantity of (central) banking data. 6.4.2 4.2 Selected data sets First, we are going to upload the tables that are available in the BIS package. datasets=get_datasets() datasets ## # A tibble: 24 x 2 ## name url ## &lt;chr&gt; &lt;chr&gt; ## 1 Locational banking statistics https://www.bis.org/statistics/full_bis_l… ## 2 Consolidated banking statistics https://www.bis.org/statistics/full_bis_c… ## 3 Debt securities statistics https://www.bis.org/statistics/full_bis_d… ## 4 Credit to the non-financial sector https://www.bis.org/statistics/full_bis_t… ## 5 Credit-to-GDP gaps https://www.bis.org/statistics/full_webst… ## 6 Debt service ratios for the priva… https://www.bis.org/statistics/full_bis_d… ## 7 Global liquidity indicators https://www.bis.org/statistics/full_bis_g… ## 8 Exchange-traded derivatives stati… https://www.bis.org/statistics/full_bis_x… ## 9 OTC derivatives outstanding https://www.bis.org/statistics/full_bis_o… ## 10 US dollar exchange rates (monthly… https://www.bis.org/statistics/full_webst… ## # … with 14 more rows For the purpose of our excercise we have choose tables 6 and 18 from the BIS data set. Table 6 corresponds to the quarterly data of the Debt service ratios for the private non-financial sector for the countries that are part of the BIS. In order to facilite our example we are going to filter our data and only keep the quaterly information that corresponds to the Private non-financial sector. Debt_service=get_bis(datasets$url[6]) glimpse(Debt_service) ## Observations: 5,334 ## Variables: 8 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;BE&quot;, &quot;BE&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CA… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Belgium&quot;, &quot;… ## $ dsr_borrowers &lt;chr&gt; &quot;H&quot;, &quot;N&quot;, &quot;P&quot;, &quot;H&quot;, &quot;N&quot;, &quot;P&quot;, &quot;P&quot;, &quot;H&quot;, &quot;N&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Households &amp; NPISHs&quot;, &quot;Non-financial corporations&quot;… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value &lt;dbl&gt; 10.0, 44.3, 16.3, 6.1, 36.3, 13.8, 40.0, 10.6, 58.0… Debt_service_filter=filter(Debt_service, Debt_service$dsr_borrowers==&quot;P&quot;) glimpse(Debt_service_filter) ## Observations: 2,580 ## Variables: 8 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;, &quot;DK… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Switze… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0, 21.… Table 18 corresponds to the quarterly data of the Property prices: long series for the countries that are part of the BIS. Property_prices=get_bis(datasets$url[18]) glimpse(Property_prices) ## Observations: 4,526 ## Variables: 6 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Q… ## $ ref_area &lt;chr&gt; &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, … ## $ reference_area &lt;chr&gt; &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, … ## $ date &lt;chr&gt; &quot;1927-q1&quot;, &quot;1927-q2&quot;, &quot;1927-q3&quot;, &quot;1927-q4&quot;, &quot;1928-q1&quot;,… ## $ obs_value &lt;dbl&gt; 0.0343, 0.0342, 0.0340, 0.0339, 0.0338, 0.0336, 0.0334… The key to joining our two tables are going to be the code of the country and the date. 6.5 5. Types of joins 6.5.1 5.1 Left_join Select all records from Table A, along with records from Table B for which the join condition is met (if at all). In our case Table A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table after the left join will keep all the records of Debt_service_filter. Additionally, we can observe that the columns where the name is the same in the tables Debt_service_filter and Property_prices appears with the suffix ‘.x’ and ‘.y’. to clarify the origin of the column. In the case of SQL, we need to highlight that the join function results in 14 columns, two more than in merge and left_join. This is explained because SQL leaves the totality of the columns while in the other two procedures the keys are not repeated. Left join functionality with dplyr. leftjoin_dplyr=left_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(leftjoin_dplyr) ## Observations: 2,580 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;, &quot;DK… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Switze… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0, 21.… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Quarter… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzerland&quot;… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA, NA, … Left join functionality with base. leftjoin_base=merge(Debt_service_filter, Property_prices, all.x = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(leftjoin_base) ## Observations: 2,580 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q2&quot;, &quot;1999-q3&quot;, &quot;1999-q4&quot;, &quot;2000-q… ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ obs_value.x &lt;dbl&gt; 16.3, 16.2, 16.4, 16.8, 17.3, 17.9, 17.9, 18.1, 17.… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ obs_value.y &lt;dbl&gt; 116.5571, 119.2170, 121.4370, 125.4269, 127.7368, 1… Left join functionality with sqldf. leftjoin_sqldf=sqldf(&quot;select * from Debt_service_filter LEFT JOIN Property_prices on Debt_service_filter.borrowers_cty = Property_prices.ref_area and Debt_service_filter.date = Property_prices.date&quot;) glimpse(leftjoin_sqldf) ## Observations: 2,580 ## Variables: 14 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;, &quot;DK… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Switze… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0, 21.… ## $ freq..9 &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency..10 &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Quarter… ## $ ref_area &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, NA, &quot;CA&quot;, &quot;CH&quot;, NA, NA, &quot;DE&quot;, &quot;DK&quot;, &quot;ES… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzerland&quot;… ## $ date..13 &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, NA, &quot;1999-q1&quot;, &quot;1999-q1&quot;, NA,… ## $ obs_value..14 &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA, NA, … 6.5.2 5.2. Right_join Select all records from Table B, along with records from Table A for which the join condition is met (if at all). In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table after the right join will keep all the records of Property_prices. Right join functionality with dplyr. rightjoin_dplyr=right_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(rightjoin_dplyr) ## Observations: 4,526 ## Variables: 12 ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers_cty &lt;chr&gt; &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ date &lt;chr&gt; &quot;1927-q1&quot;, &quot;1927-q2&quot;, &quot;1927-q3&quot;, &quot;1927-q4&quot;, &quot;1928-q… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy… ## $ obs_value.y &lt;dbl&gt; 0.0343, 0.0342, 0.0340, 0.0339, 0.0338, 0.0336, 0.0… Right join functionality with base rightjoin_base=merge(Debt_service_filter, Property_prices, all.y = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(rightjoin_base) ## Observations: 4,526 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU… ## $ date &lt;chr&gt; &quot;1970-q1&quot;, &quot;1970-q2&quot;, &quot;1970-q3&quot;, &quot;1970-q4&quot;, &quot;1971-q… ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ obs_value.y &lt;dbl&gt; 9.8398, 10.0197, 10.2997, 10.6197, 10.9197, 11.1697… Right join functionality with sqldf is not supported. 6.5.3 5.3. Inner_join Select all records from Table A and Table B, where the join condition is met. In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table will keep only the rows in the intersection. Inner join functionality with dplyr. innerjoin_dplyr=inner_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(innerjoin_dplyr) ## Observations: 1,664 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;, &quot;FR… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland&quot;, &quot;G… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 21.4, 16.8, 13.0, 21.1, 11.6, 13.3, 15.… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland&quot;, &quot;G… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, 101.6600, 90.9073, 95.9194, 140… Inner join functionality with base. innerjoin_base=merge(Debt_service_filter, Property_prices, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(innerjoin_base) ## Observations: 1,664 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q2&quot;, &quot;1999-q3&quot;, &quot;1999-q4&quot;, &quot;2000-q… ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ obs_value.x &lt;dbl&gt; 16.3, 16.2, 16.4, 16.8, 17.3, 17.9, 17.9, 18.1, 17.… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ obs_value.y &lt;dbl&gt; 116.5571, 119.2170, 121.4370, 125.4269, 127.7368, 1… Inner join functionality with sqldf. innerjoin_sqldf=sqldf(&quot;select * from Debt_service_filter INNER JOIN Property_prices on Debt_service_filter.borrowers_cty = Property_prices.ref_area and Debt_service_filter.date = Property_prices.date &quot;) glimpse(innerjoin_sqldf) ## Observations: 1,664 ## Variables: 14 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;, &quot;FR… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland&quot;, &quot;G… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 21.4, 16.8, 13.0, 21.1, 11.6, 13.3, 15.… ## $ freq..9 &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency..10 &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ ref_area &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;, &quot;FR… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland&quot;, &quot;G… ## $ date..13 &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value..14 &lt;dbl&gt; 116.5571, 115.9600, 101.6600, 90.9073, 95.9194, 140… 6.5.4 5.4. Full_join Select all records from Table A and Table B, where the join condition is met. In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table will keep all the rows in both tables. Full join functionality with dplyr. outerjoin_dplyr=full_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(outerjoin_dplyr) ## Observations: 5,442 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;, &quot;DK… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Switze… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-financ… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0, 21.… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Quarter… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzerland&quot;… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA, NA, … Full join functionality with base. outerjoin_base=merge(Debt_service_filter, Property_prices, all.y = TRUE, all.x = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(outerjoin_base) ## Observations: 5,442 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU… ## $ date &lt;chr&gt; &quot;1970-q1&quot;, &quot;1970-q2&quot;, &quot;1970-q3&quot;, &quot;1970-q4&quot;, &quot;1971-q… ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;,… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;,… ## $ obs_value.y &lt;dbl&gt; 9.8398, 10.0197, 10.2997, 10.6197, 10.9197, 11.1697… Full join functionality with sqldf is not supported. "],
["web-scraping-using-rvest.html", "Chapter 7 Web scraping using rvest 7.1 1 Overview 7.2 2 An Easy Example 7.3 3 HTML Basics 7.4 4 Rvest 7.5 5 More Examples 7.6 6 External Resources", " Chapter 7 Web scraping using rvest Huiyu Song and Xiao Ji 7.1 1 Overview This section covers how to conduct web scraping using “rvest” package 7.2 2 An Easy Example I want an example now! Here is an example of scraping the price and percentage change of trending stocks from Yahoo Finance: https://finance.yahoo.com/trending-tickers. The first thing we need to do is to check if scraping is permitted on this page using paths_allowed( ) function. library(robotstxt) paths_allowed(paths=&quot;https://finance.yahoo.com/trending-tickers&quot;) ## [1] TRUE The output is TRUE meaning that bots are allowed to access this path. Now we can scrape the data: library(rvest) TrendTicker &lt;- read_html(&quot;https://finance.yahoo.com/trending-tickers&quot;) #read the path #We need Name, Last Price, % Change Name &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col1&quot;)%&gt;%html_text() Price &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col2&quot;)%&gt;%html_text() Change &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col5&quot;)%&gt;%html_text() dt&lt;-tibble(Name,Price,Change) #combine the scrapped columns into a tibble head(dt,5) ## # A tibble: 5 x 3 ## Name Price Change ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AT&amp;T Inc. 38.11 -3.83% ## 2 The Home Depot, Inc. 226.01 -5.37% ## 3 Kohl&#39;s Corporation 47.24 -19.11% ## 4 Myovant Sciences Ltd. 12.54 +106.93% ## 5 CRISPR Therapeutics AG 67.88 +15.97% Path %&gt;% html_nodes( ) %&gt;% html_text( ) is a common syntax to scrape html text and more details will be discussed in section 4. Before that, we need some basic knowledge of HTML structures. 7.3 3 HTML Basics 7.3.1 3.1 Access the source code Move your cursor to the element whose source code you want to check and right click. Select “Inspect” The source code will be displayed on the top right corner of the screen. 7.3.2 3.2 HTML structures HTML is a markup language and it describes the structure of a Web page. A simple element in HTML looks like this: div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} &lt;p&gt;This is a paragraph.&lt;/p&gt; An HTML element usually consistes of a start tag, a end tag and the content in between. Here &lt;p&gt; is the start tag, &lt;/p&gt; is the end tag (the slash indicates that it is a closing tag), “This is a paragraph” is the content. The charater “p” represents it is a paragraph element, other kinds of elements include: div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} &lt;html&gt;: the root element of an HTML page &lt;head&gt;: an element contains meta information about the document &lt;title&gt;: an element specifies a title for the document &lt;body&gt;: an element contains the visible page content &lt;h1&gt;: an element defines a large heading &lt;p&gt;: an element defines a paragraph The basic structure of a webpage looks like this: More details can be refered to https://www.w3schools.com/html/html_intro.asp 7.4 4 Rvest When we want to scrape certain information from a website, we need to concentrate on the part that we are interested in instead of the whole page. That is why we need html_node or html_nodes to locate the interested part. 7.4.1 4.1 html_nodes and html_node Usage html_nodes(x,css,xpath) html_node(x,css,xpath) Arguments x: a node set or a single node css, xpath: Node to select css:CSS selector; xpath:XPath 1.0 selector html_node VS html_nodes Html_nodes always return a nodeset of the same length, which contains information of a set of nodes. While html_node return exactly one html_node. Here is an example: paths_allowed(&quot;https://www.eyeconic.com/&quot;) ## [1] TRUE page=read_html(&quot;https://www.eyeconic.com/contact-lenses?cid=ps:google:Eyeconic+-+US+-+SKWS+-+Contacts+-+General+-+Exact+-+Geo:NB+-+Contacts+-+Onlineutm_campaign=skws&amp;ds_rl=1239071&amp;gclid=EAIaIQobChMImpP2gqW95QIVipOzCh1XfwKbEAAYAiAAEgLWrfD_BwE&amp;gclsrc=aw.ds&quot;) page2=read_html(&quot;https://www.eyeconic.com/contact-lenses/aot/AOT.html&quot;) node&lt;- page%&gt;%html_node(xpath=&#39;//*[@id=&quot;search-result-items&quot;]/li[1]&#39;) nodes&lt;-page%&gt;%html_nodes(xpath=&#39;//*[@id=&quot;search-result-items&quot;]/li[1]&#39;) node ## {html_node} ## &lt;li class=&quot;grid-tile col-md-6 col-xl-4 pb-3 px-1 px-md-2&quot; data-colors-to-show=&quot;&quot;&gt; ## [1] &lt;script type=&quot;text/javascript&quot;&gt;//&lt;!--\\n/* &lt;![CDATA[ */\\n(function(){\\ntry ... ## [2] &lt;script type=&quot;text/javascript&quot;&gt;//&lt;!--\\n/* &lt;![CDATA[ (viewProduct-active_d ... ## [3] &lt;div class=&quot;product-tile w-100 m-auto text-center pt-5 bg-white position- ... nodes ## {xml_nodeset (1)} ## [1] &lt;li class=&quot;grid-tile col-md-6 col-xl-4 pb-3 px-1 px-md-2&quot; data-colors-to- ... 7.4.2 4.2 css and xpath Although the usage of html_nodes and html_node seems easy and convinient, for those who cannot extract right css or xpath, the function will not work. Here is a summary of how to write css or xpath, and some examples are shown. css CSS Selector are how you pick which element to apply styles to. Selector Syntax Pattern Meaning p Select all &lt;p\\&gt; elements p m Select all &lt;m\\&gt; inside of &lt;p&gt; p &gt; m Select an direct child &lt;m&gt; of &lt;p&gt; p + m Select an &lt;m&gt; that directly follows &lt;p&gt; p ~ m Select &lt;m&gt; that preceds by &lt;p&gt; p#id_name Select all &lt;p&gt; which id=“id_name” p.class_name Select all &lt;p&gt; which class=“class_name” p[attr] Select &lt;p&gt; that has “attr” attribute p[attr=“tp”] Select &lt;p&gt; that attribute attr=“tp” p[attr~=“tp”] Select &lt;p&gt; that attribute “attr” is a list of whitespace-seperated values, and one of which is “tp” p[attr^=“tp”] Select p whose sttribute “attr” begins exactly with string “tp” p[attr*=“tp”] Select p whose sttribute “attr” contains string “tp” p[attr$=“tp”] Select p whose sttribute “attr” ends exactly with string “tp” p:root Select root of &lt;p&gt; p:nth-child(n) Select nth child of p p:nth-last-child(n) Select nth child from the bottom of p p:first-child Select first child of p p:last-child Select last child of p p:nth-of-type Select nth &lt;p&gt; in any element p:nth-last-type Select nth &lt;p&gt; from the bottom in any element p:first-of-type Select first &lt;p&gt; in any element p:last-of-type Select first &lt;p&gt; from the bottom in any element p:empty Select &lt;p&gt; that has no children p:link Select p which has not yet been visited p:visited Select p already been visited Examples p#id_character item Select any item inside p which has id=“id_character” Select name of all products. info&lt;-page%&gt;% html_nodes(&#39;ul#search-result-items li span[itemprop=&quot;name&quot;]&#39;)%&gt;% html_text() info[1:6] ## [1] &quot;ACUVUE&quot; &quot;ACUVUE&quot; &quot;AIR OPTIX&quot; &quot;ACUVUE&quot; &quot;BIOFINITY&quot; &quot;ACUVUE&quot; p.class_name Select &lt;p&gt; element which has class=“class_character”. Except id, we can also use class to concentrate on certain information. Select image path of all products. acuvue&lt;-page%&gt;% html_nodes(&#39;li.grid-tile.col-md-6.col-xl-4.pb-3.px-1.px-md-2 img[itemprop=&quot;image&quot;]&#39;) acuvue[1:2] ## {xml_nodeset (2)} ## [1] &lt;img class=&quot;w-100&quot; itemprop=&quot;image&quot; src=&quot;https://www.eyeconic.com/dw/imag ... ## [2] &lt;img class=&quot;w-100&quot; itemprop=&quot;image&quot; src=&quot;https://www.eyeconic.com/dw/imag ... Tips: when class name is long and has some white-spaces inside, such as “class=”product-tile w-100 m-auto text-center pt-5 bg-white position-relative“, in html class types, string between white spaces is one class, and if a class name has many whitespaces means it has many classes. Therefore to scrape those data, we need to add”.&quot; to substitute those whitespaces. A,B,C Select all &lt;A&gt;, &lt;B&gt;, &lt;C&gt; elements. Example: Scrape all product names and detail names in the page. name&lt;-page%&gt;% html_nodes(&#39;ul#search-result-items li span[itemprop=&quot;name&quot;], ul#search-result-items li div[itemprop=&quot;name&quot;]&#39;)%&gt;% html_text() name[1:6] ## [1] &quot;ACUVUE&quot; &quot;ACUVUE Oasys For Astigmatism 6pk&quot; ## [3] &quot;ACUVUE&quot; &quot;ACUVUE 1-Day Moist 90pk&quot; ## [5] &quot;AIR OPTIX&quot; &quot;Air Optix Aqua 6pk&quot; p * Select all elements in p. Select all nodes for price. img&lt;-page2%&gt;% html_nodes(&quot;div.price-info *&quot;) img ## {xml_nodeset (0)} p:nth-child(n) Select nth child of &lt;p&gt; air&lt;-page%&gt;% html_nodes(&quot;ul#search-result-items:nth-child(1)&quot;) air ## {xml_nodeset (1)} ## [1] &lt;ul id=&quot;search-result-items&quot; class=&quot;search-result-items tiles-container r ... Xpath XPath (XML Path Language) uses path expressions to select nodes or node-sets in an XML document. These path expressions look very much like the expressions you see when you work with a traditional computer file system. Xpath Syntax In XPath, there are seven kinds of nodes: element, attribute, text, namespace, processing-instruction, comment, and document nodes. For example: &lt;bookstore&gt; (root element node) &lt;author&gt;J K. Rowling&lt;/author&gt; (element node) lang=“en” (attribute node) Pattern Meaning nodename Select all node with the name “nodename” A/B Select B from root node A//B Select B in the document from the current node that match the selection no matter where they are .A Select the current node A ..A Select the root of current node A @ Select attributes * Matches any element node @* Matches any attribute node node() Matches any node of any kind ancestor Select all ancestors(parent, grandparent, stc.) of the current node ancestor-of-self Select all ancestors(parent, grandparent, stc.) of the current node and current node itself attribute Select all attributes of the current node child Select all children of the current node descendant Select all descendant(children, grandchildren, etc.) of the current node following Select everything in the document after the closing tag of the current node namespace Select all namespace nodes of the current node | Select two nodes A Simple Way the get XPath right click–&gt;Copy–&gt;Copy XPath Examples Extract all product details in the contact links. data &lt;- data.frame() info &lt;- page%&gt;% html_nodes(&#39;ul#search-result-items li div span[itemprop=&quot;url&quot;]&#39;) %&gt;% html_text() info[1:6] ## [1] &quot;https://www.eyeconic.com/contact-lenses/aot/AOT.html&quot; ## [2] &quot;https://www.eyeconic.com/contact-lenses/a1m9/A1M9.html&quot; ## [3] &quot;https://www.eyeconic.com/contact-lenses/aos/AOS.html&quot; ## [4] &quot;https://www.eyeconic.com/contact-lenses/acuvue/OTR.html&quot; ## [5] &quot;https://www.eyeconic.com/contact-lenses/bf/BF.html&quot; ## [6] &quot;https://www.eyeconic.com/contact-lenses/ao12/AO12.html&quot; 7.5 5 More Examples 7.5.1 5.1 Scrape links using attributes HTML links are defined with the tag &lt;a&gt;. The link address is specified in the “href” attribute. Suppose we want to get the link of each trend ticker, we can right click the stock symbol and check the source code: So we use “.data-col0 a”&quot; as the node and “href” as the attribute: local_links &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col0 a&quot;)%&gt;%html_attr(&quot;href&quot;) link_names &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col0 a&quot;)%&gt;%html_text(&quot;href&quot;) #complete the full link full_links=NULL for (i in 1 : length(local_links)){ full_links[i]=paste0(&quot;https://finance.yahoo.com&quot;,local_links[i]) } dt=tibble(link_names,full_links) head(dt,5) ## # A tibble: 5 x 2 ## link_names full_links ## &lt;chr&gt; &lt;chr&gt; ## 1 T https://finance.yahoo.com/quote/T?p=T ## 2 HD https://finance.yahoo.com/quote/HD?p=HD ## 3 KSS https://finance.yahoo.com/quote/KSS?p=KSS ## 4 MYOV https://finance.yahoo.com/quote/MYOV?p=MYOV ## 5 CRSP https://finance.yahoo.com/quote/CRSP?p=CRSP 7.5.2 5.2 Scrape Table The first step is to locate the table. Then copy the Xpath. When we paste the path, it should be like: //*[@id=&quot;quote-summary&quot;]/div[1]/table Also, we need the html_table( ) function to convert the html table into a data frame: testlink=read_html(&quot;https://finance.yahoo.com/quote/TIF?p=TIF&quot;) table&lt;-testlink%&gt;% html_nodes(xpath=&#39;//*[@id=&quot;quote-summary&quot;]/div[1]/table&#39;)%&gt;% html_table() table ## [[1]] ## X1 X2 ## 1 Previous Close 124.58 ## 2 Open 124.50 ## 3 Bid 123.44 x 800 ## 4 Ask 123.49 x 800 ## 5 Day&#39;s Range 122.90 - 124.51 ## 6 52 Week Range 73.04 - 130.40 ## 7 Volume 972,652 ## 8 Avg. Volume 2,834,483 7.6 6 External Resources HTML Structure References https://www.w3schools.com/html/html_intro.asp XPath References https://en.wikipedia.org/wiki/XPath https://www.w3schools.com/xml/xml_xpath.asp CSS Selector References https://www.rdocumentation.org/packages/rvest/versions/0.3.4/topics/html_nodes http://flukeout.github.io/ https://www.w3schools.com/cssref/sel_firstchild.asp "],
["working-with-data-links.html", "Chapter 8 Working with data links 8.1 Categorical data cheatsheet 8.2 Data wrangling with R cheatsheet: 8.3 Date and Time Cheatsheet in R 8.4 rvest cheatsheet 8.5 tidyverse cheatsheet 8.6 Python vs R (video) 8.7 R package writing (workshop) 8.8 Regex (workshop) 8.9 GitHub help session (workshop)", " Chapter 8 Working with data links 8.1 Categorical data cheatsheet Zhi Qi This chapter provides a cheatsheet that helps you tackle categorical data. It breaks down what form of graph to use by the type and the number of variables. More information available on edav.info/ Cheatsheet: https://github.com/michaelqizhi/Categorical-data-cheatsheet/blob/master/Categorical%20Variables%20Cheatsheet.pdf 8.2 Data wrangling with R cheatsheet: Tabitha K. Sugumar Goal: Reference sheet for commonly used functions when doing basic data wrangling and cleaning tasks. Consolidate this information in an easy to find place in order to double check syntax, which version of a function to use, etc. Functions included: Subsetting: filter, select Applying: apply, lapply, sapply, vapply Reshaping: cbind, rbind, merge, melt, cast, gather, spread, arrage Aggregation: groupby, summarize, aggregate Manipulation: mutate, transmute, mutate_if, mutate_at, mutate_all, trimws, substr, make_clean_names, na.omit, is.na, remove_empty Link: https://github.com/tks19/EDAV/blob/master/DataWranglingwithRCheatsheet.pdf 8.3 Date and Time Cheatsheet in R Kanika Aggarwal and Swarna Bharathi Mantena We both prepared a cheatsheet for the Date and Time manipulations in R programming language. Mentioned below is the github link and the references used. NOTE: We uploaded the cheatsheet as a pdf file in the github repository. Github Link: https://github.com/SwarnaBharathiMantena/EDAV_CommunityContribution References: http://ianmadd.github.io/pages/POSIXct_and_POSIXlt.html https://www.cyclismo.org/tutorial/R/time.html https://stackoverflow.com/questions/7561400/strptime-function-in-r-to-manipulate-date-time-class https://astrostatistics.psu.edu/su07/R/html/base/html/strptime.html https://www.stat.berkeley.edu/~s133/dates.html https://readr.tidyverse.org/reference/parse_datetime.html https://www.stat.berkeley.edu/~s133/dates.html https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.time.html https://learnr.wordpress.com/2010/02/25/ggplot2-plotting-dates-hours-and-minutes/ 8.4 rvest cheatsheet Huayun Xu and Zelin Li This project creates a cheatsheet on rvest package in R. link: https://github.com/MXKLZL/rvest-cheatsheet/blob/master/contribution.pdf 8.5 tidyverse cheatsheet Huimin Jiang and Yiming Huang Caption for the picture. 8.6 Python vs R (video) Nima Chitsazan and Foad Khoshouei We created a video describing visualization in python and R. For python we specifically focused on matplotlib library and compared it to ggplot library of R. The video is available on youtube: https://youtu.be/phVKWXaAStY The slides are available: https://github.com/fk2377/EDAVCC 8.7 R package writing (workshop) Siddhant Shandilya and Mohit Chander Gulla R packages are an ideal way to package and distribute R code and data for re-use by others. This workshop will provide you with an overview of how to create your own pacakge in R. The walkthrough gives step by step instructions on how to define your functions, create a project for your package, embed your functions and its documentation within it and finally how to compile and build it into an R package that is ready to be shared or published. All the materials used in the workshop can be found at: https://github.com/siddhantshandilya/EDAV---Community-Contribution-19 You may refer to the reference links provided at the end of the pdf which goes into further details on how to publish your package on CRAN repository, if you are interested. 8.8 Regex (workshop) Author: Cheng Yan, Chao Huang This workshop may offer you a basic understanding of regular expression and how it can be used to solve various problems. The workshop is divided into four sections, from the definition of regular expression to the application of regular expression in EDAV. This page is just a roadmap of the workshop and you can find more details and examples in the slides of the workshop here. Definition of Regular Expression In this section, we introduce the definition of regular expression and typical scenarios where we can apply this powerful tool. Basic syntax of Regular Expression In this section, we introduce the basic syntax of regular expression, including wildcards, set, meta-characters, repeated matches, position matching and etc. These patterns can help solve most of the string manipulation problems we meet in our daily work. Advanced syntax of Regular Expression Besides basic syntax, we also introduce some more advanced techniques, including group capturing and looking around. With these tools, you can construct more complex and also more powerful regular expressions. Application in EDAV By using regular expression to solve a string wrapping problem given at PSet2 in one line of code, we show how to compile and use regular expression in R. For those who want to know more about regular expression, two books are highly recommended, namely “Sams Teach Yourself Regular Expressions in 10 Minutes” and “Mastering Regular Expressions”. 8.9 GitHub help session (workshop) Karthik Rajaraman Iyer and Akshay Pakhle We held a walk in doubt session for peers , helping them resolve issues with the github workflow. While some had trouble setting up environment variables in GitBash, other common clarifications were about the brached workflow, creation/handing of pull requests and the appropriate use of issues. We also helped in designing a practice assignment to get a grasp over the github workflow. "],
["edav-flowchart.html", "Chapter 9 EDAV Flowchart", " Chapter 9 EDAV Flowchart Yufeng Ma and Bing Xu This cheatsheet offers a general guideline of EDAV by purpose. EDAV flowchart cheatsheet How should we get started with EDAV? We generated a flowchart that helps with finding the right graphs for certain purposes and data types. We divided into 7 major purposes of data visualization. Distribution When we want to show the distribution. Correlation When we are looking for possible corraltion among variables Comparison When we want to compare samples. Patterns When we are looking for patterns among variables or samples, for example, missing patterns. Statistical Values (ex. Median, Range) When we want to display straightforwardly with statistical values or properties of the data, such as range and median. Time Related When we need to visualize data over time. Survey Data (Likert Scale) When we want to visualize a survey data that uses Likert scale. "],
["tuftes-principles-of-data-ink.html", "Chapter 10 Tufte’s Principles of Data-Ink 10.1 Overview 10.2 Minimal Line Plot 10.3 Range-frame (or quartile-frame) scatterplot 10.4 Dot-dash (or rug) scatterplot 10.5 Marginal histogram scatterplot 10.6 Minimal boxplot 10.7 Minimal barchart 10.8 Sparklines 10.9 References and external resources", " Chapter 10 Tufte’s Principles of Data-Ink Huazhang Liu and Jianfeng Zhuang 10.1 Overview As we are building a new data visualization, we often add more elements to the graph that may only provide small details but takes more attention away from the main takeaways. To solve this issue, Edward Tufte, an expert whose work has contributed significantly to data visualization, introduces the concept of data-ink. This section covers the concepts and examples of data-ink introduced by Edward Tufte in The Visual Display of Quantitative Information. The term, ‘data-ink’, is defined as the total ink on a graph that represents data. Tufte claims that good graphical representations maximize data-ink and erase as much non-data-ink as possible. Thus, he defines the data-ink ratio is the proportion of a graphic’s ink devoted to the non-redundant display of data-information. It should equal to (1 - the proportion of graphics that can be erased without loss of data-information). Tufte gives the following five principles: Above all else show data. Maximize the data-ink ratio. Erase non-data-ink. Erase redundant data-ink. Revise and edit of data-ink: The following sections introduce examples based on Tufte’s techniques about data-ink. We use ggplot2, ggtheme and ggExtra packages to draw the examples with data-ink requirements. 10.2 Minimal Line Plot The purpose of the minimal line plot is to keep the data-ink ratio equal to (1 - the proportion of graphic that can be erased without loss of data-information). There is not extra lines or points in the plot, such as axis. It is easier to see in time series. yr &lt;- 2010:2019 gdp &lt;- c(50749.31, 51193.47, 51565.08, 52545.25, 53654.59, 54280.36, 55004.99, 56192.49, 57252.36, 57821.28) df &lt;- data.frame(yr, gdp) ggplot(df, aes(yr,gdp)) + geom_line() + geom_point(size=1.5) + theme_tufte(base_size = 15) + theme(axis.title=element_blank()) + scale_y_continuous(breaks=seq(50000,58000,1000),label=sprintf(&quot;$%s&quot;,seq(50000,58000,1000))) + scale_x_continuous(breaks=yr,label=yr) + annotate(&quot;text&quot;, x = 2019, y = 51500, adj=1, family=&quot;serif&quot;, label = &quot;US GDP per capita\\nin constant dollars&quot;) 10.3 Range-frame (or quartile-frame) scatterplot Compared with the traditional axis we used, the range-frame is defined as the axis that is only on the range of the data points. A range-frame scatterplot only provides the most appropriate axis lines. We use the mtcars (Motor Trend Car Road Tests) dataset to give the following examples, and use geom_rangeframe() in ggthemes to satisfy this requirement. ggplot(mtcars, aes(disp, mpg)) + geom_point() + geom_rangeframe() + theme_tufte() + xlab(&quot;Displacement (cubic inch)&quot;) + ylab(&quot;Miles per gallon of fuel&quot;) + theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=1.5)) 10.4 Dot-dash (or rug) scatterplot Dot-dash scatterplot is the scatterplot with the tick marks next to the axis that represent the marginal distribution of the data. Compared with the previous two scatterplots, the dot-dash scatterplot is easier to see the distribution on one axis. We use geom_rug() from ggplot2 to exhibit the marginal distribution. ggplot(mtcars, aes(disp, mpg)) + geom_point() + geom_rug() + theme_tufte(ticks=F) + xlab(&quot;Displacement (cubic inch)&quot;)+ ylab(&quot;Miles per gallon of fuel&quot;) + theme(axis.title.x = element_text(vjust=-0.5), axis.title.y = element_text(vjust=1)) 10.5 Marginal histogram scatterplot Similar with dot-dash scatterplot, marginal histogram scatterplot use histogram to mark the marginal distribution of the data. This is useful when there is a huge amount of data points. We use quakes dataset, and use ggMarginal() from ggExtra to exhibit the marginal distribution. p &lt;- ggplot(quakes, aes(depth, mag)) + geom_point() + theme_tufte(ticks=F) + xlab(&quot;Depth (km)&quot;) + ylab(&quot;Richter Magnitude&quot;) ggMarginal(p, type = &quot;histogram&quot;, fill=&quot;transparent&quot;) ggMarginal() also provides marginal density scatterplot and marginal boxplot scatterplot that use density lines or boxplots to mark the marginal distribution of the data. ggMarginal(p, type = &quot;density&quot;, fill=&quot;transparent&quot;) ggMarginal(p, type = &quot;boxplot&quot;, size = 20, fill=&quot;transparent&quot;) 10.6 Minimal boxplot Although it is hard for boxplots to include zero non-data inks, minimal boxplot uses as little non-data ink as possible. In this case, minimal boxplot gets rid of the “box” but uses one dot and blank space to represent the median and interquartile range. In ggtheme, geom_tufteboxplot() is used to represent minimal boxplot. ggplot(quakes, aes(factor(mag), stations)) + theme_tufte() + geom_tufteboxplot(outlier.colour=&quot;transparent&quot;) + theme(axis.title=element_blank()) + annotate(&quot;text&quot;, x = 1, y = 120, adj=0, family=&quot;serif&quot;, size = 3, label = c(&quot;Number of stations \\nreporting Richter Magnitude\\nof Fiji earthquakes (n=1000)&quot;)) 10.7 Minimal barchart In order to erase the theme as much as possible without compromising the data, minimal barchart does not include grid or axes lines. Instead, We add ablines to draw Tufte-like grid lines. Also, we need to adjust the width and space of the bars. We use HairEyeColor dataset. colors &lt;- as.data.frame(HairEyeColor) eye_colors &lt;- colors %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) ggplot(eye_colors, aes(x=Hair, y=Total)) + theme_tufte(base_size=14, ticks=F) + geom_bar(width=0.15, fill=&quot;gray&quot;, stat = &quot;identity&quot;) + theme(axis.title=element_blank()) + scale_y_continuous(breaks=seq(50, 250, 50)) + geom_hline(yintercept=seq(50, 250, 50), col=&quot;white&quot;, lwd=1) + annotate(&quot;text&quot;, x = 4, y = 260, adj=1, family=&quot;serif&quot;, label = c(&quot;Total Number of\\nHair Colors\\nfrom 592 statistics students&quot;)) 10.8 Sparklines Edward Tufte states in Beautiful Evidence that, “A sparkline is a small, intense, simple, word-sized graphic with typographic resolution. Sparklines mean that graphics are no longer cartoonish special occasions with captions and boxes, but rather sparkline graphics can be everywhere a word or number can be: embedded in a sentence, table, headline, map, spreadsheet, graphic.” The difference between the typical line charts and Tufte-style sparklines is that sparklines are small and single. They are small enough that they can be able to embedded in the text. Usually, several sparklines may be grouped together. So it is essential to do the scaling. Otherwise, it is hard to group them. In this section, we use the example dataset provides in the journal Tufte in R. The graph shows the maximum, minimum, and last value of each sparkline. # extract and clean the data dd &lt;- read.csv(text = getURL(&quot;https://gist.githubusercontent.com/GeekOnAcid/da022affd36310c96cd4/raw/9c2ac2b033979fcf14a8d9b2e3e390a4bcc6f0e3/us_nr_of_crimes_1960_2014.csv&quot;)) d &lt;- melt(dd, id=&quot;Year&quot;) names(d) &lt;- c(&quot;Year&quot;,&quot;Crime.Type&quot;,&quot;Crime.Rate&quot;) d$Crime.Rate &lt;- round(d$Crime.Rate,0) # calculate the min, max and end points mins &lt;- group_by(d, Crime.Type) %&gt;% slice(which.min(Crime.Rate)) maxs &lt;- group_by(d, Crime.Type) %&gt;% slice(which.max(Crime.Rate)) ends &lt;- group_by(d, Crime.Type) %&gt;% filter(Year == max(Year)) #draw the graph with max, min, end points ggplot(d, aes(x=Year, y=Crime.Rate)) + facet_grid(Crime.Type ~ ., scales = &quot;free_y&quot;) + geom_line(size=0.3) + geom_point(data = mins, size=1, col = &#39;red&#39;) + geom_point(data = maxs, size=1, col = &#39;blue&#39;) + geom_point(data = ends, size=0.5, col = &#39;black&#39;) + geom_text(data = mins, aes(label = Crime.Rate), vjust = -1) + geom_text(data = maxs, aes(label = Crime.Rate), vjust = 2.5) + geom_text(data = ends, aes(label = Crime.Rate), hjust = 0, nudge_x = 1) + geom_text(data = ends, aes(label = Crime.Type), hjust = 0, nudge_x = 5) + expand_limits(x = max(d$Year) + (0.25 * (max(d$Year) - min(d$Year)))) + scale_x_continuous(breaks = seq(1960, 2010, 10)) + scale_y_continuous(expand = c(0.1, 0)) + theme_tufte(base_size = 15, base_family = &quot;Helvetica&quot;) + theme(axis.title=element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), strip.text = element_blank()) 10.9 References and external resources Tufte in R: introduces several graphs following Tufte’s techniques by using basic R plot, ggplot2, interactive plots with highcharter, etc. This journal is written by Lukasz Piwek. Above All Else Show the Data: based on Lukasz Piwek’s journal, Tufte in R, this introduces some other interactive plots by using plotly. Data-ink Ratio: introduces the concept of data-ink and data-ink ratio Tufte’s Principles: introduce Tufte’s principles, including data-ink. "],
["ridgeline-plots.html", "Chapter 11 Ridgeline plots 11.1 Overview 11.2 tl;dr 11.3 Simple examples 11.4 Theory 11.5 External resources", " Chapter 11 Ridgeline plots Hojin Lee and Hyuk Joon Kwon 11.1 Overview Ridgeline plot is a set of overlapped density plots, and it helps us to compare multiple distirbutions among dataset. Professor Claus O. Wilke from UT Austin, who created ggridges package, commented about ridgeline plot as below: “Ridgeline plots are partially overlapping line plots that create the impression of a mountain range. They can be quite useful for visualizing changes in distributions over time or space. These types of plots have also been called “joyplots”, in reference to the iconic cover art for Joy Division’s album Unknown Pleasures. However, given the unfortunate origin of the name Joy Division, the term “joyplot” is now discouraged.&quot; In this section, we will discuss how to create ridgeline plots using the ggplot and ggridges libraries. 11.2 tl;dr For those who do not want to go through the documents, the below is the polished version of a ridgeline plot and the codes. library(ucidata) library(ggplot2) library(ggridges) library(viridis) library(plyr) library(nycflights13) weather$month &lt;- as.factor(weather$month) ggplot(weather, aes(x = temp, y = reorder(month, desc(month)), fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom = &quot;density_ridges_gradient&quot; , alpha = 0.6 , scale = 2.3) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) + ggtitle(&quot;What is the weather like in NYC?&quot;, subtitle = &quot;Ridgeline plot for NYC temperature by months&quot;) + xlab(&quot;Temperature (F)&quot;) + ylab(&quot;Months&quot;) + labs(caption = &quot;Source: nycflights13::weather&quot;) + theme(plot.title = element_text(face=&quot;bold&quot;)) + theme(plot.subtitle = element_text(face=&quot;bold&quot;, color=&quot;grey&quot;)) + theme(plot.caption=element_text(color=&quot;grey&quot;)) For more information about dataset, type ?nycflights13::weather into the console. 11.3 Simple examples For one who needs friendly step by step approach, please read the below. First, we need to install ggridges and ggplot2 packages. #install.packages(&quot;ggridges&quot;) #install.packages(&quot;ggplot2&quot;) Make sure that the y variable is a categorical variable, otherwise the function will throw an error. You can use y = as.factor(data) to transfrom your y variable into a categorical variable. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges() If you do not want the ridgeline plot to touch each other, please use the scale variable. A scale of 1.0 will make the adjust graph to barely touch each other. If the scale is greater than 1 the graphs will overlap with each other. Otherwise, if the scale is less than 1 the graphs will not touch each other. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges(scale = 1.1) There is a raindrop function within ridgeline plots, which combine the rideline plots with scatter plots; the function will plot scatter plot under the rideline plot. library(ISwR) data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation)) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) Morevoer, it is possible to divide the data into quantiles and draw lines in between. This way, it would be easier for us to observe the median value and the interquartile range. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = temp, y = day, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines =TRUE , geom=&quot;density_ridges_gradient&quot;) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , option = &quot;plasma&quot;) In below, we have merged all the functions we have introduced, and here is the result! data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) + scale_fill_viridis(discrete=TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) Here is one last cool feature of ridgeline plots where we can overlap distributions within same data group. This enables us to compare distributions not only among different data groups but also within same data groups. library(vcd) data3 &lt;- Arthritis ggplot(data3) + geom_density_ridges(aes(x = Age, y = Treatment, group = interaction(Treatment,Improved),fill = Improved), alpha = 0.7) 11.4 Theory Ridgeline plots are an overlap of histograms over y-axis, and this allows us to visualize and compare overall shape of distribution among different groups. They work very well when the dataset has high number of groups to show. Also, since we are overlapping distributions, we can save space for graphs. In other words, if the number of groups to represent is too small, plotting ridgeline plots might not be an optimal choice for data visualization. On the other hand, the ridgeline plots work well when there are clear differences in distributions. Otherwise, because of overlaps, they would cause more confusion when deciphering the data. Couple points to think about before plotting the ridgeline plots: Ordering of groups will change overall shape of the plots. Figure out the optimal bin size &amp; bandwidth argument for the visualization. 11.5 External resources The below has more examples with the ridgeline plots: https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html https://cmdlinetips.com/2018/03/how-to-plot-ridgeline-plots-in-r/ "],
["gantt-charts.html", "Chapter 12 Gantt charts 12.1 Using geom_line 12.2 Using the package ‘plan’", " Chapter 12 Gantt charts Phani Kumar Valasa and Harish Babu Visweswaran A Gantt chart is a visual view of tasks scheduled over time. Gantt charts are used for planning projects and are a useful way of showing what work is scheduled to be done on a specific day. The other elements Gantt charts may provide include the start and end dates of a task/project, the owner(s) of each task, task dependencies, completion status, task grouping and more. In this write-up, we will be focusing only on some of those elements. This can be a useful tutorial for students interested in representing their project plans (think Final Project). We talk about two ways to plot Gantt Charts in R - one using geom_line and playing with the aesthetics and the other using a package called plan. We will compare the two methods in this brief tutorial. Let’s jump right into it. We first load the necessary packages for plotting a Gantt Chart. We actually just need to load the tidyverse package for the plot using geom_line. That will in turn load ggplot2, forcats and all other packages we need. In addition, we need the plan package for the second part of the tutorial. # Loading the necessary pacakges library(tidyverse) library(plan) 12.1 Using geom_line We clearly need to have our data ready before we learn how to make the plot. For the first part of this tutorial, we assume the data is in a dataframe. We can either import the data from an excel or a csv or we can just create the dataframe explicitly inside R if you have a manageable number of tasks. Let’s create a dataframe inside R for this tutorial. df &lt;- data.frame(task=c(&quot;Explore Ideas&quot;, &quot;Finalize Idea&quot;, &quot;Make Plots with\\n geom_line&quot;, &quot;Make Plots with\\n Plan Package&quot;, &quot;Add Writeups&quot;, &quot;Refine Plots&quot;, &quot;Review Tutorial&quot;, &quot;Submit Tutorial&quot;, &quot;Get Feedback\\n and Update&quot;), start=c(&quot;2019-10-15&quot;, &quot;2019-10-18&quot;, &quot;2019-10-19&quot;, &quot;2019-10-22&quot;, &quot;2019-10-24&quot;, &quot;2019-10-25&quot;, &quot;2019-10-26&quot;, &quot;2019-10-27&quot;, &quot;2019-10-28&quot;), end=c(&quot;2019-10-18&quot;, &quot;2019-10-19&quot;, &quot;2019-10-24&quot;, &quot;2019-10-26&quot;, &quot;2019-10-27&quot;, &quot;2019-10-27&quot;, &quot;2019-10-28&quot;, &quot;2019-10-28&quot;, &quot;2019-10-31&quot;), owner=c(&quot;Harish&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;)) Printing the first few rows of the dataframe: head(df) ## task start end owner ## 1 Explore Ideas 2019-10-15 2019-10-18 Harish ## 2 Finalize Idea 2019-10-18 2019-10-19 Phani ## 3 Make Plots with\\n geom_line 2019-10-19 2019-10-24 Harish ## 4 Make Plots with\\n Plan Package 2019-10-22 2019-10-26 Phani ## 5 Add Writeups 2019-10-24 2019-10-27 Phani ## 6 Refine Plots 2019-10-25 2019-10-27 Harish Converting the dates from factor type to date type: df &lt;- df %&gt;% mutate(start = as.Date(start), end = as.Date(end)) YYYY-MM-DD is the preferred format for dates but if you have other formats, you can still use the as.Date function to convert them to dates by passing in the tryFormats argument as a vector Ex: tryFormats = c(“%Y-%m-%d”, “%Y/%m/%d”) will check if the Date fits any of the passed formats. In order for us to be able to use the line plot, we need to tidy the data - the start and the end date need to be in the same column. We will use the gather function from tidyr to transform the dataframe. We add another column that indicates whether the date is the start date or the end date. df_tidy &lt;- df %&gt;% gather(key=date_type, value=date, -task, -owner) The trick in using a line plot for the Gantt chart is to make the line very thick so that it looks like a bar. If we then have a start and end value for the bar, we should be able to get what we need. We will flip the coordinates so that the bars are horizontal (which is more in line with how a Gantt chart looks). We will start with a basic plot using geom_line and then make updates until we are happy with the plot. To begin with, we will plot the task name on the x-axis and the date on the y-axis and then flip the coordinates. We will adjust the size parameter to get a bar instead of a line ggplot() + geom_line(data=df_tidy, mapping=aes(x=task, y=date), size=10) + coord_flip() The plot is directionally fine but we observe a few issues. The tasks are not in the order we entered them in. We can fix that by using the fct_inorder function from the forcats package (the task field is a factor and forcats provides many functions that help with factor ordering). Since we are dealing with a horizontal line plot, the plot starts from the bottom (the first value passed will be at the bottom and the last value passed will be at the top). Since we want to read out plot from the top, we will reverse the order using fct_rev. Additionally, we want to indicate the owner of each task. We will do that by passing the color argument to geom_line. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + coord_flip() Our plot looks pretty good now. Gantt charts optionally have vertical line markers that indicate the current date. This serves as a frame of reference for members to evaluate whether they are on track to complete the project in time. Let’s add this using geom_hline before the coordinate flip. Let’s use a dashed black line to represent this. Normally we would use the Sys.Date() function from base R to get the current date but here we hardcode the date to 2019-10-27, so that rerunning the code at a later date will still display the line. We will also add some labels. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) We now have a version of the Gantt Chart that we can use. We will now tidy up the graph by updating the date ticks to make them more frequent and lining up the grid lines with the labeled dates. We will also update the theme to the bw theme to get a cleaner look. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + scale_y_date(date_breaks = &quot;1 day&quot;) + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90), panel.grid.minor = element_line(colour=&quot;white&quot;, size=0.5), legend.position=&quot;right&quot;, plot.title = element_text(hjust = 0.5)) Finally, let’s say we want a less instrusive way to indicate that a certain task has been completed. One option is to make the completed bars transparent and the let the incomplete bars be the way they are. For this, we need to have the status of the task in another column. Let’s add a column named completed to the original dataframe and enter values of 1 for completed tasks and 0 for incomplete tasks. We will then use the completed column inside the aesthetics as the argument for the alpha parameter. We set alpha to 0.2 for incomplete tasks and 1 for complete tasks using the scale_alpha_discrete function. Additionally, we get a legend that is not super helpful - we remove it by passing guide=“none” inside the scale_alpha_discrete function. This is not the best possible way to visualize the status but it works. df_completed &lt;- df %&gt;% mutate(completed = factor(c(rep(1, 6), rep(0, 3)))) df_tidy &lt;- df_completed %&gt;% gather(key=date_type, value=date, -task, -owner, -completed) ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner, alpha=completed), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + scale_alpha_discrete(range=c(1, 0.2), guide=&quot;none&quot;) + scale_y_date(date_breaks = &quot;1 day&quot;) + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90), panel.grid.minor = element_line(colour=&quot;white&quot;, size=0.5), legend.position=&quot;right&quot;, plot.title = element_text(hjust = 0.5)) 12.2 Using the package ‘plan’ Now we show how to create a Gantt chart using the ‘plan’ package. It has predefined methods to read the data and plot the gantt chart. Here is the reference to the package for more details: https://cran.r-project.org/web/packages/plan/plan.pdf The data used is similar to the one we used for creating the charts using geom_line. We’ll be showing two ways to read the data. One is adding the tasks manually to the ‘gantt’ object by using the ‘ganttAddTask’ method. The other is to read the tasks from an existing file. Let’s explore the first method of adding tasks via ‘ganttAddTask’ in our tutorial project. This method takes the task description, start date, end date and percentage completion as input. As you see below, we need to add the percentage completion of the task as with any project planning. The ‘plot’ takes care of creating a Gantt chart from this object. g &lt;- new(&quot;gantt&quot;) g &lt;- ganttAddTask(g, &quot;Explore Ideas&quot;,&quot;2019-10-15&quot;,&quot;2019-10-18&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Finalize project&quot;,&quot;2019-10-18&quot;,&quot;2019-10-19&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Create WBS&quot;,&quot;2019-10-19&quot;,&quot;2019-10-20&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - geom_line&quot;,&quot;2019-10-20&quot;,&quot;2019-10-24&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - package&quot;,&quot;2019-10-22&quot;,&quot;2019-10-26&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Add Writeup&quot;,&quot;2019-10-24&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Refine Plot&quot;,&quot;2019-10-25&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Review Tutorial&quot;,&quot;2019-10-26&quot;,&quot;2019-10-28&quot;,done=80 ) g &lt;- ganttAddTask(g, &quot;Submit Tutorial&quot;,&quot;2019-10-27&quot;,&quot;2019-10-28&quot;,done=0 ) g &lt;- ganttAddTask(g, &quot;Get Feedback and Update&quot;,&quot;2019-10-28&quot;,&quot;2019-10-31&quot;,done=0 ) plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), event.time=&quot;2019-10-27&quot;, event.label=&quot;Report Date&quot;, main = &quot;Community Contribution Gantt Chart&quot;) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=gray(c(0.3, 0.9)), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) In addition to the above, using the package, we are also able to categorize sections of tasks into groups. Here, we have split the tasks into groups named Plan, Implement and Review for demonstration. The interface is simple - we just need to include a task with just the name of the task. g &lt;- new(&quot;gantt&quot;) g &lt;- ganttAddTask(g, &quot;Plan&quot;) g &lt;- ganttAddTask(g, &quot;Explore Ideas&quot;,&quot;2019-10-15&quot;,&quot;2019-10-18&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Finalize project&quot;,&quot;2019-10-18&quot;,&quot;2019-10-19&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Create WBS&quot;,&quot;2019-10-19&quot;,&quot;2019-10-20&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Implement&quot;) g &lt;- ganttAddTask(g, &quot;Gantt chart - geom_line&quot;,&quot;2019-10-20&quot;,&quot;2019-10-24&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - package&quot;,&quot;2019-10-22&quot;,&quot;2019-10-26&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Add Writeup&quot;,&quot;2019-10-24&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Refine Plot&quot;,&quot;2019-10-25&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Review&quot;) g &lt;- ganttAddTask(g, &quot;Review Tutorial&quot;,&quot;2019-10-26&quot;,&quot;2019-10-28&quot;,done=80 ) g &lt;- ganttAddTask(g, &quot;Submit Tutorial&quot;,&quot;2019-10-27&quot;,&quot;2019-10-28&quot;,done=0 ) g &lt;- ganttAddTask(g, &quot;Get Feedback and Update&quot;,&quot;2019-10-28&quot;,&quot;2019-10-31&quot;,done=0 ) plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), event.time=&quot;2019-10-27&quot;, event.label=&quot;Report Date&quot;, main = &quot;Community Contribution Gantt Chart&quot;) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=gray(c(0.3, 0.9)), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) The below is the second method of reading the tasks from a file. Here is the format of the file used, for it be read by ‘read.gantt’ method. Key,Description,Start,End,Done,NeededBy 1,Explore Ideas,2019-10-15,2019-10-18,100, 2,Finalize project,2019-10-18,2019-10-19,100, 3,Create WBS,2019-10-19,2019-10-20,100, 4,Gantt chart - package,2019-10-20,2019-10-24,100, 5,Gantt chart - geom_line,2019-10-22,2019-10-26,100, 6,Add Writeup,2019-10-24,2019-10-27,100, 7,Refine Plot,2019-10-25,2019-10-27,100, 8,Review Tutorial,2019-10-26,2019-10-28,80, 9,Submit Tutorial,2019-10-27,2019-10-28,0, 10,Get Feedback and Update,2019-10-28,2019-10-31,0, We can also add the dependency on other tasks in the above file using the NeededBy column but we haven’t used it in this example (we’ve left it blank). However there is no provision to add the owner of the task/resource who will be working on the task. We’ve explored this in creation of gantt_chart with ‘geom_line’. Once the data is read using ‘read.gantt’ method, the output object is used by ‘plot’ to create the Gantt chart. You’ll notice the code is simple and doesn’t look like a hack to produce the chart. Note that the file is hosted on github - so running the below script as is should work. gt_object &lt;- read.gantt(&quot;https://raw.githubusercontent.com/harish-cu/cc19/tasks_file/tasks.csv&quot;) plot(gt_object,event.label=&#39;Report Date&#39;,event.time=&#39;2019-10-27&#39;, col.event=c(&quot;red&quot;), col.done=c(&quot;lightblue&quot;), col.notdone=c(&quot;pink&quot;), main=&quot;Community Contribution Gantt Chart&quot; ) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=c(&quot;lightblue&quot;, &quot;pink&quot;), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) From the above, as you see, it is easy to plot a Gantt chart using the package. This provides an easy way of looking at what has been completed and pending. It calculates the proportion of the line to color from the percentage completion we provide. However it doesn’t have the ability to color/segment the chart based on the resources working on those tasks. The annotations on the chart (from ‘plot’) needs a little work as the labelling is not as straight forward compared to what we get with geom_line. And any customizations on the Gantt chart such as segmenting the chart by ‘Resource’, providing an effort based chart rather than duration etc., needs significant work and may not be directly produced by this package. As long as the output meets your requirements and the tasks can be maintained in the format required by this package, ‘plan’ provides an excellent solution to produce quick Gantt charts. However if your requirements needs additional customizations, any custom solution built using geom_line would provide a better alternative. Note: There are other packages like Candela and DiagrammeR that we have not explored in this tutorial but might be of interest as they provide similar capabilities to draw Gantt Charts Sources: https://www.molecularecologist.com/2019/01/simple-gantt-charts-in-r-with-ggplot2-and-the-tidyverse/ https://cran.r-project.org/web/packages/plan/plan.pdf "],
["plotrix-for-complex-visualizations.html", "Chapter 13 Plotrix for complex visualizations 13.1 Overview 13.2 Plotrix 13.3 zoomInPlot example 13.4 fan.plot example 13.5 pie3D example 13.6 pyramid.plot example 13.7 Sources", " Chapter 13 Plotrix for complex visualizations Alex Wan and Junyang Jiang 13.1 Overview Our graphs have to be informative and attractive to the audience to get their attention. This section covers how we can use specialized plots and accessories functions in plotrix to easily build complex visualizations. 13.2 Plotrix In plotrix, we have many functionalities. A brief description of a selcted few of them is as follows: barNest: Display a nested breakdown of numeric values zoomInPlot: Display a plot with rectangular section expanded in an adjacent plot fan.plot: Display a fan plot pie3D: Display a 3D pie chart pyramid.plot: Pyramid plot 13.2.1 barNest example In a nested bar chart, we can break down the elements of the data frame by one or more categorical elements and display the breakdown as a bar plot. It first takes as input the dependent and independent variables, as columns of a dataframe, along with the colors to be used in the plot. The showall parameter indicates whether to display bars for the entire breakdown, while the shrink parameter determines the proportion to shrink the width of bars at each level. The trueval parameter returns the proportion of of the response variable equal to ‘trueval’. titanic&lt;-data.frame( class=c(rep(&quot;1st&quot;,325),rep(&quot;2nd&quot;,285),rep(&quot;3rd&quot;,706),rep(&quot;Crew&quot;,885)), age=c(rep(&quot;Adult&quot;,319),rep(&quot;Child&quot;,6),rep(&quot;Adult&quot;,261),rep(&quot;Child&quot;,24), rep(&quot;Adult&quot;,627),rep(&quot;Child&quot;,79),rep(&quot;Adult&quot;,885)), sex=c(rep(&quot;M&quot;,175),rep(&quot;F&quot;,144),rep(&quot;M&quot;,5),rep(&quot;F&quot;,1), rep(&quot;M&quot;,168),rep(&quot;F&quot;,93),rep(&quot;M&quot;,11),rep(&quot;F&quot;,13), rep(&quot;M&quot;,462),rep(&quot;F&quot;,165),rep(&quot;M&quot;,48),rep(&quot;F&quot;,31), rep(&quot;M&quot;,862),rep(&quot;F&quot;,23)), survived=c(rep(&quot;Yes&quot;,57),rep(&quot;No&quot;,118),rep(&quot;Yes&quot;,140),rep(&quot;No&quot;,4),rep(&quot;Yes&quot;,6), rep(&quot;Yes&quot;,14),rep(&quot;No&quot;,154),rep(&quot;Yes&quot;,80),rep(&quot;No&quot;,13),rep(&quot;Yes&quot;,24), rep(&quot;Yes&quot;,75),rep(&quot;No&quot;,387),rep(&quot;Yes&quot;,76),rep(&quot;No&quot;,89), rep(&quot;Yes&quot;,13),rep(&quot;No&quot;,35),rep(&quot;Yes&quot;,14),rep(&quot;No&quot;,17), rep(&quot;Yes&quot;,192),rep(&quot;No&quot;,670),rep(&quot;Yes&quot;,20),rep(&quot;No&quot;,3))) require(plotrix) titanic.colors&lt;-list(&quot;gray90&quot;,c(&quot;#0000ff&quot;,&quot;#7700ee&quot;,&quot;#aa00cc&quot;,&quot;#dd00aa&quot;), c(&quot;#ddcc00&quot;,&quot;#ee9900&quot;),c(&quot;pink&quot;,&quot;lightblue&quot;)) barNest(survived~class+age+sex,titanic,col=titanic.colors,showall=TRUE, main=&quot;Titanic survival by class, age and sex&quot;,ylab=&quot;Proportion surviving&quot;, FUN=c(&quot;propbrk&quot;,&quot;binciWu&quot;,&quot;binciWl&quot;,&quot;valid.n&quot;),shrink=0.15,trueval=&quot;Yes&quot;) We can also set Nwidths to TRUE, if we want to scale the the widths of bars to the number of observations. barNest(survived~class+age+sex,titanic,col=titanic.colors,showall=TRUE, main=&quot;Titanic survival by class, age and sex (scaled bar widths)&quot;, ylab=&quot;Proportion surviving&quot;,FUN=c(&quot;propbrk&quot;,&quot;binciWu&quot;,&quot;binciWl&quot;,&quot;valid.n&quot;), shrink=0.15,trueval=&quot;Yes&quot;,Nwidths=TRUE) 13.3 zoomInPlot example The zoomInPlot function allows us to display one plot on the left half, and an expanded section of the plot on the right half, with lines showing the expansion. It takes as input two vectors, x and y. We can also use the rxlim and rylim parameters to dictate the limits of the expanded section. zoomInPlot(rnorm(100),rnorm(100),rxlim=c(-1,1),rylim=c(-1,1), zoomtitle=&quot;Zoom In Plot&quot;,titlepos=-1.5) 13.4 fan.plot example The fan.plot function displays numerical values as arcs of overlapping sectors. It takes as input the vector of numbers we want to plot, as well as the max.span which will be the angle of the maximum sector in radians. In the example below, the span is set to pi, so the max sector will be a half circle. The labels parameter provides the corresponding lables to the different sectors, while the ticks parameter determines the total number of ticks around the span. iucn.df&lt;-data.frame(area=c(&quot;Africa&quot;,&quot;Asia&quot;,&quot;Europe&quot;,&quot;N&amp;C America&quot;, &quot;S America&quot;,&quot;Oceania&quot;),threatened=c(5994,7737,1987,4716,5097,2093)) fan.plot(iucn.df$threatened,max.span=pi, labels=paste(iucn.df$area,iucn.df$threatened,sep=&quot;-&quot;), main=&quot;Threatened species by geographical area (fan.plot)&quot;,ticks=276) 13.5 pie3D example The pie3D function displays a 3D pie chart with optional labels. It takes as input a numeric vector where each value is a sector in the pie chart, as well as the optional labels for each sector. The radius parameter determines how large the pie is, while the explode parameter determines how much the pie sectors “explodes” outward from the center. pieval&lt;-c(2,4,6,8) pielabels&lt;- c(&quot;We hate\\n pies&quot;,&quot;We oppose\\n pies&quot;,&quot;We don&#39;t\\n care&quot;,&quot;We just love pies&quot;) # grab the radial positions of the labels lp&lt;-pie3D(pieval,radius=0.9,labels=pielabels,explode=0.1,main=&quot;3D PIE OPINIONS&quot;) 13.6 pyramid.plot example The pyramid.plot function displays a pyramid of opposing horizontal bar plots. Its primary intention is for the construction of population pyramids, although it can also display other types of opposed bar charts. It takes as input two vectors of numbers, one to be displayed on the left and the other on the right. It also needs a label vector to be displayed in the middle, which should contain a label for each left or right value, even if empty. We can also specify the colors to be used on the left and right, while setting show.values to TRUE will display the corresponding values at the end of the bars. x&lt;-c(5.66,6.13,6.03,5.45,4.69,4.14,3.71,3.21,2.5,1.71,1.51,1.52,1.45,1.04,0.79,0.48,0.17) y&lt;-c(5.37,5.88,5.74,5.24,4.62,4.33,3.76,3.16,2.48,1.79,1.56,1.52,1.41,0.94,0.77,0.39,0.1) agelabels&lt;-c(&quot;0-5&quot;,&quot;5-10&quot;,&quot;10-15&quot;,&quot;15-20&quot;,&quot;20-25&quot;,&quot;25-30&quot;,&quot;30-35&quot;,&quot;35-40&quot;,&quot;40-45&quot;,&quot;45-50&quot;,&quot;50-55&quot;,&quot;55-60&quot;,&quot;60-65&quot;,&quot;65-70&quot;,&quot;70-75&quot;,&quot;75-80&quot;,&quot;85+&quot;) fcol&lt;-color.gradient(c(0,0,0.5,1),c(0,0,0.5,1),c(1,1,0.5,1),17) mcol&lt;-color.gradient(c(1,1,0.5,1),c(0.5,0.5,0.5,1),c(0.5,0.5,0.5,1),17) par(mar=pyramid.plot(x,y,labels=agelabels,lxcol=mcol,rxcol=fcol,show.values=TRUE)) title(sub=&quot;Tunisian age pyramid in 1994&quot;,cex.sub=1.7) 13.7 Sources https://cran.r-project.org/web/packages/plotrix/plotrix.pdf "],
["stacked-bar-charts-and-treemaps.html", "Chapter 14 Stacked Bar Charts and Treemaps 14.1 1. Grouped and Stacked Bar Chart 14.2 2. Treemap", " Chapter 14 Stacked Bar Charts and Treemaps Jasmine Bao and Yingnan Wu This tutorial covers how to make static and interactive stacked bar charts and static treemaps. In this section, we discuss ways of displaying multivariate categorical data, i.e., combinations of categorical variables using bar charts and treemaps. 14.1 1. Grouped and Stacked Bar Chart 14.1.1 Overview Grouped and stacked bar charts are good for showing the counts of two or three categorical variables. The dataset we are using is the same as the data frame shown in class. 14.1.2 ggplot2 We first use package ggplot2 to make stacked bar charts. The input dataset should provide three columns which are the numeric count and two categorical variables for group and subgroup respectively. # load the library library(ggplot2) # reshape the dataset df1 &lt;- df %&gt;% group_by(Age,Favorite) %&gt;% summarise(count=n()) # make stacked bar chart ggplot(df1, aes(y=count, x=Age, fill=Favorite)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(&quot;Figure 1.1: Stacked bar chart on Age and Favorite&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) To make a percentage stacked bar chart, we just need to switch to position=&quot;fill&quot;. The y-axis label needs to be changed to proportion or percent, accordingly. If the original numerical values are counts, it is better to change the y-axis to continuous percentage scale. # make percent stacked bar chart ggplot(df1, aes(y=count, x=Age, fill=Favorite)) + geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) + ylab(&quot;percent&quot;) + ggtitle(&quot;Figure 1.2: Percent stacked bar chart on Age and Favorite&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) + scale_y_continuous(labels = scales::percent_format()) Argument position=&quot;dodge&quot; has bars align beside each other (default as position=&quot;stack&quot;). When stacked bar charts are overused, grouped barplot is preferred with common y-axis scale to compare. # make another grouped bar chart ggplot(df1, aes(y=count, x=Age, fill=Favorite)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + ggtitle(&quot;Figure 1.3: Grouped bar chart on Age and Favorite as fill&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) In this case, we have two options in terms of which is used for fill and which is used for group division. # make grouped bar chart ggplot(df1, aes(y=count, x=Favorite, fill=Age)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + ggtitle(&quot;Figure 1.4: Grouped bar chart on Favorite and Age as fill&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) We can also add facets to avoid overusing colors. Observe that colors disappear, it is because all categorical variables are clearly labeled. # make grouped bar chart with facets ggplot(df1, aes(y=count, x=Age)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + facet_wrap(~Favorite) + ggtitle(&quot;Figure 1.5: Grouped bar chart with facets on Favorite&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) Similarly, we can have facets on the other categorical variable Age. # make grouped bar chart with facets on Age ggplot(df1, aes(y=count, x=Favorite)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + facet_wrap(~Age) + ggtitle(&quot;Figure 1.6: Grouped bar chart with facets on Age&quot;) + theme(plot.title = element_text(hjust = 0.5, size=15)) The grouped bar chart with facets also works when we have three categorical variables. When reshaping the dataset, we actually need complete() here to avoid dropping zero counts. # load library library(tidyr) # reshape the dataset df2 &lt;- df %&gt;% group_by(Age,Favorite, Music) %&gt;% summarise(count=n()) %&gt;% complete(Age, Favorite, Music, fill=list(count = 0)) # make the grouped barplot ggplot(df2, aes(fill=Music, y=count, x=Favorite)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + facet_wrap(~Age) + ggtitle(&quot;Figure 1.7: Grouped bar chart for Favorite and Music with facets on Age&quot;) + theme(plot.title = element_text(hjust = 0.5, size=13)) 14.1.3 plotly R package plotly can also be utilized to plot interactive bar charts for multivariate categorical variables. # load the library library(plotly) # spread the dataset df3 &lt;- df %&gt;% group_by(Age,Favorite) %&gt;% summarise(count=n()) %&gt;% spread(key=Favorite, value=count) # make the stacked bar chart plot_ly(df3, x=~Age, y=~`bubble gum`, type=&#39;bar&#39;, name=&#39;bubble gum&#39;) %&gt;% add_trace(y=~coffee, name=&#39;coffee&#39;) %&gt;% layout(yaxis = list(title = &#39;count&#39;), title = &#39;Figure 1.8: Interactive stacked bar chart on Age and Favorite&#39;, barmode = &#39;stack&#39;) For the corresponding grouped bar chart, we only need to change barmode = 'stack' to barmode = 'group'. The counts for each subgroup can be added directly on the interactive plot as well. # make the stacked bar chart df3 %&gt;% plot_ly() %&gt;% add_trace(x=~Age, y=~`bubble gum`, type = &#39;bar&#39;, name=&#39;bubble gum&#39;, text = df3$`bubble gum`, textposition = &#39;auto&#39;) %&gt;% add_trace(x=~Age, y=~coffee, type = &#39;bar&#39;, name=&#39;coffee&#39;, text = df3$coffee, textposition = &#39;auto&#39;) %&gt;% layout(title = &quot;Figure 1.9: Interactive grouped bar chart on Age and Favorite\\nwith direct labels&quot;, barmode = &#39;group&#39;, xaxis = list(title = &quot;&quot;), yaxis = list(title = &quot;&quot;)) 14.1.4 Consideration The above stacked and grouped bar charts can only be used for the visualization of two or three categorical variables and necessitate the reshape of data (messier or tidier). For more general demonstration, we can apply mosaic plots, doubledecker plots, fluctuation diagrams, treemaps, association plots, and parallel sets/categorical parallel coordinate plots. 14.1.5 External resources Grouped, stacked and percent stacked barplot in ggplot2: a good reference of learning how to build grouped, stacked and percent stacked barplot with R and ggplot2 with multiple examples. How to make a bar chart in R using plotly: a detailed tutorial of making barplots using plotly package. R documentation tidyr: complete() R documentation ggplot2: geom_bar() 14MosaicPlots.pdf by Professor Joyce Robbins 14.2 2. Treemap 14.2.1 Overview This section covers how to make treemaps. Treemaps are filled rectangular plot representing hierarchical data, similar to pie chart in that the area of the rectangles can represent proportions. Treemaps can be drawn in R using the treemap function in the package treemap. What types of datasets are appropriate for treemaps? Firstly, we need to have a quantitative variable of positive value, then we need one or more hierarchical categorical variables associated with that quantitative variable. #load library library(ggplot2) library(tidyverse) library(treemap) #Load population dataset from Github df &lt;- read.table(&quot;https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/11_SevCatOneNumNestedOneObsPerGroup.csv&quot;, header=T, sep=&quot;;&quot;) df[ which(df$value ==-1),&quot;value&quot;] &lt;- 1 colnames(df) &lt;- c(&quot;Continent&quot;, &quot;Region&quot;, &quot;Country&quot;, &quot;Population&quot;) 14.2.2 Continent level First, we want to look at the proportions of population across the continents. treemap(df, #dataframe index=c(&quot;Continent&quot;), #categorical hierarchy variable, Continent in this case vSize = &quot;Population&quot;, #quantitative variable, Population in this case type=&quot;index&quot;, title = &quot;Figure 2.1: Population by Continents&quot;)#colors are determined by the index variables. Different branches in the hierarchical tree get different colors We can observe from the treemap that Asia has the highest population in all continents meanwhile Oceania has the lowest. 14.2.3 Region level Now we would like to look at the population at region level. We simply add “Region” after “Continent” in the index=c(&quot;Continent&quot;) line. Note that the categorical variables have to go in decreasing order of hierarchy for instance, index=c(&quot;group&quot;, &quot;subgroup&quot;, &quot;sub-subgroup&quot;,...). treemap(df, #dataframe index=c(&quot;Continent&quot;, &quot;Region&quot;), #categorical hierarchy variables vSize = &quot;Population&quot;, #quantitative variable, Population in this case type=&quot;index&quot;, title = &quot;Figure 2.2: Population by Continents and Regions&quot;) We observe that Southern Asia and Eastern Asia have the higest proportion of population within Asia and their population proportions are almost equal, while Central Asia has the lowest proportion of population within Asia. Western Africa has the higest proportion of population within Africa, Southern Africa has the lowest and so on. 14.2.4 Country Level If we would also like to look at population at the country level, we simply add &quot;Country&quot; after &quot;Region&quot; in the index=c(&quot;Continent&quot;, &quot;Region&quot;) line. treemap(df, #dataframe index=c(&quot;Continent&quot;, &quot;Region&quot;, &quot;Country&quot;), #categorical variables in the order of highest level of the hierarchy to lowest vSize = &quot;Population&quot;, type=&quot;index&quot;, title = &quot;Figure 2.3: Population by Continents, Regions and Countries&quot;) However, one problem might arise as we have many levels of hierarchy: the labels might become hard to read. In that case, we can adjust the parameters of the labels for the ease of reading. treemap(df, #dataframe index=c(&quot;Continent&quot;, &quot;Region&quot;, &quot;Country&quot;), #categorical variables in the order of highest level of the hierarchy to lowest vSize = &quot;Population&quot;, #quantitative variable type=&quot;index&quot;, # Labels fontsize.labels=c(15,8,5), # size of labels. Give the size per level of aggregation: size for group, size for subgroup, sub-subgroups... fontcolor.labels=c(&quot;black&quot;,&quot;orange&quot;, &quot;white&quot;), # Color of labels fontface.labels=c(2,1,1), # Font of labels: 1,2,3,4 for normal, bold, italic, bold-italic... bg.labels=c(&quot;transparent&quot;), # Background color of labels align.labels=list( c(&quot;center&quot;, &quot;center&quot;), c(&quot;right&quot;, &quot;bottom&quot;), c(&quot;left&quot;, &quot;top&quot;)), # Where to place labels in the rectangle? overlap.labels=0.5, # number between 0 and 1 that determines the tolerance of the overlap between labels. 0 means that labels of lower levels are not printed if higher level labels overlap, 1 means that labels are always printed. In-between values, for instance the default value .5, means that lower level labels are printed if other labels do not overlap with more than .5 times their area size. inflate.labels=F, title = &quot;Figure 2.4: Population by Continents, Regions and Countries&quot;) We observe that China has the highest proportion of population in Eastern Asia while India has the the highest proportion of population in Southern Asia and so on. 14.2.5 Consideration Static treemaps which were covered in this section could be handy when we have a hierarchical dataset with at most three levels of hierarchy. However, if there are more levels of hierarchy, we should consider plotting an interactive version of treemaps using packages such as d3Tree as it will greatly increase readability of the plot. Another important reminder is that when we have multivariate categorical data, we should only consider plotting the data using treemaps when the data has hierarchical structure and we are interested in the relationship between the quantitative variables and the different level of subgroups. If that’s not the case, we should consider plotting the data using graphs such as mosaicplots, doubledecker plots, fluctuation diagrams, association plots, and parallel sets/categorical parallel coordinate plots. 14.2.6 External resources How to make a treemap in R using treemap(): a detailed tutorial of making treemaps using treemap package. R documentation treemap() Graphical Data Analysis with R by Antony Unwin "],
["fluctuation-plots.html", "Chapter 15 Fluctuation plots", " Chapter 15 Fluctuation plots Arusha Kelkar and Tanvi Pareek Visualization involves using graphics to display and interpret the data to reveal the information in the dataset in a better sense. Fluctuation plots are one of the great ways to visualize data. Fluctuation plots are used to visualize categorical data. It is a plot consisting of rectangles aligned next to each other where the number of rectangles depends on the number of categories in the data. Every position in the grid of the fluctuation plot represents a combination of categories in the data. The frequency of combinations of categories determine the relative sizes of the rectangular plots. Each rectangular plot has a foreground and a background. The background rectangle represents the maximum of all the frequencies for all combinations of categories in the data. The foreground rectangle represents the frequency of that particular combination of categories. So, the size of the foreground rectangle is relative to the maximum value of the frequencies. Before giving an example of this using a dataset, we would like to outline the steps for the installation of the package extracat (archived in CRAN) which is used for plotting fluctuation plots in R. Download the latest tar file from the following link https://cran.r-project.org/src/contrib/Archive/extracat/ If you don’t have the TSP package installed then download it by typing the following command in the R console install.packages(“TSP”, dependencies = TRUE). In the terminal type the following command to install the extracat package: R CMD INSTALL -1 C:/Users/User/Desktop/Fluctuation extracat_1.7-6.tar Make sure the path where the package is downloaded is correct. Now we’re ready to plot the fluctuation plots in R! We have used the HairEyeColor dataset which is already available in R. It gives the distribution of hair and eye color and sex in 592 students. The library extracat is loaded first and then the dataset. library(extracat) HairEyeColor ## , , Sex = Male ## ## Eye ## Hair Brown Blue Hazel Green ## Black 32 11 10 3 ## Brown 53 50 25 15 ## Red 10 10 7 7 ## Blond 3 30 5 8 ## ## , , Sex = Female ## ## Eye ## Hair Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 The above output gives two tables separated based on the sex as Female or Male. Basically this is found by cross tabulating 592 values on 3 variables. The variable Hair has four levels namely Black, Brown, Red and Blond and the variable Eye has four levels namely brown, blue, hazel and green. The function used to plot the fluctation diagrams is fluctile. fluctile(HairEyeColor) The default is that the rectangles are centered. We can change this default by using the just argument. tile.col is used to change the foreground colour and bg.col is used to change the background colour. fluctile(HairEyeColor, just = &quot;lb&quot;, tile.col = &quot;red&quot;, bg.col = &quot;black&quot;) This leftbottom representation is more easy to interpret than the centred one. The frequency in the dataset for entries with Brown hair, brown eyes and the person is female is the highest i.e 66.Therefore in the above plot, the foreground rectangle for these values of the categories covers the background rectangle completely. The maximum size of this background rectangle which is colored black is due to the maximum frequency i.e. the frequency of people being female,having brown hair and brown eyes which is 66. All the other values, which are the counts of the values of the combinations of the 3 variables used here, are plotted with respect to this value.For example, the number of females having red hair and brown eyes is 16, so the proportion here is 16/66 , so that proportion of the rectangle is colored red. Variations in plot: The shape of the plot below has been changed to circle using shape argument. fluctile(HairEyeColor,shape=&quot;c&quot;, tile.col = &quot;red&quot;, bg.col = &quot;black&quot;) The shape of the plot below has been changed to octagon using shape argument. fluctile(HairEyeColor,shape=&quot;o&quot;,tile.col = &quot;red&quot;, bg.col = &quot;black&quot;) The above 2 plots are variations of the rectangular fluctuation plots.These also give the insights about the frequencies of the combinations of categories at a glance. If you want to change the maximum value(the background) relative to which the foreground is plotted, you can set it using maxv argument. For example, below we have set maxv = 100 . fluctile(HairEyeColor,just = &quot;lb&quot; ,tile.col = &quot;red&quot;, bg.col = &quot;black&quot; , maxv = 100) For adding border to the foreground, use the tile.border argument. fluctile(HairEyeColor,just = &quot;lb&quot; ,tile.col = &quot;red&quot;, bg.col = &quot;black&quot; ,tile.border = &quot;yellow&quot;) Why Fluctuation Plots? This plot is very helpful for comparing frequencies of combinations of categories with the maximum of these frequencies. Fluctuation diagrams are good for representing large contingency tables or transition matrices, where there is no reason to differentiate between the row variable and the column variable. If there is a large number of combinations and only a few occur at all,then a fluctuation diagram is valuable for revealing this information and for identifying categorical clusters. "],
["introduction-to-package-ggparty.html", "Chapter 16 Introduction to package ‘ggparty’ 16.1 Introdunction of class ‘party’ 16.2 Use ‘ggparty’ to visualize the tree 16.3 Customize the tree 16.4 Add plots to the tree 16.5 Application", " Chapter 16 Introduction to package ‘ggparty’ Tianyao Han (th2830) and Wancheng Chen (wc2687) # import packages library(dplyr) library(partykit) library(ggparty) library(ggplot2) library(party) We have already known how to visualize the distribution information of a single numerical feature using graphs like histogram, boxplot, and violin plot. Using QQ plot to test if this feature has a nromal distribution. Showing relationships between nummerical features using ridgeline plots or scatter plots or with the help of a heatmap. When talking about categorical features, we have mosiac plots, which is quite useful, to deal with. The relationship between features can be somewhat not easy to figure out, like the “diagnal line” problem in the previous homework, the true linear relationship between humid and temp can not be easily interpreted until we find the hiden relation between temp and dew point. In other words, it is often the case that the overall linear relation between two variables are not so significant until we find a proper way to set a partition on our data set. That is, the property of other features plays a role of adjusting or even controling the relation between our interested features. Under this circumstance, we can introduce package ‘ggparty’ to help us do this part. The ggparty package aims to extend ggplot2 functionality to the partykit package. It provides the necessary tools to create clearly structured and highly customizable visualizations for tree-objects of the class ‘party’. 16.1 Introdunction of class ‘party’ # Use WeatherPlay dataset from partykit package data(&quot;WeatherPlay&quot;, package = &quot;partykit&quot;) WeatherPlay ## outlook temperature humidity windy play ## 1 sunny 85 85 false no ## 2 sunny 80 90 true no ## 3 overcast 83 86 false yes ## 4 rainy 70 96 false yes ## 5 rainy 68 80 false yes ## 6 rainy 65 70 true no ## 7 overcast 64 65 true yes ## 8 sunny 72 95 false no ## 9 sunny 69 70 false yes ## 10 rainy 75 80 false yes ## 11 sunny 75 70 true yes ## 12 overcast 72 90 true yes ## 13 overcast 81 75 false yes ## 14 rainy 71 91 true no To represent the data as a tree-based object in partykit, two basic building blocks are used: splits of class ‘partysplit’ and nodes of class ‘partynode’. The resulting recursive partition can then be associated with a data set in an object of class ‘party’. Use ‘partysplit’ function to define split methods for the column: partysplit(varid, breaks = NULL, index = NULL, right = TRUE, prob = NULL, info = NULL) The ‘varid’ means the number of the column that will be split in the dataset (e.g., 1L for outlook, 3L for humidity, 4L for windy etc.). There are two ways to split the column data, one is to split by specfic value for continuous data and one is to split by type for categorial data. Some arbitrary information can be associated with a ‘partysplit’ object by passing it to the ‘info’ argument. sp_o &lt;- partysplit(1L, index = 1:3) sp_h &lt;- partysplit(3L, breaks = 75) sp_w &lt;- partysplit(4L, index = 1:2) Use ‘partynode’ function to define the split methods for nodes in the tree: partynode(id, split = NULL, kids = NULL, surrogates = NULL, info = NULL) The ‘split’ means the partysplit method mentioned above.’id’ is an integer identifier of the node number, ‘split’ is a ‘partysplit’ object, and ‘kids’ is a list of ‘partynode’ objects. Some arbitrary information can be supplied in ‘info’ argument. pn &lt;- partynode(1L, split = sp_o, kids = list( partynode(2L, split = sp_h, kids = list( partynode(3L, info = &quot;yes&quot;), partynode(4L, info = &quot;no&quot;))), partynode(5L, info = &quot;yes&quot;), partynode(6L, split = sp_w, kids = list( partynode(7L, info = &quot;yes&quot;), partynode(8L, info = &quot;no&quot;))))) py &lt;- party(pn, WeatherPlay) In the end, the ‘party’ function will split the data and record the splitting process in the shape of a tree. print(py) ## [1] root ## | [2] outlook in sunny ## | | [3] humidity &lt;= 75: yes ## | | [4] humidity &gt; 75: no ## | [5] outlook in overcast: yes ## | [6] outlook in rainy ## | | [7] windy in false: yes ## | | [8] windy in true: no plot(py) 16.2 Use ‘ggparty’ to visualize the tree Use ggparty function to visualize the object of class ‘party’ into a tree-based type. * geom_edge() draws the edges between the nodes * geom_edge_label() labels the edges with the corresponding split breaks * geom_node_label() labels the nodes with the split variable, node info or anything else. The shorthand versions of this geom geom_node_splitvar() and geom_node_info() have the correct defaults to write the split variables in the inner nodes or the info in the terminal nodes * geom_node_plot() creates a custom ggplot at the location of the node ggparty(py) + geom_edge() + geom_edge_label() + geom_node_label(aes(label = splitvar), ids = &quot;inner&quot;) + geom_node_label(aes(label = info), ids = &quot;terminal&quot;) 16.3 Customize the tree Change color and size of the nodes. ggparty(py) + geom_edge() + geom_edge_label() + # map color to level and size to nodesize for all nodes geom_node_splitvar(aes(col = factor(level), size = nodesize)) + geom_node_info(aes(col = factor(level), size = nodesize)) 16.4 Add plots to the tree Generate a new ‘party’ object. Set ‘fitted’ and ‘response’ variables for the ‘party’ object. That is the two variables that we want to find the correlation. n1 &lt;- partynode(id = 1L, split = sp_o, kids = lapply(2L:4L, partynode)) t2 &lt;- party(n1, data = WeatherPlay, fitted = data.frame( &quot;(fitted)&quot; = fitted_node(n1, data = WeatherPlay), &quot;(response)&quot; = WeatherPlay$play, check.names = FALSE), terms = terms(play ~ ., data = WeatherPlay) ) t2 &lt;- as.constparty(t2) New split type for the dataset. Adding fitted and response variables. print(t2) ## ## Model formula: ## play ~ outlook + temperature + humidity + windy ## ## Fitted party: ## [1] root ## | [2] outlook in sunny: no (n = 5, err = 40.0%) ## | [3] outlook in overcast: yes (n = 4, err = 0.0%) ## | [4] outlook in rainy: yes (n = 5, err = 40.0%) ## ## Number of inner nodes: 1 ## Number of terminal nodes: 3 To visualize the distribution of the variable play we will use the geom_node_plot() function. It allows us to show the data of each node in its separate plot. For this to work, we have to specify the argument gglist. Basically we have to provide a ‘list’ of all the ‘gg’ components we would add to a ggplot() call on the data element of a node. Because the response variable is categorial, use barchart to do the visualization part. ggparty(t2) + geom_edge() + geom_edge_label() + geom_node_splitvar() + geom_node_plot(gglist = list(geom_bar(aes(x = &quot;&quot;, fill = play), position = position_fill()), xlab(&quot;play&quot;))) 16.5 Application So how to dig this hiden information out, with the help of explortary data analysis? Our group will show you how to use recursive partition method and visualization techniques to find some interesting patterns that are difficult to find out without knowing this technique. 16.5.1 Categorical vs Numerical We first will show you how to automatically visualize impact of other variables on the relation between categirical and numerical variables. The dataset we will use is Pima Indians Diabetes Database. data(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) head(PimaIndiansDiabetes2) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 NA 33.6 0.627 50 pos ## 2 1 85 66 29 NA 26.6 0.351 31 neg ## 3 8 183 64 NA NA 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 NA NA 25.6 0.201 30 neg In this dataset, it is natrual to think that plasma glucose concentration glucose is an important predictor for diabetes. Suppose we want to explore the relation between glocose and diabetes. We first take the summary of the data. And result shows that we have 768 observations but variable tricpes and insulin have too many NAs so we decide not using these two variables. Then we delete NAs of other variables and get our dataframe PimaIndianDiabetes. str(PimaIndiansDiabetes2) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 NA 70 96 ... ## $ triceps : num 35 29 NA 23 35 NA 32 NA 45 NA ... ## $ insulin : num NA NA NA 94 168 NA 88 NA 543 NA ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 2 1 2 1 2 1 2 1 2 2 ... summary(PimaIndiansDiabetes2) ## pregnant glucose pressure triceps ## Min. : 0.000 Min. : 44.0 Min. : 24.00 Min. : 7.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 64.00 1st Qu.:22.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :29.00 ## Mean : 3.845 Mean :121.7 Mean : 72.41 Mean :29.15 ## 3rd Qu.: 6.000 3rd Qu.:141.0 3rd Qu.: 80.00 3rd Qu.:36.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## NA&#39;s :5 NA&#39;s :35 NA&#39;s :227 ## insulin mass pedigree age diabetes ## Min. : 14.00 Min. :18.20 Min. :0.0780 Min. :21.00 neg:500 ## 1st Qu.: 76.25 1st Qu.:27.50 1st Qu.:0.2437 1st Qu.:24.00 pos:268 ## Median :125.00 Median :32.30 Median :0.3725 Median :29.00 ## Mean :155.55 Mean :32.46 Mean :0.4719 Mean :33.24 ## 3rd Qu.:190.00 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.00 Max. :67.10 Max. :2.4200 Max. :81.00 ## NA&#39;s :374 NA&#39;s :11 PimaIndiansDiabetes &lt;- na.omit(PimaIndiansDiabetes2[,-c(4, 5)]) We first want to use ggplot2 to show the relation. Diabetes is a categorical variable while glucose is a numerical variable. So we use density curve and boxplot to show. From these two plots we can get information: People with positive diabetes have higher glucose than people have negative diabetes. More outliers(larger than normal) occur in glucose distribution under people with negative diabetes. The glucose distribution under people with positive diabetes seem to have 2 peeks and is not normally distributed. From these information we find that although generally people with positive diabetes have higher glocose, but how other varibles can affect this property(so that there are two peeks in glucose distribution of people with positive diabetes)? It is hard to tell from the plots we get so far. Maybe we can try to facet on other variables to see what happens. But when other variables are numerical, we have to first do discretization, and how wide the bin will be can just be another problem. So here is time for using Model-Based Recursive Partitioning method to help us find what the significant node is and how to visualize it. ggplot(PimaIndiansDiabetes, aes(glucose, fill = diabetes, colour = diabetes)) + geom_density(alpha = 0.1) + xlim(0,250) + ggtitle(&quot;glucose distribution under different diabetes type&quot;) ggplot(PimaIndiansDiabetes, aes(x=diabetes, y=glucose, fill=diabetes))+ geom_boxplot()+ ggtitle(&quot;glucose range under diffrent diabete type&quot;) From these information we get we find that although generally people with positive diabetes have higher glocose, but how other varibles can affect this property(so that there are two peeks in glucose distribution of people with positive diabetes)? It is hard to tell from the plots we get so far. Maybe we can try to facet on other variables to see what happens. But when other variables are numerical, we have to first do discretization, and how wide the bin will be can just be another problem. So here is time for using Model-Based Recursive Partitioning method to help us find what the significant node is and visualize it. Here we will use mob function from party library. We can directly read the print result to get some interesting information. This tree model shows three patterns about relation of glucose and diabetes. Node 2 Women with low body mass index that have on average a low risk of diabetes, however this increases clearly with glucose level. Node 4 Women with average and high body mass index, younger than 30 years, that have a higher avarage risk that also increases with glucose level. Node 5 Women with average and high body mass index, older than 30 years, that have a high avarage risk that increases only slowly with glucose level. #tree fmPID &lt;- mob(diabetes ~ glucose | pregnant + pressure + mass + pedigree + age, data = PimaIndiansDiabetes, model = glinearModel, family = binomial()) #show information print(fmPID) ## 1) mass &lt;= 26.3; criterion = 1, statistic = 43.409 ## 2)* weights = 148 ## Terminal node model ## Binomial GLM with coefficients: ## (Intercept) glucose ## -10.99945 0.06457 ## ## 1) mass &gt; 26.3 ## 3) age &lt;= 30; criterion = 1, statistic = 33.705 ## 4)* weights = 292 ## Terminal node model ## Binomial GLM with coefficients: ## (Intercept) glucose ## -6.57307 0.04504 ## ## 3) age &gt; 30 ## 5)* weights = 284 ## Terminal node model ## Binomial GLM with coefficients: ## (Intercept) glucose ## -3.31857 0.02748 Printed information are always bad for illustration, we can simply use plot function to get a better result. And the three patternd talked are quite obvious in the graph. # plot it plot(fmPID) We can compute the odds ratio for each group. On average, the odds increase by 6.7%, 4.6% and 2.8% with respect to glucose in the three groups. There is way more information than just a single boxplot or distribution plot. # show odds ratios exp(coef(fmPID)[,2]) ## 2 4 5 ## 1.066698 1.046075 1.027861 16.5.2 Numerical vs Numerical PimaIndianDiabetes dataset shows the case to explore relation between one numerical variable and one categorical variable. When we have two numerical variables, we can also use this method to show more detailed patterns. This time we will use ggparty to make our plot more pretty. TeachingRatings is a dataset on course evaluations, course characteristics, and professor characteristics for 463 courses for the academic years 2000–2002 at the University of Texas at Austin. data(&quot;TeachingRatings&quot;, package = &quot;AER&quot;) head(TeachingRatings) ## minority age gender credits beauty eval division native tenure students ## 1 yes 36 female more 0.2899157 4.3 upper yes yes 24 ## 2 no 59 male more -0.7377322 4.5 upper yes yes 17 ## 3 no 51 male more -0.5719836 3.7 upper yes yes 55 ## 4 no 40 female more -0.6779634 4.3 upper yes yes 40 ## 5 no 31 female more 1.5097940 4.4 upper yes yes 42 ## 6 no 62 male more 0.5885687 4.2 upper yes yes 182 ## allstudents prof ## 1 43 1 ## 2 20 2 ## 3 55 3 ## 4 46 4 ## 5 48 5 ## 6 282 6 Now we want to know if there relation between evaluation scores and teachers’ physical appearance. We first use a scatter plot to show that. We can generally see that beauty and eval has a positive corelation. As teachers’ rating of physical appearence goes higher, the evaluation score also goes higher. ggplot(TeachingRatings, aes(x=beauty, y=eval)) + geom_point() + ggtitle(&quot;scatter plot on beauty and eval&quot;) Faceted by minority and division, and lengended by gender, we see more detailed patterns betwenn beauty and eval. ggplot(data=TeachingRatings,aes(x=beauty, y=eval, color=gender))+ geom_point(alpha=0.5, size=0.8)+ geom_smooth(method=lm, se=FALSE)+ facet_grid(minority ~ division, margins=TRUE)+ labs(title=&quot;scatter plot on beauty and eval with gender as legend&quot;, subtitle = &quot;faeted by minority and division&quot;) If we want to see whether some categorical variables in the dataset also can affect the coreltion we found so far, traditionally we can use facet and legend by hand with ggplot2. But what variable to use is a big problem if there are many. Things can be more complicated when we want to see if a numerical variable has an effect on out found corelation. We have choose a proper bin to change the numericla variable to a factor. However, things can be much more easier if we use Model-Based Recursive Partitioning method. Here we will use lmtree method in partykit package and plot the result. data(&quot;TeachingRatings&quot;, package = &quot;AER&quot;) tr_tree &lt;- lmtree(eval ~ beauty | minority + age + gender + division + native + tenure, data = TeachingRatings, weights = students, caseweights = FALSE) plot(tr_tree) To make our plot prettier, we use package from ggparty. An interesting pattern occurs in node 8. More beautiful a female instructor more than 40 years old teaching an upper class is, lower evaluation score she has. ggparty(tr_tree, terminal_space = 0.5, add_vars = list(p.value = &quot;$node$info$p.value&quot;)) + geom_edge(size = 1.5) + geom_edge_label(colour = &quot;grey&quot;, size = 6) + geom_node_plot(gglist = list(geom_point(aes(x = beauty, y = eval), alpha = 0.5), theme_bw(base_size = 15)), scales = &quot;fixed&quot;, id = &quot;terminal&quot;, shared_axis_labels = T, shared_legend = T, legend_separator = T, predict = &quot;beauty&quot;, predict_gpar = list(col = &quot;blue&quot;, size = 1.2) ) + geom_node_label(aes(col = splitvar), line_list = list(aes(label = paste(&quot;Node&quot;, id)), aes(label = splitvar), aes(label = paste(&quot;p =&quot;, formatC(p.value, format = &quot;e&quot;, digits = 2)))), line_gpar = list(list(size = 10, col = &quot;black&quot;, fontface = &quot;bold&quot;), list(size = 18), list(size = 10)), ids = &quot;inner&quot;) + geom_node_label(aes(label = paste0(&quot;Node &quot;, id, &quot;, N = &quot;, nodesize)), fontface = &quot;bold&quot;, ids = &quot;terminal&quot;, size = 5, nudge_y = 0.01) + theme(legend.position = &quot;none&quot;) And we can take the summary of node 8 to further explore. summary(tr_tree, node = 8) ## ## Call: ## lm(formula = eval ~ beauty) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -6.4288 -1.8997 0.5785 1.8240 4.8163 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.01371 0.05626 71.343 &lt;2e-16 *** ## beauty 0.12221 0.05139 2.378 0.0203 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.617 on 67 degrees of freedom ## Multiple R-squared: 0.07785, Adjusted R-squared: 0.06408 ## F-statistic: 5.656 on 1 and 67 DF, p-value: 0.02026 "],
["multi-class-hexbins.html", "Chapter 17 Multi-class hexbins", " Chapter 17 Multi-class hexbins Hritik Jain and Shahen Mirzoyan In one of her lectures, Prof. Robbins introduces students to hexagonal heatmaps as one of the most common methods for visualizing the observed frequency of two variables in a dataset. We came across a variation of this visualization tool known as multi-class hexbins. Multi-class hexbins allow you to see two features of a dataset while accounting for a third categorical variable. The third variable is visualized by dividing up each hexbin so that every class is represented proportionally to its observed frequency. In this tutorial, we will demonstrate the R package hextri. As a running example, let’s pick up on the same assignment problem where we first got to use hexagonal heatmaps - “weather” data from the R package “nycflights13”. Here is the hexagonal heatmap that shows the heatmap of wind_dir and humid. library(tidyverse) data(&quot;weather&quot;, package=&quot;nycflights13&quot;) ggplot(weather, aes(humid, wind_dir)) + geom_hex(bins=20) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + xlab(&quot;Relative Humidity&quot;) + ylab(&quot;Wind direction (degrees)&quot;) + theme(plot.margin = margin(1.8,.7,1.8,.7, &quot;cm&quot;), panel.border = element_rect(colour = &quot;black&quot;, fill=NA), panel.background = element_rect(fill=&#39;white&#39;, color=&#39;white&#39;)) The heatmap above includes all the three source airports (variable orging in the dataset) - LGA, JFK and EWR while displaying counts in the different hex bins. Now if we want to visualize the same information separately for each of the airports, we could do faceting on origin. But what if we could do that without faceting, in one panel? library(hextri) with(weather, hextri(humid, wind_dir, class=origin, colour=c(&quot;orange&quot;,&quot;green&quot;, &quot;blue&quot;), nbins=20,xlab=&quot;Relative humidity&quot;,ylab=&quot;Wind direction (degrees)&quot;)) The above plot represents each of the three airports in our dataset as a separate color, and the representation of each airport within a given hexbin is proportional to the observed frequency for that airport. The default behavior of hextri is to use alpha to represent the weight of each bin. However, the function also gives us the flexibility to use hexagon size to represent the weight of each bin. with(weather, hextri(humid, wind_dir, class=origin,style=&quot;size&quot;,colour=c(&quot;orange&quot;,&quot;green&quot;, &quot;blue&quot;), nbins=20,xlab=&quot;Relative humidity&quot;,ylab=&quot;Wind direction&quot;)) For classes that have less than 1/12th representation, hextri allows us to use the optional diffuse parameter which takes these underrepresented classes and divides them up evenly among nearby hexbins. with(weather, hextri(humid, wind_dir, class=origin,colour=c(&quot;orange&quot;,&quot;green&quot;, &quot;blue&quot;), nbins=20,xlab=&quot;Relative humidity&quot;, ylab=&quot;Wind direction&quot;, diffuse=TRUE)) For more use cases and further customization options, please refer to the hextri documentation "],
["visualization-in-time-series-analysis.html", "Chapter 18 Visualization in Time Series Analysis 18.1 Initiate a Time series object: 18.2 Plot the data: 18.3 Transformation of nonstationary: 18.4 ACF and PACF for time series 18.5 Full model: Yt = T(Trend) + S(Seasonality) +C(Cycle)", " Chapter 18 Visualization in Time Series Analysis Yihao Li (yl4326) Note: All the pseudo-documentation introduction will only include parameter I used often, it’s not the full parameter set. Basic Settings #In case there&#39;s some new package... #install.packages(c(&quot;tidyverse&quot;, &quot;stats&quot;, &quot;forecast&quot;, &quot;dynlm&quot;, &quot;lubridate&quot;, # &quot;strucchange&quot;,&quot;sarima&quot;)) library(tidyverse) library(stats) # a lot of basic operations here library(forecast) # fantastic package with a ton of shortcut library(dynlm) # if you need some linear regression on lag of data library(lubridate) # I didn’t include any function here but necessary for time series library(strucchange) # parameter stability test #library(zoo) #used by forecast, will be automatically included require(&quot;PolynomF&quot;) library(sarima) # not for the analysis, just for simulation in showing 18.1 Initiate a Time series object: stats::ts(data = NA, start = 1, frequency = 1) where: data is the data input start refers to the time of the first observation. Frequency the number of observations per unit of time. 18.2 Plot the data: plot(time, value) Traditional built-in plot, or, ggplot2::autoplot(object) powerful plotting function not only for time series analysis but also for model decomposition object, model fit object, even forecast models. library(expsmooth) # use a dataset cangas here ts_data = cangas autoplot(ts_data) + ylab(&quot;Canadaian Gas Production&quot;) 18.3 Transformation of nonstationary: 18.3.1 Stationarity: Required by most of the time series analysis tools: 1. First Order Weakly Stationary: all R.V.s have the same means. 2. Second Order Weakly Stationary (Covariance Stationary) a. Necessary for most of the time series analysis. b. All R.V.s have same variables, same variances. c. The correlation of time variable only depend on the time difference, i.e. \\(\\rho(Y_t, Y_{t-k}) = \\rho(|k|)\\). 18.3.2 Operations From the plot we have a intuition of its stationarity and we can do following: 1. First Order Weakly Stationary: Take the first difference \\((Y_t – Y_{t-1})\\) of the data. 2. second order weakly stationary: Take the log and then take the first difference(i.e. \\((\\log(y_T) – \\log(y_{T-1}))\\)of this transformed series. 3. Growth rate. ts_data_diff = diff(ts_data) #first-order difference ts_data_log_diff = diff(log(ts_data)) #first-order log difference ts_data_growth_rate = diff(ts_data)/ stats::lag(ts_data, k=-1) #growth rate 18.4 ACF and PACF for time series Autocovariance Function(ACF): \\(corr(y_t ,y_{t-k})\\) Partial Autocorrleation Function(PACF): the autocorrelataion between \\(Y_t\\) and \\(Y_{t+k}\\) after removing(conditioning on) all the observation in between. To save your code line, there’s forecast::tsdisplay(ts_data) which plots the ggplot2::autoplot(), stats::acf(), and stats::pacf() together. It also have a very friendly forecast::ggtsdisplay(ts_data) which returns a ggplot object for you, however, ggtitle() behaves weird in this graph, not to use ggtitle with it. White Noise: Time series process with zero mean, constant variance, and no serial correlation. Most of time use Gaussian y_t ~N(0, \\(\\sigma\\)^2). For pure white noise, both ACF and PACF should be 0, only k = 0 will have ACF = PACF = 1. stats::acf(ts_data_diff) stats::pacf(ts_data_diff) tsdisplay(ts_data_diff) ggtsdisplay(ts_data_diff) 18.5 Full model: Yt = T(Trend) + S(Seasonality) +C(Cycle) Time is the money my friend, you don’t need to waste time to guess the seasonality pattern. 1. stats::decompose(ts_data, type = c(“additive”, “multiplicative”)) Decompose a time series into seasonal, trend and irregular components using moving average. Where type stands for additive/multiplicative seasonal cpomponent. 2. stats::stl(ts_data, s.window) Decompose a time series into seasonal, trend and irregular components using loess. 3. forecast::seasadj(object) Takes in a decompose or stl object. Returns seasonally adjusted data. constructed by removing the seasonal component. 4. You can also find the decomposition data in model. for decomposed.ts object, it’s in x()(original) seasonal, trend, random(remainder). for stl object it’s in time.series(seasonal, trend, and remainder). fit &lt;- decompose(ts_data, type=&#39;additive&#39;) # I use original data here! autoplot(fit) fit %&gt;% seasadj() %&gt;% autoplot() + ggtitle(&quot;Seasonally adjusted data&quot;) fit$trend %&gt;% autoplot() + ggtitle(&quot;Trend&quot;) 18.5.1 Trend(T): Linear, Quadratic, etc. For normal linear model stats::lm(formula, data, na.action) For normal linear regression; dynlm::dynlm(formula, data, na.action) For dynamic linear regression; na.action is optional but I think it saves life from cleaning data. 18.5.2 Seasonality(S): Direct visualization: decompose and stl. one-line beautiful visualization: forecast::seasonplot(ts_data) and it’s ggplot version forecast::ggseasonplot(ts_data). ggseasonplot(ts_data) 18.5.3 Cycle(C): ARIMA family: often used, powerful model in cycle analysis. Notice the model selection is quite subjective except the ARIMA one. 18.5.3.1 1. MA(q) Moving average: \\(Y_t – μ = \\sum_1^q \\theta_i \\epsilon_{t-i}\\) ACF: significant spikes in first q position PACF: decaying in absolute value ma2.sim&lt;-arima.sim(model=list(ma=c(0.7,0.3)),n=100) tsdisplay(ma2.sim) 18.5.3.2 2. AR(p) Autoregressive: \\(Y_t = c +\\sum_1^p \\phi_i Y_{t-i} +\\epsilon_t\\) ACF: decaying in absolute value PACF: significant spikes in first q position. ar2.sim &lt;- arima.sim(model=list(ar=c(0.5,0.1)), n=500) tsdisplay(ar2.sim) 18.5.3.3 3. Seasonal Model with parameter s: s stands for the period, e.g. 4 for quarterly data, 12 for monthly data, etc. 18.5.3.3.1 a. Seasonal-MA(q) Model with parameter s: \\(Y_t – μ = \\sum_1^q\\theta_{is} \\epsilon_{t-is} + \\epsilon_t\\) ACF: spikes at s, 2s,… , ps PACF: spikes at ns decaying s4ma1.sim &lt;- sim_sarima(n=144, model = list(ma=c(rep(0,3),0.8))) # SMA(1), 4 quarters tsdisplay(s4ma1.sim) 18.5.3.3.2 b. Seasonal-AR(p) Model with parameter s: \\(Y_t = c +\\sum_1^p \\phi_{is} Y_{t-is} +\\epsilon_t\\) ACF: spikes at ns decaying PACF: spikes at s, 2s,… , ps s4ar1.sim &lt;- sim_sarima(n=144, model = list(ar=c(rep(0,3),0.8))) # SAR(1), 4 quarters tsdisplay(s4ar1.sim) 18.5.3.4 4. ARMA(p,q) Autoregressive Moving Average: ARMA(p,q) = AR(p) + MA(q), inherited the characters from both AR and MA Saves parameters: ARMA(1,1) is performing better than AR(3). very subjective in the value selection of p and q… ar2ma2.sim&lt;-arima.sim(model=list(ar=c(0.9,-0.2),ma=c(-0.7,0.1)),n=100) #ARMA(2,2) tsdisplay(ar2ma2.sim) 18.5.3.5 5. ARIMA(p,d,q) Autoregressive integrated moving average: stationary and invertible ARMA(p,q) after differencing d times. a. Deal with nonstationary series b. Include d order difference, to remove d or lower order trends. c. Also deal with stochastic trends d. Quantitative one-line data choose forecast::auto.arima(y, ic = c(“aicc”, “aic”, “bic”)) where y is a univariate time series, ic is the criterion. Returns best ARIMA model with seasonsal ARIMA according to either AIC, AICc or BIC value. data = read.table(&quot;resources/Visualization_in_Time_Series_Analysis/w-gs1yr.txt&quot;, header = TRUE) ts_data = ts(data$rate, start = 1962, deltat = 1/52, freq = 52) autoplot(ts_data)+ ggtitle(&quot;US Weekly Interest Rates (%)&quot;)+ ylab(&quot;Interest Rate&quot;) tsdisplay(diff(ts_data)) t = seq(1962, length = length(ts_data), by = 1/52) fit = auto.arima(diff(ts_data)) summary(fit) ## Series: diff(ts_data) ## ARIMA(1,0,2) with zero mean ## ## Coefficients: ## ar1 ma1 ma2 ## 0.6284 -0.3065 -0.0527 ## s.e. 0.0642 0.0675 0.0299 ## ## sigma^2 estimated as 0.03143: log likelihood=768.32 ## AIC=-1528.65 AICc=-1528.63 BIC=-1505.41 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.0006226569 0.1771898 0.1047296 NaN Inf 0.6423691 -0.0002639336 plot(t[-1], diff(ts_data), main = &quot;US Weekly Interest Rates (%) (first difference)&quot;, ylab = &quot;Rates&quot;, xlab = &quot;Time&quot;, type = &quot;l&quot;) lines(fit$fitted, col = &quot;blue&quot;) 18.5.3.6 Aside: AIC(Akaike Information Criterion) biased towards overparameterized models asymptotically efficient stats::AIC(fit, k=2) where: fit is the a fitted model object, k is the penalty per parameter to be used; the default k = 2 is the classical AIC. BIC/SIC(Bayesian (Schwarz) Information Criterion) Consistent not asymptotically efficient stats::BIC(fit) where fit is the a fitted model object Both AIC and BIC are the smaller the better. 18.5.4 Summary 18.5.4.1 Steps The full model includes a trend, seasonal dummies, and cyclical dynamics For a given visualized data: a. Extract the trend and seasonality (stl) b. ACF &amp; PACF, Remove any possible dynamics(tsdisplay, auto.arima) c. Check if the residuals looks like a white noise 18.5.4.2 Residuals vs White Noise #this fit is following the last chunk tsdisplay(resid(fit)) forecast::checkresiduals(fit) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,0,2) with zero mean ## Q* = 401.53, df = 101, p-value &lt; 2.2e-16 ## ## Model df: 3. Total lags used: 104 Note: 1. Object in resid() can be either a time series model, a forecast object, or a time series (assumed to be residuals). 2. In the new graph, the scale of ACF and PACF is significantly smaller, thus we have removed some of the dynamics. 18.5.4.3 Test Parameter stability CUSUM(Cumulative Sum of Standardized Recursive Residuals): Check if the CUSUM leave the confidence interval to capture the dynamics. Recursive Residuals: strucchange::recresid(x, y, formula) autoplot(ts(recresid(fit$res ~ t[-1])))+ ggtitle(&quot;The recursive residual of ARIMA(1,0,2)&quot;)+ ylab(&quot;recursive residual&quot;) Empirical Fluctuation Processes: efp(x, y, formula, type = “Rec-CUSUM”) plot(efp(fit$res ~ t[-1], type = &quot;Rec-CUSUM&quot;)) Note: 1. In the recursive residual’s visialization, we find some instability in the middle. 2. Empirical Fluctuation Processes showed the parameter’s stability is acceptable. 18.5.5 Reference: www.rdocumentation.org My sincere appreciation to Dr. Randall Rojas (UCLA Econ 144 19’ Spring)’s work. The general model’s idea, both data set: cangas and the tidied “w-gs1yr.txt” is from the Econ 144 class. "],
["how-to-plot-likert-data.html", "Chapter 19 How to plot likert data 19.1 Introduction 19.2 Diverging stacked bar chart using function likert() 19.3 Data cleaning and preparation 19.4 Stacked bar chart using ggplot() 19.5 Summary", " Chapter 19 How to plot likert data Zining Fan and Xinyuan He 19.1 Introduction The Likert scale is a five (or seven) point scale which is used to allow the individual to express how much they agree or disagree with a particular statement. A common set of 5 answers can be selected are: Strong Disagree, Disagree, Don’t Know/Not Sure, Agree and Strongly Agree. To visualize the result of a Likert scale datasets, we can use diverging stacked bar chart or stacked bar chart, which we will introduce later in this chapter. 19.2 Diverging stacked bar chart using function likert() Here is an example of how to plot likert data. We could use function likert(), it is in the package “likert”. The plot looks like a diverging bar chart. The code and the plot are shown below, # use function likert() to plot likert data HH::likert(DBN~., b, positive.order=TRUE, as.percent = TRUE, main=&quot;At my child&#39;s school my child is safe.&quot;, xlab=&quot;percentage&quot;,ylab=&quot;School Code&quot;) In this picture, the statement is “At my child’s school my child is safe.”. We could see that the percentage of each catogary for different schools, the “don’t know” is set at the middle in the likert plot. 19.3 Data cleaning and preparation Actually, before we plot, the first thing we need to do is to tidy data. Transforming data to the format we want is very important and more challengeable than ploting. After transforming, the tidy data looks like, # view data frame b ## DBN StronglyDisagree Disagree DontKnow Agree StronglyAgree ## 1 01M509 0.27 0.43 0.13 0.13 0.03 ## 2 02M374 0.32 0.57 0.05 0.05 0.01 ## 3 02M520 0.17 0.61 0.03 0.12 0.06 ## 4 11X545 0.10 0.46 0.16 0.19 0.10 ## 5 12X271 0.37 0.40 0.12 0.09 0.02 ## 6 13K605 0.27 0.57 0.08 0.06 0.02 ## 7 29Q156 0.25 0.54 0.07 0.09 0.04 In the table above, each line represents the data from school. The data is in percentage format. If you want to know more about this dataset, you could go to https://www.schools.nyc.gov/about-us/reports/school-quality/nyc-school-survey. There is another choice to plot the dataset. In the likert data, the neutral data or “don’t know” data is the least important, so we could exclude these data. The plot looks like : # exclude &quot;don&#39;t know&quot; bb &lt;- b[,c(1,2,3,5,6)] HH::likert(DBN~., bb, positive.order=TRUE,as.percent = TRUE, main=&quot;At my child&#39;s school my child is safe.(without don&#39;t know)&quot;, xlab=&quot;percentage&quot;,ylab=&quot;School Code&quot;) In the plot above, we could clearly compare the percentage of “Agree” and “StronglyAgree” to “Disagree” and “StronglyDisagree”. 19.4 Stacked bar chart using ggplot() Another way to plot the likert data is to use stacked bar chart. We can use ggplot() and gemo_bar to implement the stacked bar chart as shown in the example below. Before drawing the stacked bar chart with ggplot(), we need to reorganize the dataset into the following format: mb = melt(b) mb ## DBN variable value ## 1 01M509 StronglyDisagree 0.27 ## 2 02M374 StronglyDisagree 0.32 ## 3 02M520 StronglyDisagree 0.17 ## 4 11X545 StronglyDisagree 0.10 ## 5 12X271 StronglyDisagree 0.37 ## 6 13K605 StronglyDisagree 0.27 ## 7 29Q156 StronglyDisagree 0.25 ## 8 01M509 Disagree 0.43 ## 9 02M374 Disagree 0.57 ## 10 02M520 Disagree 0.61 ## 11 11X545 Disagree 0.46 ## 12 12X271 Disagree 0.40 ## 13 13K605 Disagree 0.57 ## 14 29Q156 Disagree 0.54 ## 15 01M509 DontKnow 0.13 ## 16 02M374 DontKnow 0.05 ## 17 02M520 DontKnow 0.03 ## 18 11X545 DontKnow 0.16 ## 19 12X271 DontKnow 0.12 ## 20 13K605 DontKnow 0.08 ## 21 29Q156 DontKnow 0.07 ## 22 01M509 Agree 0.13 ## 23 02M374 Agree 0.05 ## 24 02M520 Agree 0.12 ## 25 11X545 Agree 0.19 ## 26 12X271 Agree 0.09 ## 27 13K605 Agree 0.06 ## 28 29Q156 Agree 0.09 ## 29 01M509 StronglyAgree 0.03 ## 30 02M374 StronglyAgree 0.01 ## 31 02M520 StronglyAgree 0.06 ## 32 11X545 StronglyAgree 0.10 ## 33 12X271 StronglyAgree 0.02 ## 34 13K605 StronglyAgree 0.02 ## 35 29Q156 StronglyAgree 0.04 Then we plot the data, the code and plot are shown below, g2 = ggplot()+ geom_bar(data = mb, aes(x = reorder(DBN,value), y=value, fill=variable), position=&quot;stack&quot;, stat=&quot;identity&quot;)+ coord_flip() + ggtitle(&quot;At my child&#39;s school my child is safe&quot;)+ ylab(&quot;Percentage&quot;)+ xlab(&quot;School Code&quot;)+ scale_fill_brewer(palette=&quot;PRGn&quot;)+ theme(legend.position=&quot;bottom&quot;) #scale_x_discrete(limits=c(&quot;StronglyAgree&quot;, &quot;Agree&quot;, &quot;DontKnow&quot;,&quot;Disagree&quot;,&quot;StronglyDisagree&quot;)) g2 One advantage of using stacked bar chart is that: since these two features “StronglyAgree” and “StronglyDisagree” are placed at the two ends of the bars,the comparisions between them are very clear. While it will be relatively hard to compare features placed in the middle of the bars, like “Disagree” and “Agree”. 19.5 Summary There are many options to visualize likert data as shown in the previous sections, and each with different advantages and disadvantages. You can choose based on your needs. If you would like to find more information you can go to https://blog.datawrapper.de/divergingbars/. "],
["chart-stacked-bar-chart-for-likert-data.html", "Chapter 20 Chart: Stacked Bar Chart (For Likert Data) 20.1 Overview 20.2 Examples 20.3 When to Use 20.4 Considerations 20.5 External Resources &amp; References", " Chapter 20 Chart: Stacked Bar Chart (For Likert Data) Ying Du and Chenlu Jia 20.1 Overview In this chapter, we will discuss about stacked bar charts and its application to deal with likert data. 20.1.1 Stacked Bar Chart A stacked bar graph (or stacked bar chart) is a chart that uses bars to show comparisons between categories of data, but with ability to break down and compare parts of a whole. Each bar in the chart represents a whole, and segments in the bar represent different parts or categories of that whole. 1 20.1.2 Likert Data Likert, used in questionnaires, that is designed to measure people’s attitudes, opinions, or perceptions. Subjects choose from a range of possible responses to a specific question or statement; responses typically include “strongly agree,” “agree,” “neutral,” “disagree,” and “strongly disagree.” 2 20.2 Examples 20.2.1 Simple Stacked Bar Chart Data: useR2016 dataset in the forwards package (available on CRAN) This data set contains results from a survey conducted by Forwards of attendees at useR! 2016, the R user conference held at Stanford University, Stanford, California, June 27 - June 30 2016. Modifications made to anonymize the data are noted in Details. install.packages(&quot;forwards&quot;, repos = &quot;http://cran.us.r-project.org&quot;) useR2016 = forwards::useR2016 Now we want to create a horizontal stacked bar chart showing the proportion of respondents for each level of Q11 who are over 35 vs. 35 or under. And here’s the code: library(ggplot2) library(tidyverse) #get rid of the &#39;NA&#39; data in the dataset noNuseR2016 &lt;- useR2016 %&gt;% dplyr::filter(Q3 != &#39;NA&#39;&amp;Q11 != &#39;NA&#39;) #plot the graph without NA ggplot(noNuseR2016) + geom_bar(aes(x = Q11, fill = Q3), position = &#39;fill&#39;)+ ylab(&#39;Proportion&#39;)+ xlab(&#39;Years&#39;)+ ggtitle(&#39;Years of using R according to Ages&#39;)+ coord_flip() According to the graph above, we can tell a lot of information. For example, the proportion of people over 35 years old in the group with people using R over 10 years is much larger than those in other groups. This can be inferred by the length since the width of each bar is the same. Now let’s take a look at a more complicated example from the same dataset: Create a horizontal stacked bar chart showing the proportional breakdown of Q3 for each level of Q11, faceted on Q2. And here’s the code: #plot the graph of data without &#39;NA&#39; ggplot(noNuseR2016)+ #&quot;fill&quot; sets different colors according to Q11 geom_bar(aes(x = Q11, fill = Q3), position = &#39;fill&#39;, width = 0.5)+ facet_grid(noNuseR2016$Q2)+ coord_flip()+ ylab(&#39;Proportion&#39;)+ xlab(&#39;Age&#39;)+ ggtitle(&#39;Proportions of years using R according to ages and genders&#39;) Here we facet on sex (“facet_grid(noNuseR2016$Q2)”), with men on the top and women at the bottom. we can tell from the graph that the bars for men and women follow the same pattern except the 5-10 years group. Comparing the two graphs above, we have some surprising observations. In the first graph, the proportion of people over 35 years old is smaller than the proportion of people under 35 years old in the 5-10 years group. However, in the second graph, we see that although this pattern is followed by women, it’s the opposite for men. 20.2.2 Likert Data with Stacked Bar Chart Likert data typically contains the responses in the format of “strongly agree,” “agree,” “neutral,” “disagree,” and “strongly disagree.” Here, let’s look at the data of NYC public school survey in 2019. Recently, there has been much debate about the lack of racial and economic diversity at Manhattan District 3 elementary schools, part of a larger and long-standing controversy about iniquities in the New York City public school system as a whole.The New York Times article, “Rezoning Plan to Remake 3 Upper West Side Schools Will Proceed, City Says,” (https://www.nytimes.com/2016/11/10/nyregion/rezoning-plan-for-3-upper-west-side-schools-will-proceed-city-says.html) (2016-11-10) identifies the 11 elementary schools in Manhattan District 3. #load the dataset, use the function for xlsx file df &lt;- readxl::read_xlsx(&#39;resources/Likert/2019-public-data-file_parent.xlsx&#39;, sheet=2) #choose the schools by names (rows) dfNew &lt;- df %&gt;% filter(DBN %in% c(&#39;03M165&#39;,&#39;03M145&#39;,&#39;03M163&#39;,&#39;03M075&#39;,&#39;03M084&#39;,&#39;03M166&#39;,&#39;03M009&#39;,&#39;03M087&#39;,&#39;03M452&#39;,&#39;03M199&#39;,&#39;03M191&#39;)) #choose a question and answers by column indies (columns) #For example, we choose : If school staff regularly communicate with me about how I can help my child learn. dfNew &lt;- dfNew %&gt;% dplyr::select(c(1,2,4:7)) #set new names for the chosen columns as they were randomly assigned names #before. Here we use the likert levels as column names. names(dfNew)[2:6] &lt;- c(&quot;School_Name&quot;, &quot;Strongly_disagree&quot;,&quot;Disagree&quot;,&quot;Agree&quot;,&quot;Strongly_agree&quot;) #change the percentage columns to numeric type dfNew$DBN &lt;- as.factor(dfNew$DBN) dfNew$School_Name &lt;- as.factor(dfNew$School_Name) dfNew &lt;- dfNew %&gt;% mutate_if(is.character, function(x) as.numeric(x)) head(dfNew) ## # A tibble: 6 x 6 ## DBN School_Name Strongly_disagr… Disagree Agree Strongly_agree ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 03M009 P.S. 009 SARAH ANDERSON 3 4 40 53 ## 2 03M075 P.S. 075 EMILY DICKINSON 2 3 56 39 ## 3 03M084 P.S. 084 LILLIAN WEBER 3 9 51 37 ## 4 03M087 P.S. 087 WILLIAM SHERMAN 0 7 49 44 ## 5 03M145 P.S. 145, THE BLOOMINGD… 2 3 49 45 ## 6 03M163 P.S. 163 ALFRED E. SMITH 4 12 50 35 Now, draw a diverging stacked bar chart to show results for the 11 schools identified in the article referenced above: And here’s the code: HH::likert(DBN ~ ., dfNew, main=&quot;School staff regularly communicate with me about \\nhow I can help my child learn.&quot;) Notice that in the bar charts of last question, the bars are aligned at both the left and right sides. However, in this diverging stacked bar chart, the bars are aligned by the neutral values. In our example, there is no neutral value, so the bars are aligned by the gapline between agree and disagree. In this graph, we can clearly compare people’s positive attitude vs. negative attitude for each item. 20.3 When to Use When the bars consist of only two segments (e.g., male and female). When we need to compare the sum of multiple parts among multiple bars. When we need to compare the percentages of responses to Likert scales. 3 20.4 Considerations 20.4.1 Interpretation of stacked bar charts: We can only tell the proportion for each category under a certain group, but we cannot make any inference about the category size or group size. 20.4.2 Alignings in Diverging Stacked Bar Charts: 20.4.2.1 100% Stacked Bar Chart This particular design does work well for the following purposes: Comparing Strongly Disagree percentages Comparing the combination of Strong Disagree and Disagree percentages Comparing Strongly Agree percentages Comparing the combination of Agree and Strongly Agree percentages Reading the percentage values for Strongly Disagree Reading the percentage values for the sum of Strongly Disagree and Disagree However, it does not work well for the following purposes: Comparing Disagree percentages Comparing Neutral percentages Comparing Agree percentages Reading percentage values of the individual segments Disagree, Neutral, -Agree, or Strongly Agree, because mental math is required Reading the percentage values for the sum of Agree and Strongly Agree, because mental math is required 4 20.4.2.2 Diverging Stacked Bar Chart with Neutral in Middle For some purposes, the Neutral results may be eliminated altogether, and for some it may be appropriate to split the Neutral results down the middle, displaying half of them as negative and half as positive, as above. 5 20.4.2.3 Diverging Stacked Bar Chart with Neutral Held Aside In most of the cases, negative results (e.g., Strongly Disagree and Disagree) as negative values running left from zero and positive results (e.g., Agree and Strongly Agree) as positive values running right from zero, as shown above. Designed in this way, differences between positive and negative results now stand out a bit more, the sum of Agree and Strongly Agree are easier to read, and the Neutral values are both easier to read and compare. 6 20.5 External Resources &amp; References https://businessq-software.com/2017/02/21/stacked-bar-chart-definition-and-examples-businessq/ https://www.britannica.com/topic/Likert-Scale https://www.perceptualedge.com/blog/?p=2239 https://blog.datawrapper.de/divergingbars/ http://t-redactyl.io/blog/2016/01/creating-plots-in-r-using-ggplot2-part-4-stacked-bar-plots.html "],
["likert.html", "Chapter 21 Likert 21.1 Overview 21.2 tl;dr 21.3 Simple examples 21.4 Stacked bar chart using ggplot 21.5 Theory 21.6 When to use 21.7 External resources", " Chapter 21 Likert Shijie He and Chutian Chen 21.1 Overview This section covers how to make stacked bar chart on likert data. Likert data is the data with likert scale. Likert scale is a several point scale which is used to allow people to express how much they agree or disagree with a particular statement. And It’s commonly used in survey and research. 21.2 tl;dr Here’s a stacked bar chart of angry levels: And here’s the code: library(HH) library(dplyr) # create data data = data.frame(&quot;Not_at_all_angry&quot;=c(0.11,0.08,0.09,0.08,0.09,0.12,0.05,0.08),&quot;Not_very_angry&quot;=c(0.75,0.75,0.74,0.70,0.78,0.68,0.86,0.71),&quot;Fairly_angry&quot;=c(0.13,0.14,0.16,0.17,0.11,0.18,0.06,0.19),&quot;Very_angry&quot;=c(0.02,0.02,0.02,0.05,0.02,0.02,0.03,0.01),&quot;Region&quot;=c(&quot;North&quot;,&quot;Midlands&quot;,&quot;East&quot;,&quot;London&quot;,&quot;South&quot;,&quot;Wales&quot;,&quot;Scotland&quot;,&quot;Northern_Ireland&quot;),&quot;England&quot; = c(&quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;Not England&quot;, &quot;Not England&quot;, &quot;Not England&quot;)) # make stacked bar chart likert(Region ~.|England, layout=c(1,2), data, positive.order = TRUE, scales=list(y=list(relation=&quot;free&quot;)), strip.left=strip.custom(bg=&quot;gray97&quot;), strip=FALSE, as.percent = &quot;noRightAxis&quot;, ReferenceZero = 2.5, main = &#39;Angry levels in different regions&#39;, ylab = &quot;Region&quot;, xlab = &quot;Percentage&quot;, sub= list(&quot;Angry Level Rating&quot;,x=unit(.6, &quot;npc&quot;))) For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/v6iuiikyxq/YG-X-MarksTheSpot-BrandsAnger-280812.pdf. 21.3 Simple examples Too complicated! Let’s see some simpler examples first. For the below examples, we will use the survey of the satisfaction of Trump for male and female. data_2 &lt;- data.frame(&quot;Great&quot;=c(4,2),&quot;Good&quot;=c(14,6),&quot;Average&quot;=c(15,16),&quot;Poor&quot;=c(17,17),&quot;Terrible&quot;=c(44,48),&quot;Gender&quot;=c(&quot;Male&quot;,&quot;Female&quot;)) data_2 &lt;- data_2[,c(6,1,2,3,4,5)] data_2 ## Gender Great Good Average Poor Terrible ## 1 Male 4 14 15 17 44 ## 2 Female 2 6 16 17 48 For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/psse08hgpj/YouGov%20-%20Trump%20state%20visit%20190520.pdf. 21.3.1 Stacked bar chart To create a stacked bar chart, we will simply use likert function. likert(Gender ~ ., data_2, ReferenceZero = 0, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to compare the end values using stacked bar chart, but it is hard to compare the neutral percentage. 21.3.2 Diverging stacked bar chart To create a diverging stacked bar chart, we will modify ReferenceZero to the neutral category. likert(Gender ~ ., data_2, ReferenceZero = 3, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to visualize the overall shape of likes and dislikes, and the percentage of neutrals. However, it is hard to compare the value of like and dislike categories. 21.4 Stacked bar chart using ggplot In fact, we can make likert plot using ggplot. There is a easy way through dividing the data into two parts which represent agreement and disagreement respectively. Then we can generate the two plots on same place with one of the value of plot is negative. If there is neutral option in data, we have to divide it into two parts, which will be a little more complicated. library(ggplot2) library(reshape2) library(RColorBrewer) library(dplyr) library(ggthemes) library(stringr) library(forcats) d &lt;- data_2 d[,2:6] &lt;- d[,2:6]/rowSums(d[,2:6]) mytitle &lt;- &quot;Satisfaction of Trump&quot; mylevels &lt;- c(&quot;Great&quot;, &quot;Good&quot;, &quot;Average&quot;, &quot;Poor&quot;, &quot;Terrible&quot;) # Generate mid value of neutral category numlevels &lt;- length(d[1,])-1 numcenter &lt;- ceiling(numlevels/2) + 1 d$midvalues &lt;- d[,numcenter]/2 d_2&lt;-cbind(d[,1],d[,2:ceiling(numlevels/2)], d$midvalues, d$midvalues,d[,numcenter:numlevels+1]) colnames(d_2)&lt;-c(&quot;Sex&quot;,mylevels[1:floor(numlevels/2)],&quot;Midlow&quot;, &quot;Midhigh&quot;,mylevels[numcenter:numlevels]) # Split into six categories numlevels&lt;-length(mylevels)+1 point1&lt;-2 point2&lt;-((numlevels)/2)+1 point3&lt;-point2+1 point4&lt;-numlevels+1 # Assign color to each categories numlevels&lt;-length(d[1,])-1 temp.rows&lt;-length(d_2[,1]) pal&lt;-brewer.pal((numlevels-1),&quot;RdBu&quot;) pal[ceiling(numlevels/2)]&lt;-&quot;#DFDFDF&quot; legend.pal&lt;-pal pal&lt;-c(pal[1:(ceiling(numlevels/2)-1)], pal[ceiling(numlevels/2)], pal[ceiling(numlevels/2)], pal[(ceiling(numlevels/2)+1):(numlevels-1)]) # Generate new data frame including all information d_3&lt;-melt(d_2,id=&quot;Sex&quot;) d_3$col&lt;-rep(pal,each=temp.rows) d_3$value&lt;-d_3$value*100 d_3$Sex&lt;-str_wrap(d_3$Sex, width = 40) d_3$Sex&lt;-factor(d_3$Sex, levels = d_2$Sex[order(-(d_2[,5]+d_2[,6]+d_2[,7]))]) highs&lt;-na.omit(d_3[(length(d_3[,1])/2)+1:length(d_3[,1]),]) lows&lt;-na.omit(d_3[1:(length(d_3[,1])/2),]) # Plot ggplot() + geom_bar(data=highs, aes(x = Sex, y=value, fill=col), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_bar(data=lows, aes(x = Sex, y=-value, fill=fct_inorder(col)), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_hline(yintercept = 0, color =c(&quot;white&quot;)) + scale_fill_identity(&quot;Percent&quot;, labels = mylevels, breaks=legend.pal, guide=&quot;legend&quot;) + theme_fivethirtyeight() + coord_flip() + labs(title=mytitle, y=&quot;&quot;,x=&quot;&quot;) + theme(plot.title = element_text(size=14, hjust=0.5)) + theme(axis.text.y = element_text(hjust=0)) + theme(legend.position = &quot;bottom&quot;) 21.5 Theory Likert data is a type of rating scale commonly used in surveys. It’s a bipolar data, representing the attitude to a statement. We can add neutral options to help candidate make choices when they are uncertain. A typical Likert scale may look like Strongly disagree Disagree Agree Strongly agree or Strongly disagree Disagree Neither agree or disagree Agree Strongly agree The options of likert scale can avoid the distortion of the survey result which is likely to be too extreme. 21.6 When to use When the data is from survey and is likert scale. 21.7 External resources 4 ways to visualize Likert Scales: The advantages and disadvantages of different stacked bar chart. Likert Plots in R: Using ggplot to plot diverging stacked bar chart. "],
["likert-vs-bar-chart.html", "Chapter 22 Likert vs. Bar Chart", " Chapter 22 Likert vs. Bar Chart Prasham Sheth and Manas Dresswala This example goes through some work with the MathAnxiety and MathAnxietyGender dataset to get to a finished Likert that is ready to present. Following are the packages that we used for it: devtools ggplot2 likert dplyr tidyr For the likert package we install and load the amended package from github, for which following set of commands are used: library(devtools) install_github(“m-dev-/likert”) Let us understand our data first, before diving into the actual code. MathAnxiety is a data set of presummarized results of the Math Anxiety Scale Survey administered to 20 stu- dents in a statistics course. It has 14 rows and 6 columns. MathAnxietyGender is a data set of presummarized results of the Math Anxiety Scale Survey administered to 20 stu- dents in a statistics course grouped by gender. It has 28 rows and 7 columns. One extra column named ‘Group’ for the gender (Male/Female). Next, we define the following function in whcih linebreak is introduced in the given sentence after every nth character instead of space and if it is not a space next nearest space is found and then it is replaces with the linebreak. The function is used to make sure that labels that appear on the graph are readable and don’t spoil the visual perspective represented by the graph. linebreaks &lt;- function(st, len=10){ loops &lt;- nchar(st)/len j &lt;- 0 for (i in 0:loops-1) { j &lt;- j + len while(substring(st, j, j) != &quot; &quot;) { j &lt;- j + 1 if (j &gt;= nchar(st)) { j&lt;- j+1 break } } substring(st,j,j) &lt;- &quot;\\n&quot; j&lt;-j+1 if (j &gt;= nchar(st)) { break } } return (st) } sentence &lt;- &quot;i find maths interesting&quot; function_result &lt;- linebreaks(sentence, 10) print(function_result) ## [1] &quot;i find maths\\ninteresting&quot; We here plot the bar charts to get the insights from the data. For the pupose of plotting the data on the bar chart we need to tidy the data by using the gather function of dplyr library. Gather takes multiple columns and collapses into key-value pairs, duplicating all other columns as needed. We then call the function implemented above, to clean the labels by updating the item column of the dataset which would be used as the X axis. library(dplyr) library(tidyr) data(&quot;MathAnxiety&quot;) df_bar &lt;- gather(MathAnxiety, key = &quot;Repsonse&quot;, value = &quot;Count&quot;, -Item) df&lt;- data.frame(df_bar$Item) df2&lt;-t(lapply(as.character(df[[1]]),linebreaks)) df3&lt;-data.frame() for (i in df2){ df3&lt;-rbind(df3,data.frame(i)) } df_bar&lt;-cbind(df_bar,df3) In the following sections we plot 3 different bar graphs. In the first one the position of the bars are dodged and the relative size of the labels is adjusted to 0.75 to make sure they appear in a proper format. The fill of the bar graphs are set to the responses to the answers to know the difference between proportions of different responses. For the second graph, we clean the data to get the responses grouped by gender of participants and tidy it in a similar way using the gather function and the function we implemented. The second graph is created in a similar way as the first one with a difference that the graphs are facted according to gender of the participants. The third graph is similar to the second one with a difference in the position of the alignment of the bar. The bars are stacked instead of keeping it side by side. The graphs are faceted bu the items that represent the responses by the participants in the survey. ggplot(df_bar, aes(y = df_bar$Count, x = df_bar$i, fill = df_bar$Repsonse)) + geom_col(position = &quot;dodge&quot;) + coord_flip()+ theme(axis.text.y = element_text(size = rel(0.75)))+ xlab(&quot;Item&quot;)+ ylab(&quot;Count&quot;)+ guides(fill=guide_legend(title=&quot;Responses&quot;)) data(&quot;MathAnxietyGender&quot;) df_bar_gender &lt;- gather(MathAnxietyGender, key = &quot;Repsonse&quot;, value = &quot;Count&quot;, -Item, -Group) df&lt;- data.frame(df_bar_gender$Item) df2&lt;-t(lapply(as.character(df[[1]]),linebreaks)) df3&lt;-data.frame() for (i in df2){ df3&lt;-rbind(df3,data.frame(i)) } df_bar_gender&lt;-cbind(df_bar_gender,df3) ggplot(df_bar_gender, aes(y = df_bar_gender$Count, x = df_bar_gender$i, fill = df_bar_gender$Repsonse)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + facet_wrap(df_bar_gender$Group) + theme(axis.text.y = element_text(size = rel(0.70)))+ xlab(&quot;Item&quot;)+ ylab(&quot;Count&quot;)+ guides(fill=guide_legend(title=&quot;Responses&quot;)) ggplot(df_bar_gender, aes(y = df_bar_gender$Count, x = df_bar_gender$Group, fill = df_bar_gender$Repsonse)) + geom_col(position = &quot;fill&quot;) + coord_flip() + facet_grid(df_bar_gender$Item,scales=&#39;free_y&#39;, space=&#39;free_y&#39;)+ xlab(&quot;Gender&quot;)+ ylab(&quot;Proportion&quot;)+ guides(fill=guide_legend(title=&quot;Responses&quot;))+ theme(legend.position=&quot;bottom&quot;) The following section plots the likert charts for the gievn survey data. We use HH package to plot the likert charts. We have ordered the chart by positive responses to the answers in a descending order. The scale of the chart labels is set to 0.65 to make the chart labels readable. data(&quot;MathAnxiety&quot;) df_1 &lt;- MathAnxiety HH::likert(Item ~., df_1, positive.order = TRUE, main=&quot;Responses to know how students react to maths anxiety.&quot;, scales = list(y = list(cex = .65))) The following is the graph containing the responses by male participants only. The responses are not sorted to maintain the order of the questions asked in the survey. For this firstly we filter the data to collect the responses by male participants who took part in the survey. data(&quot;MathAnxietyGender&quot;) df_male &lt;- MathAnxietyGender[which(MathAnxietyGender$Group==&#39;Male&#39;), ] HH::likert(Item ~., df_male, positive.order = FALSE, main=&quot;Male Responses to know how students react to maths anxiety.&quot;,, scales = list(y = list(cex = .65))) The following is the graph containing the responses by female participants only. The responses are not sorted to maintain the order of the questions asked in the survey. For this firstly we filter the data to collect the responses by female participants who took part in the survey. df_female &lt;- MathAnxietyGender[which(MathAnxietyGender$Group==&#39;Female&#39;), ] HH::likert(Item ~., df_female, positive.order = FALSE, main=&quot;Female Responses to know how students react to maths anxiety.&quot;,, scales = list(y = list(cex = .65))) Follwing graph shows the plots for male and female side by side to do a comparitive analysis from the graphs. The faceting is done using the | operator while passing the data into the likert function of the HH package. HH::likert(Item ~. | Group, MathAnxietyGender, positive.order = TRUE, main=&quot;Responses to know how students react to maths anxiety genderwise&quot;,, scales = list(y = list(cex = .65))) After looking at all the above graphs, we would like to conclude that when we have ordinal data, plotting a Likert graph would be a better option over using bar charts. Lets understand why we are trying to say this - 1. If we go through the different bar charts that we drew in the beginning, the last one seems to be the most readable. Where we can easily make the comparisons between the male and female responses for all the questions seperately. But, the main issue here is that it is very difficult to manipulate the labels on each facet. Due to this we cannot read the question clearly, thus making it difficult to understand which question are we actually looking at. 2. Another reason, why we think Likert is a better option is because we can easily compare the results keeping the neutral option in the centre. This makes is very easy to read the graph and make faster inferences. Hope this tutorial was helpful to further strengthen your understanding on Likert and why it is better to use over the bar chart. "],
["radar-plots-to-show-multivariate-continuous-data.html", "Chapter 23 Radar plots to show multivariate continuous data", " Chapter 23 Radar plots to show multivariate continuous data Charley Levy Radar plots are another way to visualize multivariate quantitative data. Radar plots are otherwise known as spider charts, web charts, polar charts or star plots. We have previously used Parallel Coordinate Plots to visualize multivariate continulous data. I became intrigued with Radar plots when I first saw them in a column by Nate Silver on FiveThirtyEight.com.(https://fivethirtyeight.com/features/why-kamala-and-beto-have-more-upside-than-joe-and-bernie/). Nate Silver broke the Democratic party into 5 different coalitions. He then attempted to show how strong each of the Democratic Candidates were among each of the 5 different coalitions. The visualizations were done by Rachael Dottle. Each candidate had their own radar plot. I used the library “fmsb” and the function “radarchart” for our visualizations. I wanted to show how each of the 4 DSI core classes varied in their subject matter. I used 5 different variables (Math, Coding, Statistics, Reading, Other). Each of the 4 observations (ProbStat, EDAV, Algorithms, ML) received a score for each of the 5 variables. The first 4 variables were on a scale of 0 to 10. The fifth variable was on a score of 0 to 20. We created a data frame with this information. For the Radarchart package, the first observation needs to correspond to the max value for each variable and the second observation needs to correspond to the min value for each variable. This allows us to scale each variable appropriately. The 1st visualization has all 4 observations (ProbStat, EDAV, Algorithms, ML) in one radar plot. I adjusted the alpha (the 4th input to the color functions) to 0.5 so there was enough transparency to see all 4 observations on one plot. You can adjust the pcol (color of polygon border) and pfcol(color of polygon fill) amongst each observation. library(fmsb) library(dplyr) min&lt;-c(0,0,0,0,0) max&lt;-c(10,10,10,10,20) ProbStat&lt;-c(7,0,10,8,14) EDAV&lt;-c(3,7,5,9,18) Algorithm&lt;-c(8,8,6,4,4) ML&lt;-c(8,10,4,2,6) DSDF&lt;-data.frame(max,min,ProbStat,EDAV,Algorithm,ML) DSDFT&lt;-t(DSDF) DS&lt;-data.frame(DSDFT) #radar plot w/ variable names colnames(DS)&lt;-c(&quot;Math&quot;,&quot;Coding&quot;,&quot;Statistics&quot;,&quot;Reading&quot;,&quot;other&quot;) color=c(rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5)) radarchart(DS, pcol=c( rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5)), pfcol=c( rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5) ), title=&quot;varying skillsets by DSI course&quot;) Our 5 variables are nominal (as opposed to ordinal). They have no fixed order. For the next visualization, I changed the order of our nominal variables to see how our plot would change. The proper ordering of these 5 variables is arbitrary. #change the order or the norminal varibales to see how polygon shapes change (order is arbitrary) DS2&lt;-DS[c(3,5,2,4,1)] colnames(DS2)&lt;-c(&quot;Statistics&quot;,&quot;other&quot;,&quot;Coding&quot;,&quot;Reading&quot;,&quot;Math&quot;) color=c(rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5)) radarchart(DS2, pcol=c( rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5)), pfcol=c( rgb(0.8,0.2,0.2,0.5), rgb(0.2,0.8,0.2,0.5),rgb(0.2,0.2,0.8,0.5), rgb(0.5,0.5,0.5,0.5)), title=&quot;varying skillsets by DSI course&quot;) For the last visualization, I chose to have each observation have its own radar plot. This is how the data was visualized in the FiveThirtyEight column. #break down the 4 classes into 4 individual radar charts DS3=DS2[1:3,] radarchart(DS3, pcol=c( rgb(0.8,0.2,0.2,0.5)), pfcol=c( rgb(0.8,0.2,0.2,0.5), title=&quot;ProbStat&quot;)) DS4=data.frame(DS2%&gt;%slice(1:2,4)) radarchart(DS4, pcol=c( rgb(0.2,0.8,0.2,0.5)), pfcol=c( rgb(0.2,0.8,0.2,0.5), title=&quot;EDAV&quot;)) DS5=data.frame(DS2%&gt;%slice(1:2,5)) radarchart(DS5, pcol=c( rgb(0.2,0.2,0.8,0.5)), pfcol=c( rgb(0.2,0.2,0.8,0.5), title=&quot;Algorithm&quot;)) DS6=data.frame(DS2%&gt;%slice(1:2,6)) radarchart(DS6, pcol=c( rgb(0.5,0.5,0.5,0.5)), pfcol=c( rgb(0.5,0.5,0.5,0.5), title=&quot;ML&quot;)) Radar plots are good for noticing patterns of each variable among the different observations. For a specific variable, are the scores fairly similar amongst observations or are they very different. You can also see which variables generally score higher or lower. These are more easily noticed on a Parallel Coordinate Plot. The PCP has the advantage of having a single, straight axis. Having too many observations/polygons in one chart makes it difficult to read. The polygons overlap eachother, particularly if they are filled in. Having too many variables/axis can also make it appear very cluttered. With radar charts, the perception of the data is not always ideal. In reality, each of the variable’s values is plotted as a point on a line. When each of the points on each axis are connected to form a polygon, we get an area. For one, this conversion from lines to area is quadratic and not linear. If each of observation B’s values are twice that of observation A, observation B will have more than twice the area of observation A. This is misleading and tends to have us overestimate differences btw/ observations. Additionally, we know from Cleveland’s hierarchy of graphical perception that area is harder to perceive than lines/length in general. This is compounded when each observation is on it’s own plot, even if the axis are identical. Resources Used: https://www.data-to-viz.com/caveat/spider.html https://www.r-graph-gallery.com/142-basic-radar-chart.html "],
["r-vs-tableau-plots.html", "Chapter 24 R vs tableau plots 24.1 We shall now show our plots using R studio 24.2 We shall now see how to do the same data visualization tasks using Tableau.", " Chapter 24 R vs tableau plots Pritam Biswas and Amogh Mishra We are performing a comparison between R and tableau for different types of data visualization. We are showing a few plots of R Studio and Tableau We will show Choropleth map in R, one for USA and one for World map. We will show the same plots in Tableau software. During the demo we will observe how easy it is use Tableau due to its drag and drop interface and dashboard facility. 24.1 We shall now show our plots using R studio This is the Choropleth map for populations of state in USA library(choroplethr) data(df_pop_state) data(df_state_demographics) state_choropleth(df_pop_state, title=&quot;2012 Population by State&quot;, legend=&quot;Population&quot;) This is the Choropleth map for GDP of countries in the world Sys.setenv(&quot;plotly_username&quot;=&quot;biswaspritam1993&quot;) Sys.setenv(&quot;plotly_api_key&quot;=&quot;p3A0ZAbHFdOt9AM3XGb0&quot;) library(plotly) df &lt;- read.csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv&#39;) l &lt;- list(color = toRGB(&quot;grey&quot;), width = 0.5) g &lt;- list( showframe = FALSE, showcoastlines = FALSE, projection = list(type = &#39;Mercator&#39;) ) p &lt;- plot_geo(df) %&gt;% add_trace( z = ~GDP..BILLIONS., color = ~GDP..BILLIONS., colors = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;green&quot;), text = ~COUNTRY, locations = ~CODE, marker = list(line = l) ) %&gt;% colorbar(title = &#39;GDP Billions US$&#39;, tickprefix = &#39;$&#39;) %&gt;% layout( title = &#39;2014 Global GDP&lt;br&gt;Source:&lt;a href=&quot;https://www.cia.gov/library/publications/the-world-factbook/fields/2195.html&quot;&gt;CIA World Factbook&lt;/a&gt;&#39;, geo = g ) chart_link = api_create(p, filename=&quot;choropleth-world&quot;) The interactive choropleth map is at location https://plot.ly/~biswaspritam1993/3/ This is the NO2 and NOx concentrations in air line plot library(ucidata) MyData &lt;- read.csv(file=&quot;resources/r_vs_tableau/air_quality_r.csv&quot;, header=TRUE, sep=&quot;,&quot;) MyData$Date &lt;- as.Date(MyData$Date, format = &quot;%d/%m/%Y&quot;) ggplot(MyData, aes(Date)) + ggtitle(&#39;NO2 and NOx concentrations&#39;) + geom_line(aes(y=NO2, color = &quot;NO2&quot;)) + geom_line(aes(y=Nox, color = &quot;Nox&quot;)) + ylab(&#39;Concentrations&#39;) 24.2 We shall now see how to do the same data visualization tasks using Tableau. Our tableau link is at : https://public.tableau.com/profile/amogh.mishra#!/vizhome/CommunityContributionEDAV/Story1 In the UCI air quality dataset (time series) we can : easily drag data and plot time series against harmful gases. do easy customization increase the granularity provide individual colours mark labels visualize relevant trends using filters change calculation of NO2 concentration from sum to average and median In the Choropleth world GDP we can : do colour-coding changes get sum as a selection of the area get labelling min/max, all We also have the added advantage of using the tableau dashboard. "],
["geommlbstadiums.html", "Chapter 25 GeomMLBStadiums", " Chapter 25 GeomMLBStadiums Luke Beasley and Chris Russo We wrote a tutorial for the GeomMLBStadiums package which contains tools for graphing baseball data. The tutorial is on GitHub: Visualization_with_GeomMLBStadiums "],
["ggmosaic.html", "Chapter 26 ggmosaic 26.1 Overview 26.2 Introduction 26.3 Order of splits 26.4 Splitting on One Variable(binned data) 26.5 Splitting on One Variable(unbinned data) 26.6 Splitting on Two Variables 26.7 Splitting on Three Variables 26.8 Adjusting the Direction of Splits 26.9 Alternative approach: Conditional 26.10 Alternative approach: Facetting 26.11 Comparison with vcd::mosaic", " Chapter 26 ggmosaic Qiang Zhao Mike Yao-Yi Wang Chinese version 26.1 Overview This cheat sheet is inspired by the Chapter 15 Chart: Mosaic of the edav.info. Instead of using the mosaic function from the package vcd to plot the mosaic plot, this cheat sheet shows how to achieve the same output through using ggmosaic. 26.2 Introduction Mosaic plot is only for categorical data Variables to put in geom_mosaic: weight: Count/Freq column x: product(Y, X2, X1) fill: dependent variable Y conds: conditional variable 26.3 Order of splits The mosaic plot follows the hierarchical structure, thus the order of adding variables is very important. Below we will show a step by step splitting by adding one variable at the time. Before going through the example, one must install and call the package ggplot2 and ggmosaic. library(ggplot2) library(ggmosaic) df_bin=data.frame(Age=c(&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;), Favorite=c(rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2),rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2)), Music=c(rep(c(&#39;classical&#39;,&#39;rock&#39;),4)), Freq=c(1,1,3,1,2,5,1,0)) df_unbin = data.frame(Age =c(rep(&quot;old&quot;,6), rep(&quot;young&quot;, 8)), Favorite = c(rep(&quot;bubble gum&quot;, 2),rep(&quot;coffee&quot;, 4), rep(&quot;bubble gum&quot;, 7), &quot;coffee&quot;), Music = c(&quot;classical&quot;, &quot;rock&quot;, rep(&quot;classical&quot;, 3), &quot;rock&quot;, rep(&quot;classical&quot;, 2), rep(&quot;rock&quot;, 5), &quot;classical&quot;)) 26.4 Splitting on One Variable(binned data) df_bin ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 ## 7 young coffee classical 1 ## 8 young coffee rock 0 First, we will show the ggmosaic only split on Age: Important: The ggmosaic can take binned data by assigning the weight = Freq column of the dataset at its aesthetics, it is not like vcd::mosaic(), which can only take binned data with count column name as Freq. ggplot(data = df_bin)+ geom_mosaic(aes(x = product(Age), fill = Age, weight = Freq))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(binned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 26.5 Splitting on One Variable(unbinned data) However, for unbinned data, we could just ignore the weight and let it set to default. The unbinned data: df_unbin ## Age Favorite Music ## 1 old bubble gum classical ## 2 old bubble gum rock ## 3 old coffee classical ## 4 old coffee classical ## 5 old coffee classical ## 6 old coffee rock ## 7 young bubble gum classical ## 8 young bubble gum classical ## 9 young bubble gum rock ## 10 young bubble gum rock ## 11 young bubble gum rock ## 12 young bubble gum rock ## 13 young bubble gum rock ## 14 young coffee classical ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age), fill = Age))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(unbinned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note: We will use unbinned data for the rest of example 26.6 Splitting on Two Variables Split on Age, then Music: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Music, Age), fill = Music))+ labs(x = &quot;Age&quot;, y = &quot;Music&quot;, title = &quot;Spliting on Age, then Music&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Split on Music, then Age: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age, Music), fill = Age))+ labs(x= &quot;Music&quot;, y = &quot;Age&quot;, title = &quot;Spliting on Music, then Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) For plotting mosaic plot on Y ~ X, we want to set x = product(Y, X) in aes as we always want to split the dependent variable last. We also need to set fill = Y as we want to color base on dependent variable. 26.7 Splitting on Three Variables Split on Age, then Music, then Favorite: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite))+ labs(x = &quot;Favorite:Age&quot;, y = &quot;Music&quot;, title = &quot;Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note that in the above example, by default the order of split and their directions as follows: Age – vertical split Music – horizontal split Favorite – vertical split 26.8 Adjusting the Direction of Splits The directions can be adjusted as we want. For example, we want to create a doubledecker plot for the above example following below criteria: Splitting order: Age – vertical split (“hspine”) Music – vertical split (“hspine”) Favorite (dependent variable)– horizontal split (“vspine”) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note that the divider vector is in order of which the variables appear in the product(Favorite, Music, Age), however the order of splits is Age, Music, then Favorite. Also note that in the divider vector, “vspine” = horizontal split and “hspine” = vertical split. 26.9 Alternative approach: Conditional We can also use conditional property to achieve the same result as the above. In this case, geom_mosaic(aes(x = product(last_split), fill = last_split, conds = product(second_split, first_split)). ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite), fill = Favorite, conds = product(Music, Age)), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - (Favorite | Music, Age)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 26.10 Alternative approach: Facetting We can also achieve similar result through facetting. ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music), fill = Favorite))+ facet_grid(. ~Age)+ labs(x=&quot;Music&quot;, y = &quot;favorite&quot;, title = &quot;Favorite ~ Music and facet on Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 26.11 Comparison with vcd::mosaic There are often confusions between ggmosaic:geom_mosaic and vcd:mosaic as the syntax for splitting order and splitting direction are quite different for the two. The vcd:mosaic follows the order of mosaic(last_split ~ first_split + second_split) and the direction vector in the order of splits is (first_split, second_split, third_split) with “v” being the vertical split and “h” being the horizontal split. However, ggmosaic:geom_mosaic follow the different pattern, the order of split is product(last_split, second_split, first_split) and the divider (similar to direction in vcd:mosaic) in the order of split is divider = c(last_split, second_split, first_split) with “vspine” being the horizontal split and “hspine” being the vertical split. "],
["comparative-study-of-vcdmosaic-and-geom-mosaic.html", "Chapter 27 Comparative Study of vcd::mosaic and geom_mosaic 27.1 1. vcd::mosaic: 27.2 2. geom_mosaic: 27.3 3. vcd::mosaic vs geom_mosaic – which one is better?", " Chapter 27 Comparative Study of vcd::mosaic and geom_mosaic Mamunur Rashid and Jiaying Chen vcd::mosaic and geom_mosaic are two mostly used functions to create mosaic plot - a way of visualizing multivariate categorical data which is an area-proportional visualization of frequencies, composed of tiles (or block) created by recursive vertical and horizontal splits of a rectangle. There are several functions available to produce mosaic plot including mosaicplot - a generic function from base R function, mosaic from vcd (Visualizing Categorical Data) package, geom_mosaic from ggmosaic package etc. Currently, there is inadequate online help or guidance to use those functions. Moreover, the existing documentation of these functions is not user-friendly for beginners. It has a limited number of examples and test cases. In the following sections, we are going to discuss a comparative study of vcd::mosaic and geom_mosaic. Our goal is to describe different aspects of these functions with some user-friendly examples and detail explanation of some of the parameters, and finally to compare ease-of-use &amp; effectiveness of each of the functions. In this study, we tried to explain how to use these two functions to produce different mosaic plots. We have elaborated some of the necessary parameters/ arguments in detail that are used in the functions. As a matter of fact, it will be easy to learn for a new user. Finally, we showed which function is better in which scenario such as vcd::mosaic is better in some cases whereas geom_mosaic is better in other cases based on the available data and the objective. To demonstrate different aspects of these functions, we will use the Titanic dataset to plot different mosaic plots. str(Titanic) ## &#39;table&#39; num [1:4, 1:2, 1:2, 1:2] 0 0 35 0 0 0 17 0 118 154 ... ## - attr(*, &quot;dimnames&quot;)=List of 4 ## ..$ Class : chr [1:4] &quot;1st&quot; &quot;2nd&quot; &quot;3rd&quot; &quot;Crew&quot; ## ..$ Sex : chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..$ Age : chr [1:2] &quot;Child&quot; &quot;Adult&quot; ## ..$ Survived: chr [1:2] &quot;No&quot; &quot;Yes&quot; tail(as.data.frame(Titanic)) ## Class Sex Age Survived Freq ## 27 3rd Male Adult Yes 75 ## 28 Crew Male Adult Yes 192 ## 29 1st Female Adult Yes 140 ## 30 2nd Female Adult Yes 80 ## 31 3rd Female Adult Yes 76 ## 32 Crew Female Adult Yes 20 27.1 1. vcd::mosaic: library(vcd) mosaic is one of the functions of strucplot framework which we can get from vcd R package. The main idea of strucplot is to visualize the tables’ cells arranged in rectangular form structable objects. The “flattened” contingency table can be obtained using the structable() function. The basic Usage of mosaic functions is: vcd::mosaic(x, condvars = NULL, split_vertical = NULL, direction = NULL, spacing = NULL, spacing_args = list(), gp = NULL, expected = NULL, shade = NULL, highlighting = NULL, highlighting_fill = grey.colors, highlighting_direction = NULL, zero_size = 0.5, zero_split = FALSE, zero_shade = NULL, zero_gp = gpar(col = 0), panel = NULL, main = NULL, sub = NULL, ...) OR vcd::mosaic(formula, data, highlighting = NULL, ..., main = NULL, sub = NULL, subset = NULL, na.action = NULL) Some of the basic parameters/arguments are described in the following section: x: A contingency table in array form, with optional category labels specified in the dimnames(x) attribute, or an object of class “structable”. x is a table or formula. A structable can also be deduced from data frame or contengency table using formula. For example, HEC &lt;- structable(Survived ~ Sex + Age, data = Titanic) HEC ## Survived No Yes ## Sex Age ## Male Child 35 29 ## Adult 1329 338 ## Female Child 17 28 ## Adult 109 316 formula: A formula specifying the variables used to create a contingency table from data. For convenience, conditioning formulas can be specified; the conditioning variables will then be used first for splitting. If any, a specified response variable will be highlighted in the cells. For example, If we want to split the plot by 1 varialbe, we can specify as ~V1. If V2 is dependent on V1 and V1 needs to be highlighted, we can do V2 ~ V1. In the following example, a plot is crated from Titanic data: vcd::mosaic(Survived ~ Sex + Age, data = Titanic, main = &quot;Survival on the Titanic&quot;) In the above plot, the data is partinioned by sex first and then age; after that survival is used to show the dependency. vcd::mosaic(Survived ~ ., data = Titanic) This type of formuala is used when one variable is needed to be highlighted based on rest of the varialbes. Survived is highlihgted based on all otther dependend variables. data: Either a data frame, or an object of class “table” or “ftable”. If we provide, structable contingency table, we do not need to provide data (df or table). If data is in a data frame, the count column must be called Freq. direction: character vector of length k, where k is the number of margins of x (values are recycled as needed). For each component, a value of “h” indicates that the tile(s) of the corresponding dimension should be split horizontally, whereas “v” indicates vertical split(s). By default, vcd::mosaic() splits horizontally from left for the first variable (e.g. ~ V1, ~ V1 + V2, ~ V2 | V1 or V2 ~ V1 – here V1 is considered as first variable) and vertically from top for the second variable and so on. If we want to alter this direction, we can provide this “direction” parameter. For example, vcd::mosaic(Survived ~ Sex + Age, data = Titanic, main = &quot;Survival on the Titanic&quot;,direction=c(&quot;v&quot;,&quot;v&quot;,&quot;h&quot;)) # Sex = Vertical, Age = Vertical, Survived = Horizonal In the above plot, the sex variable is used to split the plot vertically, then age variable is used to sub-devide verticaly and then survived sub-division is highlighted horizontally from the other side. gp: Object of class “gpar”, shading function or a corresponding generating function (see details and shadings). Components of “gpar” objects are recycled as needed along the last splitting dimension. Ignored if shade = FALSE. if gp is not provided, the default gray color is applied with shade = TRUE vcd::mosaic(Survived~ Sex + Age, data = Titanic, main = &quot;Survival on the Titanic&quot;, shade = TRUE, direction=c(&quot;v&quot;,&quot;v&quot;,&quot;h&quot;)) If gp is provided with shade = TRUE (or no shade parameter but not shade = FALSE), the provided color along with other graphical paramters (if provided any) will apply. vcd::mosaic(Survived~ Sex + Age, data = Titanic, main = &quot;Survival on the Titanic&quot;, shade = TRUE, direction=c(&quot;v&quot;,&quot;v&quot;,&quot;h&quot;), gp = gpar(fill=c(&#39;lightblue&#39;, &#39;pink&#39;)) ) shade: Logical specifying whether gp should be used or not (see gp). If TRUE and expected is unspecified, a default model is fitted: if condvars (see strucplot) is specified, a corresponding conditional independence model, and else the total independence model. If we provide shade = FALSE, the gp will not apply at all. vcd::mosaic(Survived~ Sex + Age, data = Titanic, main = &quot;Survival on the Titanic&quot;, shade = FALSE, direction=c(&quot;v&quot;,&quot;v&quot;,&quot;h&quot;), gp = gpar(fill=c(&#39;lightblue&#39;, &#39;pink&#39;)) ) 27.2 2. geom_mosaic: geom_mosaic is similar to vcd::mosaic, which is also designed to display the relationship between categorical variables. geom_mosaic can be imported from ggmosaic package which is integrated in ggplot2 as a geom and allows for facetting and layering. library(ggmosaic) library(gridExtra) We are also using the same Titanic dataset. But, here will convert the dataset from contengency table to data frame to easy to use. Titanic_df&lt;-as.data.frame(Titanic) The basic usage or parameters of geom_mosaic is: geom_mosaic(mapping = NULL, data = NULL, stat = &quot;mosaic&quot;, position = &quot;identity&quot;, conds=NULL, na.rm = FALSE, divider = mosaic(), offset = 0.01, show.legend = NA, inherit.aes = FALSE, ...) The following is the description of parameters: mapping: In geom_mosaic, this is used to specify categorical varaibles and numeric variable to be displayed. The basic parameters of mapping is aes(weight=NULL, x=NULL, fill=NULL, conds=NULL) Aesthetics contains the arugments of weight, x, fill and conds. weight: select a weighting variable, it can be frequency, relative frequency or other similar weighting variables, but it must be numeric. In mosaic graph, this parameter is used to show the area of each splitting rectangle. x: select variables to add to formula, which is used to assign categorical varaibles. The basic parameters of x is: x=product(col_1, col_2, col_3,...) For example, ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex))) The data is partinioned by Sex first, then Age, and finally Survived. fill: Select a variable to be filled, it is used to color each rectangle in different colors, which is same to splitting each rectangle into two smaller rectangle and then coloring each smaller rectangle in the same parent rectangle in different colors. For example, ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product( Age, Sex), fill=Survived)) conds: Select variables to condition on, which will be used first for splitting. The basic parameters of conds is conds=product(col_1, col_2, ...) For example, ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Age, Sex), conds=product(Class, Survived),fill=Survived)) In this example, the data is partitioned by variables in conds first, (Survived, then Class), and then by variables in x. When variables in “product”, then the order is always from the right most varible to the left most variable, not only in x but also in conds. And geom_mosaic is a little different to other geom_* functions, we don’t need to and also can’t specific y. In geom_mosaic, there is no y value. order of split ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex), fill=Class)) In this example, the order of partition is Sex, Age, Survived, and then each rectangle splitted by Sex, Age and Survived will also be partitioned by Class into 4 smaller rectanglers and then color these smaller ones into four different colors. The most important thing for this parameter is that the direction of splitting depends on the number of patameters in x. In the above example, the number of patameters in x is 3, which is odd, then the data is partitioned by Class horizontally. While when the number of patameters in x is even, for example, if it is 2, we can see the direction is now vertical. ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Age, Sex), fill=Class)) data: Only data frame can be used, no “table” or “ftable”. If we want to use an object of “table”, we must transfer it into dataframe. #Titanic &lt;- as.data.frame(Titanic) And we can also put data in the function of ggplot. na.rm: The default is “FALSE”, which means it will not remove “NA”; while when it equals to “TURE”, it will remove missing values. divider: We have two methods to set divider. The first one is mosaic().The default of directions is first vertical and then horizontal, repeatedly. And we can also use divider to reset the directions of splitting. For example, ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex), fill=Survived), divider = mosaic(&quot;v&quot;)) The default parameter is “horizontal”(or “h”), the splitting directions is v-h-v-h-…; while when using “vertical”(or “v”), the directions will become h-v-h-v-…. In other words, it is transversed, varaibles in y-axis will be in x-axis, and varaibles in x-axis will be in y-axis. The second one is setting divider directly, hspine, vspine, vbar or hbar. But in this method, the data can only be splitted by one categorical variable. Using hspine or vspine, it is same to the original mosaic graph, which using vbar or hbar, it will be like bar chart. For example, p1&lt;-ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Sex)), divider=&quot;hspine&quot;)+ggtitle(&quot;Hspine&quot;) p2&lt;-ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Sex)), divider=&quot;hbar&quot;)+ggtitle(&quot;Hbar&quot;) grid.arrange(p1, p2, nrow = 1) offset: Set the space between the first spine, but at the same time, the space between other spines also will change in equal proportion. For example, g1&lt;-ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex),fill=Survived),offset = 0.01)+ggtitle(&quot;Offset=0.01&quot;) g2&lt;-ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex),fill=Survived),offset = 0.05)+ggtitle(&quot;Offset=0.1&quot;) grid.arrange(g1, g2, nrow = 1) Besides, geom_mosaic also has other similar features of ggplot, like it can use facet_* to facet. For example, ggplot(data = Titanic_df) + geom_mosaic(aes(weight = Freq, x = product(Survived, Age, Sex), fill=Survived))+facet_grid(Class~.) 27.3 3. vcd::mosaic vs geom_mosaic – which one is better? Type of data geom_mosaic can only use data frame as its data argument, however the type of data in vcd::mosaic can be data frame or table. Handling variables used in partition geom_mosaic, in fact ggplot2, is not capable of handling a different number of variables in aesthetics. product, a wrapper function for a list, is used as a workaround for this. However, this workaround leads to issues with the labeling; those can be fixed manually though. On the other hand, the formualla of vcd::mosaic is capable of handling a number of splitting variables and labeling is also very easy in this case. Use of different layer including faceting Since geom_mosaic is a faction of ggplot2, it inherits the layered mechanism. So, we can plot mosaic with other plots. We can also use other grouping techniques like faceting. Order of splits vcd::mosaic is much easier to control the order of split than geom_mosaic. vcd::mosaic uses formula (e.g. V3 ~ V1 + V2) to set the split order which has a clear explanation of the position of the variables. On the other hand, geom_mosaic uses odd or even number of variables to decide the split. If the number of parameters is odd, the data is partitioned by the last dependent variable horizontally. Direction of splits vcd::mosaic has a direction argument that directs split directions whether the variable should be split horizontally or vertically. geom_mosaic has default direction to split the product variables. The default of directions is first vertical and then horizontal, repeatedly. divider is used to reset the directions of splitting, but a different combination of splitting direction with the variable is very challenging Color In the use of color in different places (e.g. fill, border), both functions have a straightforward structure. geom_mosaic also gives an auto legend facility in the use of color. offset Setting offset and spacing, geom_mosaic is very easy to work with. In conclusion, it’s easier to use vcd::mosaic, especially when setting the arguments of categorical variables and their splitting directions. What’s more, vcd::mosaic can also display the relationship between categorical variables more clearly. But geom_mosaic is integrated in ggplot2 as a geom which allows for faceting, layering, setting offset and so on. Reference: https://pdfs.semanticscholar.org/2416/2c1a46669f94356854176ece8548bf7fb989.pdf https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Mosaic_Plots.pdf https://en.wikipedia.org/wiki/Mosaic_plot http://www.pmean.com/definitions/mosaic.htm https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html "],
["latex-visualization.html", "Chapter 28 Latex Visualization", " Chapter 28 Latex Visualization Yuki Nishimura and Jay Zern Ng 28.0.1 Summary LaTeX Visualization is essential for writers that are hoping to publish academic papers with visualizations of concepts and findings. For first time academic writers, it may be difficult to figure out the types of visualizations that could be done using LaTeX. Therefore, we have written a tutorial for LaTeX Visualization to accommodate the needs of inexperienced academic writers in the following link: Click Here Hope you enjoy the tutorial :D "],
["cheat-sheet-of-wordcloud2-package.html", "Chapter 29 Cheat sheet of wordcloud2 package", " Chapter 29 Cheat sheet of wordcloud2 package Contributor: Yujing Song, Wen Fan Group number: 53 The community contribution for our team is working on creating a cheat sheet of wordcloud2 package in r. Our work is divided into two parts, one is for the word could package general introduction and text adjustment, the other part is for the word cloud shape adjustment. Song works for the first part, which showes in the first two columns on the cheat sheet, and Wen works for the second part that displaies on the last two columns of the cheat sheet. The final cheat sheet is a pdf file and our work coding is uploaded as a rmd file. Our Cheet Sheet and coding can be accessed at the following link: https://github.com/YujingSong/Community-Contribution.git "],
["wordcloud.html", "Chapter 30 Wordcloud 30.1 1. Introduction 30.2 2. Demo of wordcloud2 Package", " Chapter 30 Wordcloud Chengyou Ju and Yujie Wang This Rmd file is created by Chengyou Ju (UNI: cj2624) and Yujie Wang (UNI: yw3442) for STAT GR5702 Community Contribution Group 15. In this file, we will provide a tutorial on how to draw Wordcloud graphs using the wordcloud2 package in R. The dataset in this project are from the demoFreq package. 30.1 1. Introduction A Wordcloud is a visual representation of text data. Wordclouds are useful for quickly perceiving the most prominent terms, which makes them widely used in media and well understood by the public. A Wordcloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the greater frequency within a given text and the more important it is. There are two packages in R that can help us draw a wordcloud. wordcloud is the basic package to build the graph, while wordcloud2 package allows more customization. In our demo, we will focus on the wordcloud2 package, which is more widely used. 30.2 2. Demo of wordcloud2 Package For our demo, we will use a built-in dataset demoFreq, which has 1011 observations of 2 variables, words and frequancy. library(devtools) devtools::install_github(&quot;lchiffon/wordcloud2&quot;) ## ## checking for file ‘/tmp/RtmpgWatC5/remotes6fac46b40627/Lchiffon-wordcloud2-8a12a3b/DESCRIPTION’ ... ✔ checking for file ‘/tmp/RtmpgWatC5/remotes6fac46b40627/Lchiffon-wordcloud2-8a12a3b/DESCRIPTION’ ## ─ preparing ‘wordcloud2’: ## checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ## ─ checking for LF line-endings in source and make files and shell scripts ## ─ checking for empty or unneeded directories ## Removed empty directory ‘wordcloud2/examples/img’ ## Removed empty directory ‘wordcloud2/examples’ ## ─ looking to see if a ‘data/datalist’ file should be added ## ─ building ‘wordcloud2_0.2.2.tar.gz’ ## ## library(wordcloud2) head(demoFreq) ## word freq ## oil oil 85 ## said said 73 ## prices prices 48 ## opec opec 42 ## mln mln 31 ## the the 26 Parameters for wordcloud2 from Rdocumentation data - data frame with word and freqency of the word size - Font size, default is 1. The larger size means the bigger word fontFamily - font used in the word cloud fontWeight - Font weight to use, e.g. normal, bold or 600 color - color of the text, keyword ’random-dark’ and ’random-light’ can be used. backgroundColor - Color of the background minRotation - If the word should rotate, the minimum rotation (in rad) the text should rotate. maxRotation - If the word should rotate, the maximum rotation (in rad) the text should rotate. shuffle - Shuffle the points to draw so the result will be different each time for the same list and settings. rotateRatio - Probability for the word to rotate. Set the number to 1 to always rotate. shape - The shape of the “cloud” to draw. Can be a keyword present. widgetsize - size of the widgets figPath - The path to a figure used as a mask. hoverFunction - Callback to call when the cursor enters or leaves a region occupied by a word. 30.2.1 2.0 Basic Wordcloud Graph Building a wordcloud graph is simple. We can use the wordcloud2 package directly after successfully installing it. wordcloud2(data = demoFreq) As we can see, the word cloud is easy to build and to read. Words with large frequency like ‘said’ and ‘oil’ are displayed in big font size. It is actually an interactive plot. If we hover on a certain word, it will pop up the word with its frequency. 30.2.2 2.1 Font Size We can also modify the font size of the graph. wordcloud2(data = demoFreq, size = 0.5) wordcloud2(data = demoFreq, size = 1.5) 30.2.3 2.2 Color and Background Color The word color can be changed using the “color” argument, while the background color can be changed with “backgroundColor”. wordcloud2(demoFreq, color = &#39;random-dark&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;, backgroundColor = &quot;black&quot;) wordcloud2(demoFreq, color = rep_len(c(&quot;skyblue&quot;, &quot;blue&quot;), nrow(demoFreq))) 30.2.4 2.3 Shape We can also customize the shape of a wordcloud using the “shape” argument. Here are some examples. wordcloud2(demoFreq, size = 0.5, shape = &#39;star&#39;) wordcloud2(demoFreq, size = 0.5, shape = &#39;pentagon&#39;) 30.2.5 2.4 Rotation We can also do rotation on the wordcloud graph. wordcloud2(demoFreq, minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1) 30.2.6 2.5 Language We can draw a wordcloud graph of words in Chinese. wordcloud2(demoFreqC, fontFamily = &quot;????????????&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) 30.2.7 2.6 Customized shape We can build wordcloud with the shape of a word using function letterCloud. letterCloud(demoFreq, word = &quot;R&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) Also, we can create user-defined shape for the wordcloud by simply adding the image we choose to figPath. wordcloud2(demoFreq, figPath =&quot;~/Desktop/batman.png&quot;, size = 1, color = &quot;random-light&quot;,backgroundColor = &quot;black&quot;) batman "],
["visualizing-movie-reviews-in-word-cloud.html", "Chapter 31 Visualizing Movie Reviews in Word Cloud 31.1 IMDB Reviews 31.2 Cleaning the data! 31.3 Word Cloud", " Chapter 31 Visualizing Movie Reviews in Word Cloud Samrat Halder and Hariz Johnson 31.1 IMDB Reviews We first scrape all the movie reviews from IMDB library(rvest) library(xml2) library(plyr) result &lt;- c() ID &lt;- 4154796 movie_IMDb &lt;- read_html(paste0(&quot;https://www.imdb.com/title/tt&quot;,ID,&quot;/reviews?ref_=tt_urv&quot;)) reviews &lt;- movie_IMDb %&gt;% html_nodes(&quot;#pagecontent&quot;) %&gt;% html_nodes(&quot;div.content&quot;) %&gt;% html_text() #perfrom data cleaning on user reviews reviews &lt;- gsub(&quot;\\r?\\n|\\r&quot;, &quot; &quot;, reviews) reviews &lt;- tolower(gsub(&quot;[^[:alnum:] ]&quot;, &quot; &quot;, reviews)) write.csv(reviews, paste0(getwd(),&quot;/resources/wordcloud_sentiment/reviews.csv&quot;)) head(reviews) ## [1] &quot; there is no way that i could describe my emotions for this movie i m totally speechless i haven t laughed even cried this much in marvel movie or even in any movie i m fully on my emotion there are so many tears of joy and loss amazing story the acting is outstanding epic action great cgi the best storytelling ever told in a superhero movie amazing performance i love it more than 3000 happiness sadness pure joy excitement i m gonna miss this moment in my whole life because let s face it it s been awhile movies can bring such a big enthusiasm like this it is such an experience you ll gonna remember it forever people are clapping laughing crying full of a state emotion it s 3 hours long but it went by like a finger snapping by thanos and now i m thinking i m actually in quantum realm because it felt like 5 seconds even though you know where the story is gonna bring you because it s still a superhero movie but it left me speechless it s not just a superhero movie it s more than that spoiler alert even some characters that you didn t like before you will love them in this movie like captain marvel i didn t like her before in her own movie but in endgame they showed how powerful how strong and how capable she really is and now i kinda love her but marvel should really be careful of her line though i didn t like some of her line like i m the strongest and you didn t win because before you didn t have me duh also not gonna forget hawkeye to be honest in avengers i really didn t like hawkeye because he s just a guy with an arrow and just randomly showed up in any scene but actually he s character is so badass everything that we need what we want is here also i don t get it why some people can not accept that thor is fat or treat him like a joke i mean this movie might be trying to give us a message about no matter what shape are you you can be a hero and save the world to be honest i like what they did to thor that man is depressed and taking the most responsible after he missed the shot to kill thanos in infinity war even though he already did killed him but it s not gonna bring back his people of asgard also he s god of thunder for god sake he could slap his stomach with thunder and got his abs back i m so happy too for captain america finally he could do his dance with peggy and i didn t expected that we could see the fight between captain america with his own self also when tony met his father really warm my heart and we can see the real life jarvis i didn t expected that too when the whole superhero arrived it is so epic the whole person in theater are screaming and shocked gives me a goosebumps and still left me speechless this movie is absolute perfection the ending of this movie is we and the characters deserved it gives a perfect balance for all these past 10 years epic and perfect ending i was not disappointed at all this movie was completely emotional and visually stunning now i get it why the russo brothers told us to do a marvel movie marathon before watching endgame because this movie had everything to accomplish what left behind before all of this ready or not whatever it takes go watch it for yourself 4 207 out of 6 378 found this helpful was this review helpful sign in to vote permalink &quot; ## [2] &quot; can t ignore the truth that this is a super hero film while except the last several minute other plots can attract any attention can t even compare with avengers infinity war narrative thread and music disordered throughout 3 hour movie 15 out of 19 found this helpful was this review helpful sign in to vote permalink &quot; ## [3] &quot; after avengers infinity war we waited for the avengers endgame we wondered how the story would go on how our heroes would turn back what would be the end of thanos many theories related to this have been put forward avengers endgame was undoubtedly the most anticipated film of recent years normally the higher the expectation the higher the probability of disappointment but this is not the case for endgame whatever you re expecting you find much more in the film this means that the biggest concern about the film has disappeared on the other hand another comparison comes up is endgame more successful than infinity war we can comfortably say it avengers infinity war is just the beginning of the story endgame was the finale of the story so we shouldn t think of these two films as two separate stories there is only one story divided into two parts avengers endgame is above all a great homage to the ten year history of the marvel cinematic universe the story highlights the original avengers team iron man captain america thor hulk black widow and hawkeye are at the center of events no character comes in front of them of course there are many characters that play an important role in the story outside the original avengers team everyone s concern was that captain marvel who was included in the marvel world overshadowed other heroes we can say that this certainly did not happen what is important in this struggle is not how strong you are but how good you are this comes to the fore in all areas it gives good message about being a hero and a family of course avengers endgame has some critical aspects for example is the three hour period necessary in terms of the story it can be discussed the head of the story moves much slower than the rest it also drags the heroes into an emotional predicament then the tempo is rising and the heavy scenes we are watching are getting more meaningful the last 45 minutes of the movie is fully action packed but the last 45 minutes goes so fast that you don t even realize it action and battle scenes are really successful there is not even a slight distress about visual effects there are also slight logic errors in the film but in general the story is so successful that these details become meaningless and insignificant after a certain point lastly avengers endgame doesn t have a movie end scene because after the film s final there is no need for another scene the marvel legend stan lee appears with a small stage but this is the last surprise scene in the marvel cinematic universe moreover there is no clue about marvel s future this makes us wonder more about spider man far from home 10 10 1 927 out of 3 341 found this helpful was this review helpful sign in to vote permalink &quot; ## [4] &quot; i have to say my first reaction walking out of the cinema was that it was great probably an 8 10 you know there s so much fan service in this movie and i particularly loved the i can do this all day ca line from one ca to another it was almost toy story 2 enlightened buzz to naive buzz banter i loved clever hulk found thor hilarious though a bit annoying at times and loved the references to past movies cap swinging mjollnir around was beautiful the deaths in this movie were also pretty surprising but i agree with some of the more balanced reviews in here that it was bizarre that so much time was spent on hawkeye does anyone really care about him not really killing off black widow was a surprising touch but i like many others probably felt more of an emotional reaction to banner s relationship with her and not hawkeye because no one actually cares about him renner as an actor just doesn t cut it much unfortunately the real problem with end game is the time travel i don t know why any movie franchise would ever want to deal with time travel knowing how much grief it creates sure they try to explain everything with their hilarious conversation about movies but in the end there are just too many questions left at the end of the day particularly when they already try to explain it through the ancient one the most glaring one of these is if they change the past they re changing another timeline so how selfish is that if tony stark snaps his fingers and kills thanos what happens to that original timeline if he sent thanos and everyone back how does he magically know not to send gamora back who is now stuck in 2023 and so does that mean 2014 quill never meets gamora what the hell essentially erasing the entire 2014 gotg franchise in that universe if captain america goes back in time he will inevitably change history in some way or another so that is another timeline right so while everyone else s timeline is meant to stay the same in the present as said by stark how does cap end up sitting by the same lake as everyone else there are also way too many questions about new timelines created but perhaps that is intentional such as the loki tv show but this is why i hate it when they bring time travel into any movie and just add stuff to it to make it wrap up nicely in their own universe also don t get me started on how impossibly strong thanos suddenly is without an infinity gauntlet even with the ig he couldn t stop thor s stormbringer but now without the ig can easily defeat thor with stormbringer and mjollnir iron man and cap how convenient i also think marvel made a serious mistake creating captain marvel how is she brought in as a convenient deus ex machina and yet doesn t really stick around to show off all her abilities and she can t even defeat thanos single handedly pfft that was rich she can barely prevent thanos from snapping his fingers yet in infinity war cap was able to actually hold off his hand too bizarre parallels so okay sure it was enjoyable to watch but i just feel that there are too many plot inconsistencies creeping into end game there should have been better ways to get the infinity stones back and i think someone stupid just thought time travel was the best way to do it 2 676 out of 4 679 found this helpful was this review helpful sign in to vote permalink &quot; ## [5] &quot; i want to start out saying i love all the marvel movies leading up to endgame i loved infinity war because it seemed to be setting up a terrific conclusion to the original 6 avengers story arc s i was very wrong let s start with tony s arc in ironman 1 tony is told you are a man who has everything and nothing in ironman 2 we find out his dad was distant and cold to him and in ironman 3 he is getting panic attacks because he thinks it s up to him to save the world from all threats so in endgame he finally gets everything that s important he becomes a dad and husband and was proven right by having to die saving the universe himself why because we are supposed to believe tony can t retire of course he can didn t he say to steve in aou isn t that why we fight so we can end the fight so we can go home tony wanted to retire tony s arc should have been about being a good dad like his father never was and finally becoming the man who has everything important he should have learned that he doesn t have to do it all on his own team work should have won the final battle what s the point of finally getting all the avengers together if it s left to one man to sacrifice himself to save everyone tony s ending was not satisfying and it was not the end they have been building towards from the beginning now let s talk about the real tragedy in this movie steve rogers character we know he is a good man who always does what he believes is right he calls home the avengers facility and says he can t ignore a situation that s headed south he loved peggy but found out she had gotten married and had a happy life and he tried moving on with her niece this storyline was ignored which was ok because most of steves storyline in the mcu has been about saving bucky you remember him right steve didn t seem to for the entire movie instead of thinking about sam and bucky and all the people they are going to save if their plan works when they find thanos in the beginning of endgame steve s just looking at the peggy compass that he hasn t looked at in at least 3 movies why so we can all forget about his actual character development and think it s a great ending when he goes back in time to be with peggy i m just going to ignore all the plot holes associated with steve going back in time and just focus on the character assasination this commits this means the same guy who says he can t ignore a situation headed south who never ran away from a fight who said i m with you till the end of the line to bucky is just going to live out his life in the past never saving his friend from being a brain washed assassin never saving his other friends parents from being killed by his best friend never stopping any of the thousands of bad things he would know are going to happen that s not steve the writers never let steve get a personal life he never found someone after peggy and he never had anything except the avengers he was the one that was set up to die the final battle starts with what is probably the most epic scene in comic book movie history but fizzles our soon after doctor stranges plan makes no sense how was this the only way they could win why couldn t captain marvel snap why couldn t captain marvel just fly the gauntlet to another galaxy why couldn t doctor strange open a portal and drop the gauntlet on some random planet in infinity war why didn t doctor strange use his portal to go to wakanda and tell thor to aim for the head the characters powers were very inconsistent for the first time in the mcu why was thanos with no infinity stones able to fight mjolnir and storm breaker but thanos with all 6 stones would have lost to storm breaker if thor would have just aimed for the head why can captain marvel fly through a huge spaceship 10 times but she gets knocked out after one punch from thanos why does no one attack thanos at the same time as someone else where s the big team up you would think getting everyone together for the first time would have been the only way to defeat thanos but you d be wrong because it s all about tony stark even steve picking up mjolnir means nothing really because it has no affect on the end of the battle thor and steve could be taken out of this movie and nothing would change they do nothing of importance this movie seems so disconnected from infinity war that it s boggles my mind how this movie has the same writers and directors as infinity war and it was filmed at the same time as infinity war how can you get one so wrong and the other so right it seems clear to me that the writers and directors decided that tony was dying and steve was going back to the past no matter how much their story suffered from doing it it s like they had a check list of things to put in the movie without actually making a good story that fit in with the rest of their movies this is by far the most dissapointing ending to anything i ve ever looked forward to 326 out of 550 found this helpful was this review helpful sign in to vote permalink &quot; ## [6] &quot; 3 hours out of my life i will never get back not worth the hype you think the soap opera is done by the ultimate battle think again let us milk it and release another spider man geez a lou 197 out of 326 found this helpful was this review helpful sign in to vote permalink &quot; 31.2 Cleaning the data! In this part first we clean the data using standard text mining techniques eg. removing redundant characters, stop words, stemming etc library(tm) library(plyr) library(stringr) library(wordcloud) library(RColorBrewer) removeURL &lt;- function(x) { gsub(&quot;http[[:alnum:]]*&quot;, &quot;&quot;, x) } removelongWORDS &lt;- function(x) { gsub(&quot;\\\\b[[:alpha:]]{15,}\\\\b&quot;, &quot;&quot;, x, perl=FALSE) } removeCharacters &lt;-function (x, characters) { gsub(sprintf(&quot;(*UCP)(%s)&quot;, paste(characters, collapse = &quot;|&quot;)), &quot;&quot;, x, perl = FALSE) } reviews &lt;- read.csv(paste0(getwd(),&quot;/resources/wordcloud_sentiment/reviews.csv&quot;), row.names = NULL) dataset &lt;- reviews$x dataset &lt;- str_replace_all(dataset, &quot;[^[:alnum:]]&quot;, &quot; &quot;) CorpusObj&lt;- VectorSource(dataset) CorpusObj&lt;- Corpus(CorpusObj) CorpusObj &lt;- tm_map(CorpusObj, removelongWORDS) CorpusObj &lt;- tm_map(CorpusObj, removeURL) CorpusObj &lt;- tm_map(CorpusObj, removePunctuation) CorpusObj &lt;- tm_map(CorpusObj, removeNumbers) CorpusObj &lt;- tm_map(CorpusObj, removeCharacters, c(&quot;\\uf0b7&quot;,&quot;\\uf0a0&quot;)) CorpusObj &lt;- tm_map(CorpusObj, tolower) CorpusObj &lt;- tm_map(CorpusObj, stemDocument, language = &quot;english&quot;) CorpusObj &lt;- tm_map(CorpusObj, removeWords, c(stopwords(&quot;english&quot;), &quot;text show-more__control&quot;, &quot;movi&quot;, &quot;like&quot;,&quot;vote&quot;, &quot;also&quot;,&quot;review&quot;, &quot;permalink&quot;, &quot;help&quot;, &quot;stori&quot;,&quot;charact&quot;)) CorpusObj&lt;-tm_map(CorpusObj,stripWhitespace) CorpusObj.tdm &lt;- TermDocumentMatrix(CorpusObj, control = list(minWordLength = 3)) freqr &lt;- rowSums(as.matrix(CorpusObj.tdm)) CorpusObj.tdm.sp &lt;- removeSparseTerms(CorpusObj.tdm, sparse=0.90) We make a term document matrix mTDM &lt;- as.matrix(CorpusObj.tdm) v &lt;- sort(rowSums(mTDM),decreasing=TRUE) d &lt;- data.frame(word = names(v),freq=v) head(d, 10) ## word freq ## can can 41 ## thano thano 38 ## just just 37 ## film film 30 ## time time 30 ## aveng aveng 29 ## endgam endgam 29 ## found found 29 ## even even 28 ## infin infin 25 31.3 Word Cloud Finally we create the word cloud from the corpus object that we created in the last part. pal &lt;- brewer.pal(9, &quot;BuGn&quot;) pal &lt;- pal[-(1:2)] png(paste0(getwd(),&quot;/resources/wordcloud_sentiment/wordcloud.png&quot;), width=1280,height=900) wordcloud(d$word,d$freq, min.freq=300, scale=c(7,0.5), colors=brewer.pal(8, &quot;Dark2&quot;), random.color= TRUE, random.order = FALSE) dev.off() ## png ## 2 Please note to see the word cloud check the png file. It is not the best practice to plot word cloud from a huge corpus on R console because of the limited resolution "],
["data-art-talk.html", "Chapter 32 Data art (talk)", " Chapter 32 Data art (talk) Yuheng Shi &amp; Banruo Xie We will introduce two very interesting data visualization graphic. The first goal of this workshop is to expand our foundamential data visualization knowledge and try to build more sophisticated graphics that contain more important data information. The second goal would be how we could use different factors to build multiple dimensions in one graphics and help readers interpret data better and draw more meaningful conclusions. If the attendees want to know more about bookdown, they can always go back to our slides for reference. We have uplodaed our complete slide to the repo (url: https://github.com/HansShi/EDAV_Community_Contribution; file name: data visualization workshop.pdf). "],
["shiny.html", "Chapter 33 Shiny 33.1 Part 1 How to Build a Shiny App 33.2 1. Install the shiny package 33.3 2. Template for creating a shiny app 33.4 3. Add elements to user interface using fluidPage() 33.5 4. Build output in server instructions 33.6 5. Share your app 33.7 Part 2 How to Customize Reactions 33.8 1. Reactivity 33.9 3. Summary", " Chapter 33 Shiny Duanyue Yun, Boyu Liu In this tutorial, we will use the cars dataset as an example to wall through the process of building a shiny app. The cars dataset contains various information about a particular car. cars_info &lt;- read.csv(&quot;cars.csv&quot;) 33.1 Part 1 How to Build a Shiny App 33.2 1. Install the shiny package First of all, we can install the shiny package by running the code below. install.packages(&quot;shiny&quot;) 33.3 2. Template for creating a shiny app A shiny app consists of two main components: user interface (ui) and server instructions (server). The user interface will contain the elements that a user sees on your shiny app, which can be input (possible user interactions) and output display. The server instructions will define how the app should react to a user’s action. Therefore, a basic template for creating a shiny app consists of 3 parts as shown below: library(shiny) #1 define user interface ui &lt;- fluidPage() #2 define server instructions server &lt;- function(input, output) {} #3 putting everything together shinyApp(ui = ui, server = server) 33.4 3. Add elements to user interface using fluidPage() The arguments of the fluidPage() function could be Input() functions or Output() functions. 33.4.1 Input functions Inputs define the possible ways a user can interact with our shiny App. For a numerical variable, the input could be a slider that a user can move along to select a certain value. For a categorical variable, the input could be a box where the user can select a particular category from a drop down list. All Input() functions contain 2 required arguments: inputId = and label =. inputId is for us to identify a particular input. Later we can use the same input ID in server instructions to decide the corresponding output. Therefore, to avoid errors, it is better to give a unique name to each input. label is what the user sees on the shiny App, so it should be informative. The common Input() functions supported are: actionButton(), submitButton(), checkboxInput(), checkboxGroupInput(), dateInput(), dateRangeInput(), fileInput(), numericInput(), passwordInput(), radioButtons(), selectInput(), sliderInput(), textInput(). Each Input() function has some specific arguments. For example, the sliderInput() function requires min, max arguments to set the range of the slider and also a value argument which is the default value the user sees when the shiny app is launched. You can find more about the function using ?sliderInput(). For example, we can add a select box by running the code below. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)])) 33.4.2 Output functions We can display an output, for example a plot, by adding Output() functions to fluidPage(). Each Output() function requires one argument, which is outputId =. We will talk about how to build output in server instructions. ui &lt;- fluidPage(plotOutput(&quot;histogram&quot;)) The common Output() functions supported are: dataTableOutput(), htmlOutput(), imageOutput(), plotOutput(), tableOutput(), textOutput(), uiOutput(), verbatimTextOutput(). 33.5 4. Build output in server instructions 33.5.1 (1): Save objects you want to display to output$ server &lt;- function(input, output) { output$histogram &lt;- # code } We can use the same name in the form of a string in fluidPage() to display the output. 33.5.2 (2): Build objects with render() The render() functions that are supported include renderDataTable(), renderImage(), renderPlot(), renderPrint(), renderTable(), renderText(), renderUI(). Within render() functions, we could use {} to wrap the code so that we can write multiple lines of code to create more sophisticated output. As an example, the following code builds a histrogram of the variable mpg to our shiny app. Remember to add it to ui() to display it in the shiny app. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info$mpg, main = &quot;&quot;, xlab = &quot;mpg&quot;) }) } 33.5.3 (3): Use input values with input$ When we use input$, the app will be interactive. For the following example, when the user selects a different variable, the histogram will change accordingly. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } 33.6 5. Share your app 33.6.1 Save your app You should save your app to one directory with every file the app needs: app.R (must be the exact file name) datasets, images, css, helper scripts, etc. 33.6.2 Publish your app on Shinyapps.io Go to https://www.shinyapps.io to sign up for an account. When you signed up for a new account on https://www.shinyapps.io, there will be instructions on how to associate your account with your RStudio IDE and how to deploy your app. 33.7 Part 2 How to Customize Reactions 33.8 1. Reactivity 33.8.1 What is reactivity? Let’s think about Microsoft Excel. In Excel, we can type some value into a cell x and type a formula that uses x into a new cell y. Then whenever we change the value in x, y’s value will change correspondingly. This is reactivity, which is also what Shiny does. Shiny has an input object input$x and an output object output$y. Any changes in input$x will cause changes in output$y. So now let’s start with reactive values, which is where reactivity starts in Shiny. 33.8.2 Reactive values Reactive values are what the user selects and depend on Input() functions. In the previous example where we create a select box, the reactive values are the variable that the user selects. Note that reactive values don’t work on their own. They actually work together with reactive functions. 33.8.3 Reactive functions (reactive toolkit) They are a kind of functions that are expected to take reactive values and know what to do with them. They are notified that they need to re-execute whenever the reactive value changes. They are included in the server instructions section to build (and rebuild) an object. We can think of reactivity in R as two-step process. Consider the following example. We use input function selectInput() to get user’s choice. input$varname is the reactive value. When we choose different variables, firstly reactive values will notify the functions which use them that they become outdated. After that its job is over and it’s time for reactive functions to do their jobs, which is rebuild the corresponding object using new values. The process is automatic in shiny. Suppose we want to output the corresponding histogram whenever the user chooses a variable. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), # Add corresponding output plotOutput(outputId = &quot;histogram&quot;) ) server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } shinyApp(ui = ui, server = server) 33.8.4 Modularize code with reactive() See the example below. When the user selects a number, the shiny app plots a histogram for that number of N(0,1) variables and also computes the summary statistics. This app has only one reactive value (number the user chooses) but has two objects, a histogram and a block of text which includes the statistics of the data. When the reactive value changes, it will notify these two objects and they will rerun the code to update themself. But the probelm is that because they rerun their code successively, so rnorm(input$num) is called twice. Since rnorm is random, each object generates a different set of values, which means the histogram describes a dataset and the summary of the statistics describes another dataset. # Before ui &lt;- fluidPage( # Add a slider sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), # Display the histogram plotOutput(outputId = &quot;hist&quot;), # Display the summary statistics verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { # Build the histogram output$hist &lt;- renderPlot({hist(rnorm(input$num), main = &quot;&quot;, xlab = &quot;num&quot;)}) # Build the object that contains the summary statistics output$stats &lt;- renderPrint({summary(rnorm(input$num))}) } shinyApp(ui = ui, server = server) For example, when we only select one normal variable. It is clear that the histogram and the summary statistics do not correpond to the same data. Can the two objects describe the same data? The answer is yes! The strategy is calling rnorm(input$num) only once and saving the dataset it creates. Then use this dataset downstream when we need it. Shiny provides a function called reactive(), which can wrap a normal expression to create a reactive expression and realize what we hope to achieve. reactive(rnorm(input$num)) In this specific example, we add a code data &lt;- reactive(rnorm(input$num)) to the server and replace rnorm(input$num) in the reactive functions with data(). Note that you should call a reactive expression like a function. So here we use data() instead of data. # After ui &lt;- fluidPage( sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), plotOutput(outputId = &quot;hist&quot;), verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { data &lt;- reactive(rnorm(input$num)) output$hist &lt;- renderPlot({hist(data(), main = &quot;&quot;, xlab = &quot;num&quot;)}) output$stats &lt;- renderPrint({summary(data())}) } shinyApp(ui = ui, server = server) Now when will select 1 variable, the two objects will describe the same data. 33.8.5 Prevent reactions with isolate() Sometimes we might want to delay a reactive function. For example, the following shiny app plots a scatterplot between 2 variables of the user’s choose and also allows the user to give the plot a customized title. So there are 3 inputs: the title of the scatterplot, an x variable and a y variable. With our regular code, the title will change instantaneously as the user types. # Before ui &lt;- fluidPage( textInput(inputId = &quot;title&quot;, label = &quot;Enter a title&quot;, value = &quot;displacement vs mpg&quot;), selectInput(&#39;xcol&#39;, &#39;X Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)]), selectInput(&#39;ycol&#39;, &#39;Y Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)], selected=colnames(cars_info)[[4]]), plotOutput(&#39;scatterplot&#39;) ) server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], main = input$title) }) } shinyApp(ui = ui, server = server) Say we do not want the title to change until the user has chosen two variables. In this case, we can use isolate() to isolate the input title. It returns the result as a non-reactive value. That means the observed object will only react to its changes when other inputs also change. server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], # This line isolates the input title main = isolate({input$title})) }) } 33.8.6 Trigger code with observeEvent() we can create an action button or link whose value is initially zero, and increments by one each time it is pressed. When we have an input like action button, we can trigger a response when the user clicks on the button by using observeEvent() function. Examples of an action button include download which allows the user to download a file. actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) The observeEvent() function takes two arguments: the first argument is the reactive value(s) it responds to. In our example, it will be the action button. The second armgument is a code block which runs behind the scene whenever the input changes. observeEvent(input$download, {print(input$download)}) Here is how we can use it in our app. Every time we click the Go! button, the observer will update, which is running the block of code print(as.numeric(input$goButton). The result won’t appear in the user panel, but to appear back of our app. ui &lt;- fluidPage( actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) ) server &lt;- function(input, output) { observeEvent(input$downloadn, { print(as.numeric(input$download)) }) } shinyApp(ui = ui, server = server) Along with observeEvent() which triggers code, there’s another function called observe(), which does the same thing and it’s a parallel of observeEvent(). But its syntax is more like render*() functions. We just give a block of code to it and it will respond to every reactive value in the code. observe({print(input$download)}) 33.8.7 Delay reactions with eventReactive() Sometimes we don’t want the outputs to change as soon as the user changes some input in the user interface. Instead, we would like to change them when the user clicks an ‘update’ button. In others words, we hope to prevent the output from updating until the user hits the button. The way to do this in Shiny is with the function eventReactive(). It creates a reactive expression that only responds to specific values, similar to reactive() but having different syntax. First we give a reactive value to it. The second argument is the code the function uses to build or rebuild the object when it’s clicked. In addition, similar to observeEvent(), the expression treats this block of code as if it has been isolated with isolate(). data &lt;- eventReactive(input$update, {rnorm(input$num)}) Let’s look at the entire code. ui &lt;- fluidPage( selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), actionButton(inputId = &quot;update&quot;, label = &quot;Update&quot;), plotOutput(outputId = &quot;hist&quot;) ) server &lt;- function(input, output) { data &lt;- eventReactive(input$update, {input$varname}) output$hist &lt;- renderPlot({ hist(cars_info[[data()]], main = &quot;&quot;, xlab = data()) }) } shinyApp(ui = ui, server = server) If we choose different variable without clicking Update button, the histogram would not be updated. 33.8.8 Manage state with reactiveValues() We know that the reactive value changes whenever a user changes the input in the user panel. But we cannot set these values in our code. Fortunately, although Shiny doesn’t give us the power to overwrite the input values in our app, it gives us the power to create our own list of reactive values, which you can overwrite. reactiveValues() is a function that creates a list of reactive values to manipulate programmatically. Note that it has nothing to do with input reactive values. rv &lt;- reactiveValues(data = rnorm(100)) Let’s look at an example. If we click mpg vs displacement, the Shiny app would select column mpg and displacement from the cars_info dataset and plot a scatter plot for them. If we click mpg vs weight, it would select column mpg and weight from the cars_info dataset and plot a scatter plot for them. ui &lt;- fluidPage( actionButton(inputId = &quot;scatter1&quot;, label = &quot;mpg vs displacement&quot;), actionButton(inputId = &quot;scatter2&quot;, label = &quot;mpg vs weight&quot;), plotOutput(&quot;scatter&quot;) ) server &lt;- function(input, output) { rv1 &lt;- reactiveValues(data = cars_info[,2], label = &quot;mpg&quot;) rv2 &lt;- reactiveValues(data = cars_info[,4], label = &quot;displacement&quot;) observeEvent(input$scatter1, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,4] rv2$label &lt;- &quot;displacement&quot; }) observeEvent(input$scatter2, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,6] rv2$label &lt;- &quot;weight&quot; }) output$scatter &lt;- renderPlot({ plot(rv1$data, rv2$data, xlab = rv1$label, ylab = rv2$label)}) } shinyApp(ui = ui, server = server) 33.9 3. Summary Till now, We have learnt both syntax and usage of the basic reactive functions in Shiny. Now there are still some important tips we need to provide. We should reduce repetition when we create shiny apps. That is to place code where it will be re-run as little as necessary. Keep in mind that, Code outside the server function will be run once per R session (worker). So you only need it to run once when setting up the R session, outside the server function. For example, codes that load the help file or some library should be placed outside the server function. Code inside the server function will be run once per end user session (connection). Code inside the reactive function will be run once per reaction, which means many times. If you are interested in Shiny and would like to learn more about it, you can go to the official website or download the documentation of Shiny. The relevant resources are listed below. Official website: https://shiny.rstudio.com/ Documantation of pacakge “Shiny”: https://cran.r-project.org/web/packages/shiny/shiny.pdf Share your Shiny apps: https://www.shinyapps.io/ Shiny cheat sheet: https://shiny.rstudio.com/images/shiny-cheatsheet.pdf The main source of this tutorial is the video on the Shiny official website. We adapted it with some new examples based on the cars dataset. Hope this can help you and any suggestion is welcome. "],
["html-javascript-and-d3.html", "Chapter 34 HTML, JavaScript, and D3", " Chapter 34 HTML, JavaScript, and D3 Yitao Liu (yl4343) and Yiyang Sun (ys3284) In addition to the visualization tools we have learned using R, we would like to introduce another powerful visualization tool, D3.js. We created a GitHub Page as well as the GitHub Repository to introduce basic knowledge of building a D3 visualization. We have made cheat sheets for HTML, JavaScript – two cornerstones of coding with D3 library, and a cheat sheet for D3.js. To better help fellow students to kick-off with D3 visualization, we also provided code examples of HTML and D3 in our GitHub Repository. Link to our GitHub Page: https://tonyytliu.github.io/Stat5702_CC60/ Link to our GitHub Repository: https://github.com/tonyytliu/Stat5702_CC60 "],
["technical-analysis-for-stocks-using-plotly.html", "Chapter 35 Technical Analysis for Stocks using Plotly 35.1 Import all libraries 35.2 Download data from Alpha Vantage 35.3 Simple plot: 2 traces in same axis 35.4 Many traces in independent axis but in same plot 35.5 Aesthetics: background and margins 35.6 More aesthetics: hide legends and hide X-axis slider 35.7 Shortcuts to slice data by pre-fixed date ranges", " Chapter 35 Technical Analysis for Stocks using Plotly Ivan Ugalde (du2160); Bernardo Lopez (bl2786) The goal of this R markdown file is to show how to use Apha Vantage API to download stocks data, organize it and make interactive plots using Plotly that will help to find trends in the market. 35.1 Import all libraries We will be ussing: plotly: For interactive plots. alphavantager: R package from Alpha Vantage for downloading stocks data. tidyverse library(&#39;plotly&#39;) library(&#39;tidyverse&#39;) library(&#39;alphavantager&#39;) 35.2 Download data from Alpha Vantage Alpha Vantage provides free historical and real time information on stocks via an API. 35.2.1 Usefull links for more information: Alpha Vantage API: alphavantager R package documentation. RSI: Relative strength index. MACD: Moving Average Convergence Divergence. PSAR: Parabolic SAR (Stop and Reverse) indicator. We will store in symbol the code for the stock we want to analyse. In this example we wil use Apple (AAPL), but you can change it to whatever you want. Manchester United (MANU), Starbucks (SBUX), Netflix (NFLX) and New York Times (NYT) are some examples. In start_date we store the date from which we want to start analyzing the stock. With the API we will download data for the selected stock from 2018-01-01 till today. We will download the daily value of the stock and the indicators SAR, MACD and RSI. In some cases the API returns the date also with the hour, so we will use the function mutate and as.Date() to have the same date format in all the API calls. The function av_get from the package alphavantager is used to get financial data from the Alpha Vantage API. It requires two main parameters: symbol for the code of the stock and av_fun, the Alpha Vantage function that describes the type of data you want to get. For this example we will use “TIME_SERIES_DAILY” for the daily value of the stock, and “SAR”, “MACD” and “RSI” to get those financial indicators. Tu use Alpha Vantage API you have to create your own key, it is free. To create the html file we used our key, but you will have to replace your new key in the chunk bellow. av_api_key(api_key) stock_symbol &lt;- &#39;AAPL&#39; start_date &lt;- as.Date(&#39;2018-01-01&#39;) ohlc &lt;- av_get(symbol = stock_symbol, av_fun = &#39;TIME_SERIES_DAILY&#39;, outputsize = &#39;full&#39;) %&gt;% filter(timestamp &gt;= start_date) psar &lt;- av_get(symbol = stock_symbol, av_fun = &#39;SAR&#39;, interval=&#39;daily&#39;) %&gt;% filter(time &gt;= start_date) %&gt;% mutate(time=as.Date(time,format=&quot;%Y-%m-%d&quot;)) macd &lt;- av_get(symbol = stock_symbol, av_fun = &#39;MACD&#39;, interval=&#39;daily&#39;, series_type = &#39;close&#39;) %&gt;% filter(time &gt;= start_date) %&gt;% mutate(time=as.Date(time,format=&quot;%Y-%m-%d&quot;)) rsi &lt;- av_get(symbol = stock_symbol, av_fun = &#39;RSI&#39;, interval=&#39;daily&#39;, time_period=14, series_type=&#39;close&#39;) %&gt;% filter(time &gt;= start_date) %&gt;% mutate(time=as.Date(time,format=&quot;%Y-%m-%d&quot;)) The following code merges all of the data using the date as binder. data &lt;- ohlc %&gt;% merge(y = psar, by.x = &#39;timestamp&#39;, by.y = &#39;time&#39;) %&gt;% merge(y = macd, by.x = &#39;timestamp&#39;, by.y = &#39;time&#39;) %&gt;% merge(y = rsi, by.x = &#39;timestamp&#39;, by.y = &#39;time&#39;) data %&gt;% head() ## timestamp open high low close volume sar macd macd_hist ## 1 2018-01-02 170.16 172.30 169.26 172.26 25555900 167.3888 0.3870 -0.4921 ## 2 2018-01-03 172.53 174.55 171.96 172.23 29517900 167.4869 0.3751 -0.4032 ## 3 2018-01-04 172.54 173.47 172.08 173.03 22434600 167.5841 0.4254 -0.2823 ## 4 2018-01-05 173.44 175.37 173.05 175.00 23660000 167.6802 0.6170 -0.0725 ## 5 2018-01-08 174.35 175.61 173.93 174.35 20567800 167.7754 0.7083 0.0150 ## 6 2018-01-09 174.55 175.06 173.41 174.33 21584000 167.8697 0.7702 0.0615 ## macd_signal rsi ## 1 0.8791 51.9084 ## 2 0.7783 51.8233 ## 3 0.7077 53.9903 ## 4 0.6896 58.8936 ## 5 0.6933 56.7448 ## 6 0.7087 56.6763 35.3 Simple plot: 2 traces in same axis To add traces to the plot we use add_trace. In this example we will use as type “candlestick” and “scatter”. plot1 &lt;- plot_ly(data) %&gt;% add_trace(type = &#39;candlestick&#39;, name = &#39;OHLC&#39;, x = ~timestamp, open = ~open, high = ~high, low = ~low, close = ~close, increasing = list(line = list(color=&#39;rgba(52,169,102,1)&#39;, width=1), fillcolor = &#39;rgba(0,0,0,0)&#39;), # Transparent decreasing = list(line = list(color=&#39;rgba(220,68,59,1)&#39;, width=1), fillcolor = &#39;rgba(0,0,0,0)&#39;), # Transparent legendgroup = &#39;one&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;markers&#39;, x = ~timestamp, y = ~sar, name = &#39;PSAR&#39;, marker = list(color = &#39;orange&#39;, size = 4), legendgroup = &#39;one&#39;) plot1 35.4 Many traces in independent axis but in same plot Plotly give us interactivity, that allows to zoom the data for the same dates for all graphs simultanously. By using the parameter yaxis we can plot traces in different Y axis but same X axis. plot2 &lt;- plot1 %&gt;% add_trace(type = &#39;bar&#39;, x = ~timestamp, y = ~macd_hist, name = &#39;MACD Histogram&#39;, marker = list(color = &#39;gray&#39;), yaxis = &#39;y2&#39;, legendgroup = &#39;two&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;lines&#39;, marker = NULL, x = ~timestamp, y = ~macd, name = &#39;MACD&#39;, line = list(color = &#39;red&#39;), yaxis = &#39;y2&#39;, legendgroup = &#39;two&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;lines&#39;, marker = NULL, x = ~timestamp, y = ~macd_signal, name = &#39;Signal&#39;, line = list(color = &#39;plum&#39;), yaxis = &#39;y2&#39;, legendgroup = &#39;two&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;lines&#39;, marker = NULL, x = ~timestamp, y = ~rsi, name = &#39;RSI&#39;, line = list(color = &#39;plum&#39;), yaxis = &#39;y3&#39;, legendgroup = &#39;three&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;lines&#39;, marker = NULL, x = c(~min(timestamp), ~max(timestamp)), y = c(70,70), name = &#39;RSI&#39;, line = list(color = &#39;red&#39;, width = 0.5, dash = &#39;dot&#39;), yaxis = &#39;y3&#39;, legendgroup = &#39;three&#39;) %&gt;% add_trace(type = &#39;scatter&#39;, mode = &#39;lines&#39;, marker = NULL, x = c(~min(timestamp), ~max(timestamp)), y = c(30,30), name = &#39;RSI&#39;, line = list(color = &#39;red&#39;, width = 0.5, dash = &#39;dot&#39;), yaxis = &#39;y3&#39;, legendgroup = &#39;three&#39;) %&gt;% layout(yaxis = list(domain = c(0.62, 1), fixedrange = FALSE), yaxis2 = list(domain = c(0.32, 0.58), fixedrange = FALSE), yaxis3 = list(domain = c(0., 0.28), fixedrange = FALSE), height = 500) plot2 35.5 Aesthetics: background and margins We use parameter paper_bgcolor and plot_bgcolor of layout to change the color of the background of the paper and the plot, we decided to use black. Parameter margin is used to change the size of the plot margins. plot3 &lt;- plot2 %&gt;% layout(paper_bgcolor=&#39;rgba(37,37,37,1)&#39;, plot_bgcolor=&#39;rgba(37,37,37,1)&#39;, margin = list(l=60, r=20, t=30, b=5)) plot3 35.6 More aesthetics: hide legends and hide X-axis slider To hide the legend we must set the parameter showlegend as false. For every Y-axis we used parameters titlte to set the title of the axis and titlefont and tickfont to change the colors of the title and the ticks. plot4 &lt;- plot3 %&gt;% layout(xaxis = list(titlefont = list(color=&#39;rgb(200,115,115)&#39;), tickfont = list(color=&#39;rgb(200,200,200)&#39;), linewidth=1, linecolor = &#39;white&#39;), yaxis = list(domain = c(0.62, 1), title = &#39;PSAR &amp; OHLC&#39;, titlefont = list(color=&#39;rgb(200,115,115)&#39;), tickfont = list(color=&#39;rgb(200,200,200)&#39;), linewidth=1, linecolor = &#39;white&#39;, mirror = &quot;all&quot;), yaxis2 = list(domain = c(0.32, 0.58), title = &#39;MACD&#39;, titlefont = list(color=&#39;rgb(200,115,115)&#39;), tickfont = list(color=&#39;rgb(200,200,200)&#39;), linewidth=1, linecolor = &#39;white&#39;, mirror = &quot;all&quot;), yaxis3 = list(domain = c(0., 0.28), title = &#39;RSI&#39;, titlefont = list(color=&#39;rgb(200,115,115)&#39;), tickfont = list(color=&#39;rgb(200,200,200)&#39;), linewidth=1, linecolor = &#39;white&#39;, mirror = &quot;all&quot;), showlegend = FALSE, height = 500) plot4 35.7 Shortcuts to slice data by pre-fixed date ranges This last chunk of code will add a rangeselector to the x axis. This buttons will allow us to focus only in the last 3, 6 or 12 months. We can also see all the data or see it year to date (from the begining of the year to the current day). For each button in the rangeselector we have to specify the count, label, step and stepmode. plot5 &lt;- plot4 %&gt;% layout(xaxis = list( rangeselector = list( buttons = list( list(count = 3, label = &quot;3 mo&quot;, step = &quot;month&quot;, stepmode = &quot;backward&quot;), list( count = 6, label = &quot;6 mo&quot;, step = &quot;month&quot;, stepmode = &quot;backward&quot;), list( count = 1, label = &quot;1 yr&quot;, step = &quot;year&quot;, stepmode = &quot;backward&quot;), list( count = 1, label = &quot;YTD&quot;, step = &quot;year&quot;, stepmode = &quot;todate&quot;), list(step = &quot;all&quot;))), rangeslider=list(visible=FALSE))) plot5 "],
["googlevis.html", "Chapter 36 GoogleVis 36.1 Overview 36.2 Example: Line chart 36.3 Example: Geo Chart 36.4 Example: Sankey chart 36.5 googleVis in RStudio 36.6 Reference and Resource", " Chapter 36 GoogleVis Junyang Jiang and Alex Wan 36.1 Overview This section covers how to make charts with googleVis. Google Charts Tools is a powerful online tool that enables users to create attractive graphical charts and embed them in Web pages with JavaScript. And the googleVis package can help R users take fully advantage of Google charts tools, which provides an interface between R and the Google charts tools and allows users to create interactive charts based on R data frames. This package (Version 0.6.4) provides interfaces to Motion Charts, Annotated Time, Lines, Maps, Geo Maps, Geo Charts, Intensity Maps, Tables, Gauges, Tree Maps, further Line, Bar, Bubble, Column, Area, Stepped Area, Combo, Scatter, Candlestick, Pie, Sankey, Annotation, Histogram, Timeline, Calendar and Org Charts. 36.2 Example: Line chart Let’s use GDP dataset from wbstats to have a look at how googleVis draws line chart. Here’s the code for preparing dataset: library(wbstats) suppressMessages(library(googleVis)) op &lt;- options(gvis.plot.tag=&#39;chart&#39;) # set googleVis plot option to display chart in RMarkdown dat &lt;- wb(indicator=&#39;NY.GDP.PCAP.KD&#39;, country=c(&#39;MX&#39;,&#39;CA&#39;,&#39;US&#39;,&#39;CH&#39;,&#39;IN&#39;,&#39;JP&#39;), start=1980, end=2018)[c(&quot;country&quot;,&quot;value&quot;,&quot;date&quot;)] gdp &lt;- as.data.frame.matrix(xtabs(value ~ date + country, data=dat)) gdp &lt;- cbind(rownames(gdp),gdp) colnames(gdp)[1] &lt;- &quot;date&quot; head(gdp) ## date Canada India Japan Mexico Switzerland United States ## 1980 1980 31839.23 422.9038 25854.58 8016.885 54891.43 28589.67 ## 1981 1981 32542.60 438.0069 26744.56 8494.012 55466.16 29028.90 ## 1982 1982 31132.46 442.7985 27443.61 8253.446 54420.97 28235.09 ## 1983 1983 31628.16 464.1769 28217.50 7784.859 54534.43 29260.59 ## 1984 1984 33182.41 470.9742 29301.37 7872.458 55973.70 31107.56 ## 1985 1985 34438.33 484.6437 30646.88 7872.018 57774.34 32118.76 Then use the gvisLineChart method to initialize the chart and then the plot() method to render it: Line &lt;- gvisLineChart(gdp, xvar = &quot;date&quot;, yvar = c(&quot;Mexico&quot;, &quot;United States&quot;)) plot(Line) where xvar is name of the character column which contains the category labels for the x-axes and yvar is a vector of column names of the numerical variables to be plotted. GoogleVis allows users to pass list of configuration options by using a named list options. The parameters have to map those of the Google documentation. Note that you need to use the R syntax and wrap configuration options into a character. For more details see the Google API documentation and the googleVis Reference Manual. In the following code, we create a line chart with two axis: Line &lt;- gvisLineChart(gdp, xvar = &quot;date&quot;, yvar = c(&quot;Mexico&quot;, &quot;United States&quot;), options=list( series=&quot;[{targetAxisIndex: 0}, {targetAxisIndex:1}]&quot;, vAxes=&quot;[{title:&#39;Mexico&#39;}, {title:&#39;United States&#39;}]&quot; )) plot(Line) To smooth the lines, you can curve the Lines by setting the curveType option to &quot;function&quot;: Line &lt;- gvisLineChart(gdp, xvar = &quot;date&quot;, yvar = c(&quot;Mexico&quot;, &quot;United States&quot;), options=list( series=&quot;[{targetAxisIndex: 0}, {targetAxisIndex:1}]&quot;, vAxes=&quot;[{title:&#39;Mexico&#39;}, {title:&#39;United States&#39;}]&quot;, curveType=&#39;function&#39; )) plot(Line) Compared to ggplot2, googleVis provides more interactive features. For example, you can target a single element using Crosshairs: Line &lt;- gvisLineChart(gdp, xvar = &quot;date&quot;, yvar = c(&quot;Mexico&quot;, &quot;United States&quot;), options=list( series=&quot;[{targetAxisIndex: 0}, {targetAxisIndex:1}]&quot;, vAxes=&quot;[{title:&#39;Mexico&#39;}, {title:&#39;United States&#39;}]&quot;, crosshair=&quot;{ trigger: &#39;both&#39; }&quot; )) plot(Line) This method are available for other charts including scatter charts, line charts, area charts, and combo charts. If the lines are too intensive or you would like to find detailed information of a line, you should use explorer function. This option allows users to pan and zoom Google charts. Setting dragToZoom allows users to zoom in and out when scrolling and rightClickToReset is for returning it to the original pan and zoom level clicking on the chart. You can also try other options like dragToPan and explorer.maxZoomIn. Line &lt;- gvisLineChart(gdp, options=list( explorer=&quot;{actions: [&#39;dragToZoom&#39;, &#39;rightClickToReset&#39;]}&quot; )) plot(Line) 36.3 Example: Geo Chart Geochart is far more interesting chart for showing GDP for different countries. A geochart is a map of a country, a continent, or a region with areas identified in one of three ways: The region mode colors whole regions, such as countries, provinces, or states. The markers mode uses circles to designate regions that are scaled according to a value that you specify. The text mode labels the regions with identifiers (e.g., “Russia” or “Asia”). In googleVis, creating geochart is simple. For regions mode format, you can call gvisGeoChart function and pass data containing region location and region color. Region location is a string of a country name (for example, “England”) or region code name (for example, “US”) or an area code. Region color is a numeric column used to assign a color to this region (for example, gdp value in our example). Note that markers mode format and text mode format have slightly different formats. dat &lt;- wb(indicator=&#39;NY.GDP.PCAP.KD&#39;, start=2018, end=2018) Geo &lt;- gvisGeoChart(dat, &quot;iso2c&quot;, &quot;value&quot;) print(Geo) &lt;!DOCTYPE html PUBLIC “-//W3C//DTD XHTML 1.0 Strict//EN” “https://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd”&gt; GeoChartID717141f2772d body { color: #444444; font-family: Arial,Helvetica,sans-serif; font-size: 75%; } a { color: #4D87C7; text-decoration: none; } Data: dat • Chart ID: GeoChartID717141f2772d • googleVis-0.6.4 R version 3.6.1 (2017-01-27) • Google Terms of Use • Documentation and Data Policy Like other chart, you can customize the colors of GeoCharts for background color, chart fill color, chart border color, etc. Please check correct format. Geo &lt;- gvisGeoChart(dat, &quot;iso2c&quot;, &quot;value&quot;, options=list( colorAxis=&quot;{colors:[&#39;yellow&#39;, &#39;orange&#39;, &#39;red&#39;, &#39;purple&#39;]}&quot;, backgroundColor=&quot;lightblue&quot;) ) print(Geo) &lt;!DOCTYPE html PUBLIC “-//W3C//DTD XHTML 1.0 Strict//EN” “https://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd”&gt; GeoChartID7171505391ae body { color: #444444; font-family: Arial,Helvetica,sans-serif; font-size: 75%; } a { color: #4D87C7; text-decoration: none; } Data: dat • Chart ID: GeoChartID7171505391ae • googleVis-0.6.4 R version 3.6.1 (2017-01-27) • Google Terms of Use • Documentation and Data Policy One important feature of googleVis is ChartEditor Class which is used to open an in-page dialog box that enables a user to customize a visualization on the fly. In this method, you can customize color, region, chart type and so on directly. And this class works for most Google Charts. Try click the “Edit me!” button in the following chart and see what happens. Geo &lt;- gvisGeoChart(dat, &quot;country&quot;, &quot;value&quot;, options=list( gvis.editor=&quot;Edit me!&quot;) ) plot(Geo) 36.4 Example: Sankey chart GoogleVis can draw Sankey chart as well. A sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called nodes and the connections are called links. Sankeys are best used when you want to show a many-to-many mapping between two domains (e.g., universities and majors) or multiple paths through a set of stages (for instance, Google Analytics uses sankeys to show how traffic flows from pages to other pages on your web site). For A, b # From Google Charts Guide data &lt;- data.frame(From=c(rep(&quot;A&quot;,3), rep(&quot;B&quot;, 3)), To=c(rep(c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;),2)), Weight=c(5,7,6,2,9,4)) Sankey &lt;- gvisSankey(data, from=&quot;From&quot;, to=&quot;To&quot;, weight=&quot;Weight&quot;) plot(Sankey) You can also create a Sankey chart with multiple levels of connections. # From googleVis Examples on CRAN data &lt;- data.frame(From=c(rep(&quot;A&quot;,3), rep(&quot;B&quot;, 3), rep(&quot;X&quot;,2), &quot;Y&quot;, rep(&quot;Z&quot;,3), &quot;W&quot;), To=c(rep(c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;),2), c(&quot;M&quot;, &quot;N&quot;), &quot;M&quot;, c(&quot;M&quot;, &quot;N&quot;, &quot;W&quot;), &quot;M&quot;), Weight=c(5,7,6,2,9,4,2,3,10,2,8,3,4)) Sankey &lt;- gvisSankey(data, from=&quot;From&quot;, to=&quot;To&quot;, weight=&quot;Weight&quot;) plot(Sankey) You can set custom colors for nodes and links using sankey.node and sankey.node. Both nodes and links can be given custom color palettes using their colors options. You can also set a coloring mode for the links between nodes using the colorMode option. # From googleVis Examples on CRAN data &lt;- data.frame(From=c(rep(&quot;A&quot;,3), rep(&quot;B&quot;, 3), rep(&quot;X&quot;,2), &quot;Y&quot;, rep(&quot;Z&quot;,3), &quot;W&quot;), To=c(rep(c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;),2), c(&quot;M&quot;, &quot;N&quot;), &quot;M&quot;, c(&quot;M&quot;, &quot;N&quot;, &quot;W&quot;), &quot;M&quot;), Weight=c(5,7,6,2,9,4,2,3,10,2,8,3,4)) Sankey &lt;- gvisSankey(data, from=&quot;From&quot;, to=&quot;To&quot;, weight=&quot;Weight&quot;, options=list( sankey=&quot;{link: {color: { fill: &#39;#d799ae&#39; } }, node: {color: { fill: &#39;#a61d4c&#39; }, label: { color: &#39;#871b47&#39;} }}&quot;)) plot(Sankey) ## Set options back to original options options(op) There are more interesting charts designed to address your data visualization needs we do not cover in this tutorial. Please see Chart Gallery if you like. 36.5 googleVis in RStudio Writing googleVis in RStudio is easy. Normally after runing plot command it will open a standalone browser window and render charts in the web page. Besides, RStudio also supports viewing local web content in Viewer pane. You can also use command plot(object, browser=rstudioapi::viewer) instead of plot(object) to view googleVis charts in Viewer window of Rstudio. To Knit the markdown file to HTML, you should print the chart element only op &lt;- options(gvis.plot.tag=&#39;chart&#39;) # or using plot(object, &#39;chart&#39;) when ploting charts and set the chunk option results to ’asis’ with {r results='asis'}. 36.6 Reference and Resource googleVis Source Code googleVis CRAN Page googleVis Examples googleVis Reference Manual Google Chart Tools Documentation Google APIs Terms of Service Author’s Blog "],
["interactive-graph-links.html", "Chapter 37 Interactive graph links 37.1 Bokeh Cheatsheet 37.2 SandDance (video) 37.3 OpenCPU (talk)", " Chapter 37 Interactive graph links 37.1 Bokeh Cheatsheet Zihe Wang(zw2624) and Yaotian Dai(yd2512) We created a cheat sheet for Bokeh, a python package great for interactive data visualization. Please visit it through the link: https://github.com/zw2624/bokeh_cheatsheet 37.2 SandDance (video) Mughilan Muthupari and Anjani Prasad Atluri We have made a video tutorial on SandDance (a visualization tool by Microsoft), posted on YouTube here. Note: The presentation used can be found in the description of the video. 37.3 OpenCPU (talk) Matthew Mackenzie 37.3.1 What is OpenCPU? OpenCPU is a “API for Embedded Scientific Computing.” OpenCPU consits of 3 main parts: a server to host OpenCPU apps locally or on the cloud, a HTTP API for data analysis using R, and a JavaScript library to integrate everything together. OpenCPU works as a platform to create web apps centered around using R for any needed data analysis and visualizations. 37.3.2 What is this Tutorial? There is not a whole lot of information out there having to do with actually creating an OpenCPU app, so this tutorial will attempt to piece what information is available together by working through an example project. There are 4 major steps involved: Creating a Disfunctional App: creating an R package to acomplish the data processing we need and the HTML for the user to interact with, OpenCPU.js: connecting the HTML to the R package with the OpenCPU JavaScript library, Local Development: testing the app locally, and App Deployment: deploying the app to the OpenCPU Cloud. As mentioned, this tutorial will be centered around an example… enter Distogram. 37.3.3 Distogram: A Working OpenCPU Example I wanted to keep things relatively simple, but I think this example gets the point across of the power of using R in the browser. Distogram is an app that prompts users to choose a sample size and probability distribution to sample from, uses R to create a histogram based on these and other parameters, and then presents that plot in the browser. The full tutorial, as well as the running example, can be found at the links below. Full Tutorial: mbmackenzie/distogram Example Application: mbmackenzie.ocpu.io/distogram "],
["stamen-maps-with-ggmap.html", "Chapter 38 Stamen maps with ggmap 38.1 Mutilayerd plots with ggmaps 38.2 Getting Deeper", " Chapter 38 Stamen maps with ggmap Mrugank Akarte Here is an example to get started with ggmap using get_stamenmap() to plot the longitude/latitude maps. The data for the following plots is available at https://simplemaps.com/data/us-cities. The get_stamenmap() function reqiures a bounding box, i.e the top, bottom, left and right latitude/longitude of the map you want to plot. For example, the latitude/longitude for US map are as follows: bbox &lt;- c(bottom = 25.75, top = 49 , right = -67, left = -125) You can find these values from https://www.openstreetmap.org. The other important parameters of this function are zoom and maptype. Higher the zoom level, the more detailed your plot will be. Beaware that ggmap connects to Stamen Map server to download the map, so if your bounding box is large and zoom level is high, it will have to download a lot of data and may take some time. There are differnt types of plots available via Stamen Map like terrain, watercolor, toner which can be set to maptype parameter according to your preference. You can find about avaiable options in help (?get_stamenmap). For the following examples the maptype is set to ‘toner-lite’. Let’s plot the US map. library(ggmap) usmap &lt;- get_stamenmap(bbox = bbox, zoom = 6, maptype = &#39;toner-lite&#39;) ggmap(usmap) Great! We have the US map, now let’s use the US population data to see the spread of counties across nation. Notice that we haven’t included Alaska in the map and hence will be removing the data from Alaska. library(dplyr) df &lt;- read.csv(unz(&#39;resources/ggmap/data/uscities.zip&#39;, &#39;uscities.csv&#39;)) # Removing data of Alaska from dataset df &lt;- df %&gt;% filter(state_name != &#39;Alaska&#39;) # Spread of counties across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Spread of counties across US&#39;) This is not good! Most of the points are overlapping and thus it is not easy to interpret what’s going on here. Let’s try alpha blending and reduce the size of points. # Spread of counties across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population), size = 0.8, stroke= 0, alpha = 0.4) + ggtitle(&#39;Spread of counties across US&#39;) That’s much better! We can now easily identify the areas where number of counties are more. You might have noticed there is no light blue dot visible on the plot. This is because it must be lying somewhere between those dense areas. One such location is New York, you can find this out by zooming the plot. Another reason is that when you use alpha blending, your colors fade and thus it becomes difficult to identify such points. We can also look at spread of counties using geom_density as follows # spread of counties across US using Density_2d ggmap(usmap) + geom_density_2d(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Spread of counties across US&#39;) 38.1 Mutilayerd plots with ggmaps We can add multiple layers to the plot as described in earlier chapters. Let’s look at the location of military stations located across US along with population density. # Location of Military units df1 &lt;- df %&gt;% filter(military == TRUE) ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population, text = city), show.legend = F, size = 0.8, stroke= 0, alpha = 0.4) + geom_point(data = df1, mapping = aes(x = lng, y = lat , text = city), show.legend = F, size = 0.9, color = &#39;red&#39;) + ggtitle(&#39;Military stations across US&#39;) As you can see, there are 3 layers in this plot. First base layer consists of US map, second layer consists of spread of counties across US and the third layer consists of location of military bases. It is not easy to plot such multilayered graphs using other packages. Let’s zoom the map for state of California and see some other map types offered by Stamen Maps. # California Boundaries par(mfrow=c(3,1)) CAbox &lt;- c(bottom = 32.213, top = 42.163 , right = -113.95, left = -124.585) camap1 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;watercolor&#39;) camap2 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;terrain&#39;) camap3 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;toner-hybrid&#39;) ggmap(camap1) ggmap(camap2) ggmap(camap3) 38.2 Getting Deeper This was just a glimpse of what you can do with ggmaps using the get_stamenmap(). Note that Stamen Maps is not limited to US and can be used to plot any part of the world. If you liked this alternative to Google Maps API, I highly recommend you to check the Stamen Maps website http://maps.stamen.com for more details. "],
["mapping-in-r.html", "Chapter 39 Mapping in R 39.1 Overview 39.2 What is maps? 39.3 Installing maps 39.4 Simple Demonstration (using maps) 39.5 Simple Demonstration (using ggplot2) 39.6 Mapping with geom_map 39.7 Considerations 39.8 External Resources", " Chapter 39 Mapping in R Hanjun Li and Chengchao Jin 39.1 Overview This section covers how to draw maps using R. The packages we will be using are ggplot2 and maps. 39.2 What is maps? The package maps, which contains a lot of outlines of continents, countries, states, and counties, is used to visualize geographical map. Some notable features and functions of maps are: county: counties of the United States mainland generated from US Department of the Census data. we can use county.fips to check the all counties listed. map: main function used to draw lines and polygons as specified by a map database. state: states of the United States mainland generated from US Department of the Census data usa: this database produces a map of the United States 39.3 Installing maps You can install maps from CRAN: install.packages(&quot;maps&quot;) Load the required packages library(maps) library(ggplot2) 39.4 Simple Demonstration (using maps) maps::map(&quot;usa&quot;, col = &quot;#9FF781&quot;, fill = TRUE) map.axes(cex.axis=0.8) maps::map(&quot;state&quot;, lty = 2, add = TRUE, col = &quot;#0B3B39&quot;) # map the state borderline to the US map title(main = &quot;United States Mainland by States&quot;, xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, cex.lab = 0.8) The function map(&quot;usa&quot;) plots a map of the United States Mainland. The x-axis and y-axis represent longtitude and latitude respectively. Positive latitude is above the equator (N), and negative latitude is below the equator (S). Positive longitude is east of the prime meridian, while negative longitude is west of the prime meridian (a north-south line that runs through a point in England). The function map(&quot;state&quot;,add=True) adds the state borderline to the US map. maps::map(&#39;county&#39;, region = &#39;new york&#39;, col = &quot;#5E610B&quot;) map.cities(us.cities, country=&quot;NY&quot;, col = &quot;#642EFE&quot;, cex = 0.6) # map cities recorded in us.cities to NY State map.axes(cex.axis=0.8) title(main = &quot;New York State by Counties&quot;, xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, cex.lab = 0.8) The graph above plots the state map of New York. The function map.cities points out the recorded cities in us.cities in New York State map (blue dots). 39.5 Simple Demonstration (using ggplot2) We would first introduce the function map_data from ggplot2, which converts the map to a data frame. The most important variable we need to pass to map_data is the name of map provided by the maps package. These include: maps::usa, maps::france, maps::italy and etc. usa &lt;- map_data(&quot;usa&quot;) class(usa) ## [1] &quot;data.frame&quot; head(usa) ## long lat group order region subregion ## 1 -101.4078 29.74224 1 1 main &lt;NA&gt; ## 2 -101.3906 29.74224 1 2 main &lt;NA&gt; ## 3 -101.3620 29.65056 1 3 main &lt;NA&gt; ## 4 -101.3505 29.63911 1 4 main &lt;NA&gt; ## 5 -101.3219 29.63338 1 5 main &lt;NA&gt; ## 6 -101.3047 29.64484 1 6 main &lt;NA&gt; ggplot() + geom_polygon(data = usa, aes(x=long, y = lat), fill = &quot;#9F81F7&quot;) + labs(title = &quot;Map of the United States Mainland&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_fixed(1.3) + theme(panel.background = element_blank()) We could plot the map of the United States Mainland using ggplot2 as well. First, we require a data which contains information of longitude and latitude. The function map_data can easily turn data from the maps package into a data frame suitable for plotting with ggplot2. Then we pass the suitable data frame to geom_polygon function. Again, the two axis represent longitude and latitude. states &lt;- map_data(&quot;state&quot;) counties &lt;- map_data(&quot;county&quot;) NewYork &lt;- subset(states, region == &quot;new york&quot;) head(NewYork) ## long lat group order region subregion ## 9050 -73.92874 40.80605 34 9050 new york manhattan ## 9051 -73.93448 40.78886 34 9051 new york manhattan ## 9052 -73.95166 40.77741 34 9052 new york manhattan ## 9053 -73.96312 40.75449 34 9053 new york manhattan ## 9054 -73.96885 40.73730 34 9054 new york manhattan ## 9055 -73.97458 40.72584 34 9055 new york manhattan ny_county &lt;- subset(counties, region == &quot;new york&quot;) head(ny_county) ## long lat group order region subregion ## 52932 -73.78550 42.46763 1795 52932 new york albany ## 52933 -74.25533 42.41034 1795 52933 new york albany ## 52934 -74.27252 42.41607 1795 52934 new york albany ## 52935 -74.24960 42.46763 1795 52935 new york albany ## 52936 -74.22668 42.50774 1795 52936 new york albany ## 52937 -74.23241 42.56504 1795 52937 new york albany ggplot() + geom_polygon(data = NewYork, aes(x=long, y = lat, fill = subregion)) + geom_polygon(data = ny_county, aes(x=long, y = lat, group = group), color = &quot;white&quot;, fill = NA) + labs(title = &quot;New York State by Counties&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_fixed(1.3) + theme(panel.background = element_blank()) To plot the map of New York State, we need to preprocess the data frame using map_data and subset. We first fill the map by subregions, and then we add the borderlines to the map. 39.6 Mapping with geom_map we will use the built-in state.x77 dataset. This 50 by 8 dataset contains some US State facts and figures. For instance, the variable Population indicates the population estimate as of July 1, 1975 in each states. For our example, we choose to investigate Income and Murder head(state.x77) ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 library(tidyverse) df &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) df$state &lt;- tolower(df$state) ggplot(df, aes(map_id = state)) + geom_map(aes(fill = Income), map = states) + expand_limits(x = states$long, y = states$lat) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;#FE2EC8&quot;) + labs(title = &quot;US Per Capita Income by States, 1974&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;, caption = &quot;source: https://www.rdocumentation.org/packages/ datasets/versions/3.6.1/topics/state&quot;) + coord_fixed(1.3) + theme(panel.background = element_blank()) The graph above plots the heatmap of US per Capita income by state in 1974. To plot the state map, we need to preprocess data to make sure the state names are all in lowercase. Then, we use map_id to plot the states. We set the fill in geom_map function to the variable of interest to plot the heatmap. ggplot(df, aes(map_id = state)) + geom_map(aes(fill = Murder), map = states, col = &quot;white&quot;) + expand_limits(x = states$long, y = states$lat) + scale_fill_distiller(name = &quot;murder rate&quot;, palette = &quot;Spectral&quot;) + labs(title = &quot;US Murder Rate per 100,000 Population, 1976&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;, caption = &quot;source: https://www.rdocumentation.org/packages/ datasets/versions/3.6.1/topics/state&quot;) + coord_fixed(1.3) + theme_minimal() This is another example of plotting with geom_map. The graph shows US murder rate in 1976. 39.7 Considerations When we visualize the map using ggplot2 and geom_ploygon, it is necessary to add coord_fixed() to ggplot as it fixes the ratio between x and y direction. The value 1.3 we used in coord_fixed() is an arbitrary value that makes the plot look good 39.8 External Resources https://cran.r-project.org/web/packages/maps/maps.pdf is an R documentation on the package maps. https://www.rdocumentation.org/packages/maps/versions/3.3.0/topics/map talks specifically about the function map in the package maps. https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html demonstrates some examples of mapping in R The package ggmap is also used to draw maps. It uses the Google map platform and users need to register for an API key prior to accessing the database. More details on https://cran.r-project.org/web/packages/ggmap/ggmap.pdf and https://rdrr.io/cran/ggmap/man/register_google.html "],
["plotting-maps-with-r-an-example-based-tutorial.html", "Chapter 40 Plotting Maps with R: An Example-Based Tutorial 40.1 Plotting using base R 40.2 Plotting using ggplot2 40.3 Plotting interactively using leaflet 40.4 Plotting using tmap", " Chapter 40 Plotting Maps with R: An Example-Based Tutorial Jonathan Santoso and Kevin Wibisono In this short tutorial, we would like to introduce several different ways of plotting choropleth maps, i.e. maps which use differences in shading, colouring, or the placing of symbols within areas to indicate a particular quantity associated with each area, using R. The data set used throughout this tutorial is the 2015 to 2019 crime data from the city of Milwaukee, Wisconsin (obtained from https://data.milwaukee.gov/dataset/wibr). The variables of interest are as follows: ReportedYear, which takes integer values from 2015 to 2019. ALD (Aldermanic District), which takes integer values from 1 to 15. Each of these districts will be represented with an area in our choropleth maps. The data set contains some observations whose ALD equal 0 or NA, and we decided not to include these observations in our exploratory data analysis and visualisation. Arson, which takes binary values. It has a value of 1 if and only if the crime can be categorised as an arson. AssaultOffense, which takes binary values. It has a value of 1 if and only if the crime can be categorised as an assault offence. CriminalDamage, which takes binary value. It has a value of 1 if and only if the crime can be categorised as a criminal damage to property. LockedVehicle, which takes binary value. It has a value of 1 if and only if the crime can be categorised as a locked vehicle entry. VehicleTheft, which takes binary value. It has a value of 1 if and only if the crime can be categorised as a vehicle theft. We note that a crime can be categorised as more than one categories. For example, the 27th row of the data set refers to a crime categorised as both an assault offence and a criminal damage to property. As a first step, we load all the necessary libraries. library(rgdal) # R wrapper around GDAL/OGR library(tidyverse) library(RColorBrewer) library(ggplot2) library(leaflet) library(tmap) In order to plot custom map boundaries, we will need a .shp file for the boundaries, which can be obtained from https://data.milwaukee.gov/dataset/aldermanic-districts. This file contains coordinates, labels and shapes, and can be read (and automatically parsed) using the ‘rgdal’ package. In order to access the data in the .shp file, we use the command shapefile@data. Also, we use the fortify method to convert the .shp file into a dataframe. The chunk of code below converts the .shp file into a dataframe # reads in the SHP file shapefile &lt;- readOGR(&quot;./resources/plotting_maps_tutorial/alderman_coord.shp&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/home/travis/build/jtr13/cc19/resources/plotting_maps_tutorial/alderman_coord.shp&quot;, layer: &quot;alderman_coord&quot; ## with 15 features ## It has 6 fields ## Integer64 fields read as strings: ALD COLORCAT shapefile_df &lt;- fortify(shapefile) shapefile_df$id &lt;- as.factor(shapefile_df$id) Now, we load the crime data, focussing on the seven columns mentioned above. Note that we delete rows whose ALD values are 0 or NA. # load the data crimes &lt;- read_csv(&#39;./resources/plotting_maps_tutorial/wibr.csv&#39;) crimes_df &lt;- crimes %&gt;% select(ReportedYear, ALD, LockedVehicle, VehicleTheft) %&gt;% filter(!(ALD %in% c(0, NA))) In order to label the plots nicely, we will need to plot the legends at the centroid of each polygon, whence centroid calculations must be performed. Also, we will need to map the ID column in the .shp file to our desired labelling, which is ALD. The mapping can be found in the .shp data file, where the row names corresponds to ID and the ALD column corresponds to our labels. Reference: https://stackoverflow.com/questions/28962453/how-can-i-add-labels-to-a-choropleth-map-created-using-ggplot2. # labels lab &lt;- data.frame(shapefile$ALD, shapefile$ALDERMAN) lab &lt;- mutate(lab, id = strtoi(rownames(lab)) - 1) # centroid calculations centroids &lt;- setNames(do.call(&quot;rbind.data.frame&quot;, by(shapefile_df, shapefile_df$id, function(x) {Polygon(x[c(&#39;long&#39;, &#39;lat&#39;)])@labpt})), c(&#39;long&#39;, &#39;lat&#39;)) centroids$factors &lt;- levels(shapefile_df$id) centroids &lt;- merge(centroids, lab, by.x = &quot;factors&quot;, by.y = &quot;id&quot;, all.x = TRUE) #remove lab rm(lab) 40.1 Plotting using base R Now, let’s use base R to visualise the number of vehicle-related crimes in each of the fifteen districts in 2018. #create the veh_2018 dataframe veh_2018 &lt;- crimes_df %&gt;% filter(ReportedYear == 2018) %&gt;% group_by(ALD, ReportedYear) %&gt;% summarise(sum(LockedVehicle),sum(VehicleTheft)) %&gt;% mutate(Vehicle = `sum(LockedVehicle)` + `sum(VehicleTheft)`) %&gt;% select(ALD, Vehicle, ReportedYear) %&gt;% as.data.frame() # first, we create a copy temp of the .shp file since we do not want to modify the original fie. # next, we add a column called identifier to temp@data in order to preserve the sorting order and not to mess up the labelling. # after that, we can merge temp@data with the arson dataframe with a common key, i.e. ALD. # we then reorder the data based on the original order by using the identifier column, and assign it back to the .shp file variable. temp &lt;- shapefile a &lt;- temp@data a &lt;- a %&gt;% mutate(identifier = strtoi(rownames(a)) + 1) b &lt;- sp::merge(a, veh_2018, by = &quot;ALD&quot;, all.x= TRUE) b &lt;- b[order(b$identifier),] row.names(b) &lt;- 0:14 temp@data &lt;- b # remove unnecessary variables rm(a) rm(b) # plotting in base R requires us to define the colour schemes. In this tutorial, we will use the brewer.pal method to generate the colours. # we also need to define our own intervals for cutting the target variable, and map our target to the interval. my_colors &lt;- brewer.pal(8, &quot;YlGn&quot;) mybreaks &lt;- seq(0, 1400, 200) cut(temp@data$Vehicle, mybreaks) ## [1] (400,600] (200,400] (400,600] (400,600] (0,200] (200,400] ## [7] (400,600] (600,800] (400,600] (200,400] (800,1e+03] (400,600] ## [13] (600,800] (400,600] (600,800] ## 7 Levels: (0,200] (200,400] (400,600] (600,800] ... (1.2e+03,1.4e+03] mycolourscheme &lt;- my_colors[findInterval(temp@data$Vehicle, vec = mybreaks)] # we can then generate our plot from the modified .shp file # the labels are generated from the centroids plot(temp, col = mycolourscheme, main = &quot;Vehicle-Related Crimes by Milwaukee Districts in 2018&quot;, cex = 5, ylim = c(min(shapefile_df$lat) - 0.05, max(shapefile_df$lat))) text(centroids$long, centroids$lat, labels = centroids$shapefile.ALD) legend(min(shapefile_df$long) - 0.3, min(shapefile_df$lat) + 0.12, legend = levels(cut(temp@data$Vehicle, mybreaks)), fill = my_colors, cex = 0.8, title = &quot;Vehicle-Related Crimes&quot;) Plotting maps in base R can be frustrating sometimes. Even though we only need to write a relatively short code, we are required to manually define the colour schemes. Moreover, we will also need to modify the data in the .shp file since we can only plot from the S4 data type. 40.2 Plotting using ggplot2 Next, we will produce a similar chart using ggplot2. This time, we will use the same vehicle data frame as we used in the previous plot. # first, we need to create a dataframe to map id to ALD lab &lt;- data.frame(shapefile$ALD, shapefile$ALDERMAN) lab &lt;- mutate(lab, id = strtoi(rownames(lab)) - 1) # in order to use ggplot, we will need the fortified dataset and merge it with the external data set using the id ggplotdf &lt;- merge(shapefile_df, lab, by = &quot;id&quot;, all.x = TRUE) ggplotdf &lt;- merge(ggplotdf, veh_2018, by.x = &quot;shapefile.ALD&quot;, by.y = &quot;ALD&quot;, all.x = TRUE) # then, we can use the geom_polygon method to create the boundaries and use the external data set fill. This is similar to a normal ggplot2 syntax. map &lt;- ggplot(data = ggplotdf, aes(x = long, y = lat)) + geom_polygon(mapping = aes(group = group, fill = Vehicle), color = &quot;white&quot;) + with(centroids, annotate(geom=&quot;text&quot;, x = long, y = lat, label=shapefile.ALD, size=3)) + scale_fill_gradient(low=&quot;white&quot;, high=&quot;darkgreen&quot;) + ggtitle(&quot;Vehicle-Related Crimes by Milwaukee Districts in 2018&quot;) + theme(plot.title = element_text(hjust = 0.5)) map Plotting using ggplot2 is fairly straightforward since we only need to merge a fortified data set and an external data set. We could then proceed with the usual ggplot2 grammar of graphics syntax. 40.3 Plotting interactively using leaflet Now, we will create our map using leaflet, which provides interactivity features. For this plot, we are using the same dataframe. # first, we create a copy temp of the .shp file since we do not want to modify the original fie. # next, we add a column called identifier to temp@data in order to preserve the sorting order and not to mess up the labelling. # after that, we can merge temp@data with the arson dataframe with a common key, i.e. ALD. # we then reorder the data based on the original order by using the identifier column, and assign it back to the .shp file variable. temp &lt;- shapefile a &lt;- temp@data a &lt;- a %&gt;% mutate(identifier = strtoi(rownames(a)) + 1) b &lt;- sp::merge(a, veh_2018, by = &quot;ALD&quot;, all.x= TRUE) b &lt;- b[order(b$identifier),] row.names(b) &lt;- 0:14 temp@data &lt;- b # remove unnecessary variables rm(a) rm(b) # labels for interactivity labels &lt;- paste0( &quot;&lt;strong&gt;Alderman District: &lt;/strong&gt;&quot;, temp$ALD, &quot;&lt;br/&gt;Vehicle-related crimes in 2018: &quot;, temp$Vehicle ) # define a color scheme pal &lt;- colorQuantile(&quot;YlGn&quot;, NULL, n = 5) # create leaflet plot m &lt;- leaflet() %&gt;% setView(lng = -87.9065, lat = 43.0389, zoom = 11) %&gt;% addTiles() %&gt;% addPolygons(data = temp, fillColor = ~pal(Vehicle), fillOpacity = 0.8, color = &quot;#BDBDC3&quot;, weight = 1, popup = labels) m 40.4 Plotting using tmap We will now use tmap to generate a faceted map of the sum of vehicle-related crimes in 2016 to 2019. #create the veh dataframe veh &lt;- crimes_df %&gt;% filter(ReportedYear %in% c(2016,2017,2018,2019)) %&gt;% group_by(ALD, ReportedYear) %&gt;% summarise(sum(LockedVehicle),sum(VehicleTheft)) %&gt;% mutate(Vehicle = `sum(LockedVehicle)` + `sum(VehicleTheft)`) %&gt;% select(ALD, Vehicle, ReportedYear) %&gt;% as.data.frame() #modify the .shp file temp &lt;- shapefile a &lt;- temp@data a &lt;- a %&gt;% mutate(identifier = strtoi(rownames(a)) + 1) b &lt;- sp::merge(a, veh, by = &quot;ALD&quot;, all.x= TRUE) b &lt;- b[order(b$identifier),] b &lt;- b[order(b$ReportedYear),] row.names(b) &lt;- 0:59 temp@data &lt;- b # remove unnecessary variables rm(a) rm(b) #create maps facetmaps &lt;- tm_shape(temp) + tm_borders() + tm_facets(by = &quot;ReportedYear&quot;, nrow = 2, free.coords = TRUE) + tm_fill(col=&#39;Vehicle&#39;, palette = &#39;YlGn&#39;) + tm_layout(title = &quot;Number of Vehicle-Related Crimes in Milwaukee&quot;) facetmaps We can also generate an animated map based on the faceted map above. #create animated map animatedmaps &lt;- tm_shape(temp) + tm_borders() + tm_facets(along = &quot;ReportedYear&quot;, nrow = 2, free.coords = FALSE) + tm_fill(col=&#39;Vehicle&#39;, palette = &#39;YlGn&#39;) # code to create tmap animation #gif &lt;- tmap_animation(animatedmaps, filename = &#39;edav.gif&#39;, width = 800, height = 1000, delay = 50) The tmap_animation method will automatically generate a .gif file named ‘edav.gif’ in the same working directory as this .Rmd file. In order to display the .gif file, one may need to upload the file to giphy, and insert the link in the plain text of the .Rmd file. The link for this .gif file is https://media.giphy.com/media/lp6S5QJA78fMFfnUAh/giphy.gif. From these plots, at least two insights can be drawn: Vehicle-related crimes more often happened in downtown districts (e.g. 3, 4, 6 and 12). This trend is consistent across the years. Using the facet map or the animated map, we can clearly see that the number of vehicle-related crimes in most districts had decreased quite signiificantly throughout the year. In conclusion, ggplot2 offers a practical yet powerful way to plot maps. The same holds for leaflet, which provides interactivity. One may also consider tmap, a “powerful and flexible map-making package” which allows for a broader range of spatial classes. As tmap is built on the basis of a grammar of graphics, users already familiar with ggplot2 should be able to learn to use this versatile package easily. In the future, this tutorial can be expanded to create interactive plots that display how crime varies across years and potentially selectors to visualise different crimes in a single plot. Happy coding with R! "],
["different-ways-of-plotting-u-s-map-in-r.html", "Chapter 41 Different Ways of Plotting U.S. Map in R 41.1 Introduction 41.2 Using usmap package 41.3 Using ggplot2 package 41.4 Using maps package 41.5 Using plotly package 41.6 Using mapview package 41.7 Using leaflet package 41.8 Using tmap package", " Chapter 41 Different Ways of Plotting U.S. Map in R Zhiyi Guo and Fan Wu 41.1 Introduction When dealing with national data/geographical data, i.e., election results, it is often useful to visualize the data onto a map as it could help draw conclusion geographycally. Hence, in this tutorial, we will explore different packages that could help with mapping geographical data, specifically for the U.S. 41.2 Using usmap package This is probabily the most convinent package to plot a U.S. map along with some data. usmap provides very helpful functions to select certain regions within the U.S.. Another feature of this package is that it creates a ggplot object and hence we could use all the nice functions that come with ggplot2 package. Package(s) we need: #install.packages(&quot;usmap&quot;) library(usmap) #import the package library(ggplot2) #use ggplot2 to add layer for visualization Plot all states of the U.S. to create an empty map. plot_usmap(regions = &quot;states&quot;) + labs(title = &quot;U.S. States&quot;, subtitle = &quot;This is a blank map of the United States.&quot;) + theme(panel.background=element_blank()) Besides states, we could also plot all counties of the U.S. plot_usmap(regions = &quot;counties&quot;) + labs(title = &quot;U.S. counties&quot;, subtitle = &quot;This is a blank map of the United States.&quot;) + theme(panel.background=element_blank()) usmap also provides many different regions to select, for example, the South region. The “include” and “exclude” functions give us freedom to select/delete regions as we desire, and this works for states and counties. plot_usmap(include = .south_region, exclude = c(&quot;VA&quot;), labels = TRUE) You could also select certain states using factors(state abbreviations). plot_usmap(include = c(&quot;CT&quot;, &quot;ME&quot;, &quot;MA&quot;, &quot;NH&quot;, &quot;VT&quot;)) + labs(title = &quot;New England Region&quot;) + theme(panel.background = element_rect(color = &quot;blue&quot;)) The complete list of regions to choose is the following: .east_north_central: East North Central census division .east_south_central: East South Central census division .midwest_region: Midwest census region .mid_atlantic: Mid-Atlantic census division .mountain: Mountain census division .new_england: New England census division .northeast_region: Northeast census region .north_central_region: North-Central census region .pacific: Pacific census division .south_atlantic: South Atlantic census division .south_region: South census region .west_north_central: West North Central census division .west_region: West census region .west_south_central: West South Central census division usmap really provides users with the choice to mix and match areas of interest, hence makes it very desirable to manipulate the map. The package comes with geographical/national data sets such as population, poverty, and etc. For example, using countypov data set from the package, we could see the poverty estimates for New England counties in 2014: plot_usmap(data = countypov, values = &quot;pct_pov_2014&quot;, include = c(&quot;CT&quot;, &quot;ME&quot;, &quot;MA&quot;, &quot;NH&quot;, &quot;VT&quot;), color = &quot;blue&quot;) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;blue&quot;, name = &quot;Poverty Percentage Estimates&quot;, label = scales::comma) + labs(title = &quot;New England Region&quot;, subtitle = &quot;Poverty Percentage Estimates for New England Counties in 2014&quot;) + theme(legend.position = &quot;right&quot;) We could also see population of these counties in 2015: plot_usmap(data = countypop, values = &quot;pop_2015&quot;, include = .new_england, color = &quot;red&quot;) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;red&quot;, name = &quot;Population&quot;, label = scales::comma) + labs(title = &quot;New England Region&quot;, subtitle = &quot;Population in New England Counties in 2014&quot;) + theme(legend.position = &quot;right&quot;) 41.3 Using ggplot2 package (Note: In this part, we only care about continental U.S.) First, we get U.S. map data. In R, there are packages named maps and mapdata which save a lot of map information, for instance, continents, countries and states. We can use their data directly by using map_data function in ggplot2 package. Package(s) we need: library(ggplot2) library(maps) library(mapdata) usa &lt;- map_data(&#39;usa&#39;) Next, we use geom_ploygon function to plot U.S. map. ggplot(data=usa, aes(x=long, y=lat, group=group)) + geom_polygon(fill=&#39;lightblue&#39;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + ggtitle(&#39;U.S. Map&#39;) + coord_fixed(1.3) In order to get more information about U.S., now we can add state data to the map. Like above, we obtain data from maps and mapdata packages, and use ggplot2 to plot. state &lt;- map_data(&quot;state&quot;) ggplot(data=state, aes(x=long, y=lat, fill=region, group=group)) + geom_polygon(color = &quot;white&quot;) + guides(fill=FALSE) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + ggtitle(&#39;U.S. Map with States&#39;) + coord_fixed(1.3) What if we only care about one state? Definitely, we can draw one state map using ggplot2. Firstly we need to filter data. washington &lt;- subset(state, region==&quot;washington&quot;) counties &lt;- map_data(&quot;county&quot;) washington_county &lt;- subset(counties, region==&quot;washington&quot;) ca_map &lt;- ggplot(data=washington, mapping=aes(x=long, y=lat, group=group)) + coord_fixed(1.3) + geom_polygon(color=&quot;black&quot;, fill=&quot;gray&quot;) + geom_polygon(data=washington_county, fill=NA, color=&quot;white&quot;) + geom_polygon(color=&quot;black&quot;, fill=NA) + ggtitle(&#39;Washington Map with Counties&#39;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) ca_map 41.4 Using maps package (Note: In this part, we only care about continental U.S.) We also can directly plot using maps package. However, with only maps package, we are limited by mapping techniques. Thus, we recommend using other packages instead. Package(s) we need: #install.packages(&quot;maps&quot;) library(maps) maps::map(&quot;state&quot;, interior=FALSE) maps::map(&quot;state&quot;, boundary=FALSE, col=&quot;gray&quot;, add=TRUE) 41.5 Using plotly package We first need to import data from US-State-Boundaries-Census-2014.shp, and then use plotly package. Suppose we care about water level for each state in U.S. Package(s) we need: library(tidyverse) library(sf) library(plotly) states &lt;- read_sf(&quot;resources/us_map/US-State-Boundaries-Census-2014.shp&quot;) %&gt;% st_zm() %&gt;% mutate(water_km2 = (AWATER / (1000*1000)) %&gt;% round(2)) g &lt;- ggplot(states) + geom_sf(aes(fill=water_km2)) + scale_fill_distiller(&quot;water level&quot;, palette=&quot;Spectral&quot;) + ggtitle(&quot;Water by State&quot;) ggplotly(g) 41.6 Using mapview package We use the same dataset from last method. It gives us an interactive map with all information. Package(s) we need: #install.packages(&quot;mapview&quot;) library(mapview) mapview(states) 41.7 Using leaflet package We use the same dataset from last method. We construct a interactive map with no specific information. If you are interested in a specific area, for example, water level, you can add specific choropleth to show more information. Package(s) we need: #install.packages(&quot;leaflet&quot;) library(leaflet) leaflet(states) %&gt;% addTiles() %&gt;% addPolygons() 41.8 Using tmap package tmap is a very powerful map-drawing package that could create various types of maps. In our tutorial, we will focus on how to create an interactive U.S. map using tmap package. Package(s) we need: #devtools::install_github(&#39;walkerke/tigris&#39;) #install.packages(&quot;tmaptools&quot;) #install.packages(&quot;tmap&quot;) library(tigris) #download shp file from U.S. census website library(tmaptools) library(tmap) Using tmap requires a shapefile format file that contains vector data of locations, shape and attributes of geographic features. Thus, before we could use the package, we have to import the “shapes” of the map first. We will use the shapefile of the U.S. states defined by the U.S. government, downloaded from U.S. census website. We will then read in the shp file to create a spatial object with U.S. states. download.file(&quot;http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_state_20m.zip&quot;, destfile = &quot;states.zip&quot;) unzip(&quot;states.zip&quot;) us_geo&lt;-read_shape(&quot;cb_2015_us_state_20m.shp&quot;, as.sf = TRUE, stringsAsFactors = FALSE) Using the 2015 state population data set from usmap package, we could then draw a interactive map using tmap package. pop &lt;- usmap::statepop #read in the dataset popmap &lt;- append_data(us_geo, pop, key.shp = &quot;NAME&quot;, key.data = &quot;full&quot;) #merge the data set and the spatial obect # not working, issue with leaflet-providers html_dependency # map &lt;- qtm(popmap, fill = &quot;pop_2015&quot;) #draw a static map first # interactive_map&lt;-tmap_leaflet(map) #then add interactivity to the map # interactive_map "],
["using-stamen-maps-for-plotting-spatial-data.html", "Chapter 42 Using Stamen Maps for Plotting Spatial Data", " Chapter 42 Using Stamen Maps for Plotting Spatial Data Kumari Nishu and Neelam Patodia Objective: We intend to highlight the usability of Stamen Maps for visualizing spatial data. Approach: We conduct a comparative study between different types of graphs to understand the visualization tool which best demonstrates spatial data.We use the publicly available data on the number of vehicle collisions in New York for this demonstration. library(dplyr) library(ggmap) # Dataset available at &quot;https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95/data&quot;. raw_df &lt;- read.csv(file =&quot;https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv&quot;, header = TRUE, sep = &quot;,&quot;) # Selecting specific columns and rows reasons &lt;- c(&quot;Driver Inattention/Distraction&quot;, &quot;Failure to Yield Right-of-Way&quot;, &quot;Following Too Closely&quot;, &quot;Fatigued/Drowsy&quot;, &quot;Backing Unsafely&quot;) df &lt;- raw_df %&gt;% select(ACCIDENT.TIME, BOROUGH, LATITUDE, LONGITUDE, NUMBER.OF.PERSONS.INJURED, CONTRIBUTING.FACTOR.VEHICLE.1) %&gt;% na_if(&quot;&quot;) %&gt;% # recode empty strings &quot;&quot; by NAs na.omit %&gt;% # remove NAs filter(NUMBER.OF.PERSONS.INJURED != 0) %&gt;% filter(CONTRIBUTING.FACTOR.VEHICLE.1 %in% reasons) df$TIME &lt;- as.character(df$ACCIDENT.TIME) df$TIME &lt;- as.numeric(unlist(strsplit(df$TIME, &quot;:&quot;))[seq(1, 2 * nrow(df), 2)]) #creating label based on the hour of day breaks &lt;- c(0, 6, 12, 18, 24) labels &lt;- c(&quot;Nght&quot;, &quot;Morng&quot;, &quot;Noon&quot;, &quot;Evng&quot;) df$TIME_LABEL &lt;- cut(x=df$TIME, breaks = breaks, labels = labels, include.lowest=TRUE) For the purpose of our analysis, we would try to see patterns associated with number of persons injured. For this we have selected the following potential parameters : Time, Latitude, Longitude, Borough and Contributing Factor with respect to vehicles. Since data was available for each granular time, we’ve created a broader bucket to categorise time into 4 slots. library(forcats) # Summary of dataset # accident count by borough ggplot(data=df, aes((fct_rev(fct_infreq(BOROUGH))))) + geom_bar( color=&quot;blue&quot;, fill=&quot;LIGHTBLUE&quot;) + ggtitle(&quot;Number of Accidents in different BOROUGH&quot;, ) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;Borough&quot;) +ylab(&quot;Accidents Count&quot;) + coord_flip() + theme(legend.position = &quot;top&quot;) # accidents by contributing factors ggplot(data=df, aes((fct_rev(fct_infreq(CONTRIBUTING.FACTOR.VEHICLE.1))))) + geom_bar( color=&quot;black&quot;, fill=&quot;tomato&quot;) + ggtitle(&quot;Number of Accidents due to different Factors&quot;, ) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;Contributing Factor&quot;) +ylab(&quot;Accidents Count&quot;) + coord_flip() + theme(legend.position = &quot;top&quot;) # Number of accidents across different Boroughs for different time slots and contributing factors ggplot(df, aes(fill=CONTRIBUTING.FACTOR.VEHICLE.1, y=NUMBER.OF.PERSONS.INJURED, x=TIME_LABEL)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) + facet_wrap(~BOROUGH)+ ggtitle(&quot;Number of Accidents due to different Factors across differnt time and Boroughs&quot;, ) Using the plots above, we had to plot 5 graphs (3rd GGPLOT) to understand the pattern of accidents occuring across differnt Boroughs at different times and the factor contributing the most to the accident. However, this results in assimilating information from multiple graphs which is indeed tedious. Additionally, if we were to further analyse the sub locatlities within each Borough, plotting the latitudes and longitudes on an x-y axis would not convey much information. We, thus try a different approach of adding these plots on an actual map of the area recorded. #Step 1 #plotting map of New York #Constraining the plot to a bounding box of latitude and longitude coordinates corresponding to NYC. min_lat &lt;- 40.5774 max_lat &lt;- 40.9176 min_long &lt;- -74.15 max_long &lt;- -73.7004 ggplot(df, aes(x=LONGITUDE, y=LATITUDE)) + geom_point(size=0.06, color=&quot;seagreen&quot;) + scale_x_continuous(limits=c(min_long, max_long)) + scale_y_continuous(limits=c(min_lat, max_lat)) + ggtitle(&quot;Map of New York City&quot;, ) + theme(plot.title = element_text(hjust = 0.5)) #Step 2 #Plotting all accidents qmplot(x = LONGITUDE, y = LATITUDE, data = df, maptype = &quot;watercolor&quot;,darken = .2, geom = c(&quot;point&quot;,&quot;density2d&quot;), size = NUMBER.OF.PERSONS.INJURED, color = I(&quot;darkred&quot;), , alpha = I(.6), legend = NA) + scale_x_continuous(limits=c(min_long, max_long)) + scale_y_continuous(limits=c(min_lat, max_lat)) #Step 3 #Plotting by borough and contributing factor with size proportional to number of persons injured gdf &lt;- df %&gt;% group_by(BOROUGH, CONTRIBUTING.FACTOR.VEHICLE.1) %&gt;% summarize(NUMBER.OF.PERSONS.INJURED = sum(NUMBER.OF.PERSONS.INJURED), HOUR_OF_DAY = names(which(table(TIME) == max(table(TIME)))[1]), LATITUDE = mean(LATITUDE), LONGITUDE = mean(LONGITUDE)) #Step 3a #Plotting with size proportional to number of persons injured qmplot(x = LONGITUDE, y = LATITUDE, data = gdf, maptype = &quot;toner&quot;,darken=0.2, geom = c(&quot;point&quot;), size = NUMBER.OF.PERSONS.INJURED, color = I(&quot;red&quot;)) #Step 3b #Plotting with size proportional to number of persons injured and color proportional to hour of the day qmplot(x = LONGITUDE, y = LATITUDE, data = gdf, maptype = &quot;toner&quot;,darken = .2, geom = &quot;point&quot;, size = NUMBER.OF.PERSONS.INJURED, color = HOUR_OF_DAY) We have used Stamen maps as opposed to Google maps as the latter requires an API and costs money beyond a certain number of views per day. The qmplot function in Stamen maps is used to add the map background. We get the base map of New York based on the Latitudes and longitudes (Step 1). Similar to ggplot, we can add layers on the maps which enbales us with a better visualization of events occuring in a locality. Step 2 shows how the map would look if we plotted all the accidents in the raw format based solely on the latitudes and longitudes. In the next steps, we have thus grouped data on potential parameters such as Borough and Contributing Factor. Additionally, instead of plotting all data points for all latitudes and longitudes, we consider the mean latitude and longitude to give a better perspective of the number of accidents in a locality. Stamen Maps have various map backgrounds such as toner, watermark, burning, terrain etc for varied requirements. For our analysis, toner seemed to be the best fit as it highlighted the cities and cross sections which are vital for understanding vehicle collisions on the road. We use a variation of size and colour to highlight 2 parameters i.e the size of the bubble denotes the if more or less number of accidents occured in a region based on latitude and longitude. And colour denotes if there is a pattern across different time stamps. The colour gradient can be changed to accomodate for different parameter. Using maps have thus enabled us to seamlessly identify patterns in the data by accomodating for multiple parameters into one graph. Overall, it offers a better visualization of spatial data as opposed to normal graphs. "],
["world-heatmap-in-plotly.html", "Chapter 43 World Heatmap in Plotly 43.1 INTRODUCTION 43.2 DEMONSTRATION 43.3 CONCLUSION 43.4 REFERENCES", " Chapter 43 World Heatmap in Plotly Harguna Sood and Siddhanth Vinay 43.1 INTRODUCTION In an era of digitization, the datasets and it’s size is increasing on a tremendous rate, however, the capabilities of humans to go through that data still remains the same. How does one perceive the huge amount of data? Here comes the role sof data visualisation. Through this article, we wish to explain a technique via which one can create an interactive heatmap all over the world. Sounds interesting, right! There are multiple perspectives or motivation behind the application of this technique, but today we will learn about it’s motivation for a business/company. 43.2 DEMONSTRATION For the process of demonstration, we shall write a script to generate a geo_plot on a particular dataset which contains the business rankings across different metrics for all the countries in the world. Higher rankings (a low numerical value) indicate better, usually simpler, regulations for businesses and stronger protections of property rights. The following metrics are available in the dataset: Ease of doing business Starting a business Dealing with construction permits Protecting minority investors We can get an idea about how this information is important to the company. It can help the company with scalibility, profit optimization etc. So, we get the essence of the importance of visualising this information. Now, let us proceed with the implementation part: Installing and loading the required packages for the visualization: We will be using the Plotly library in R, which is used to make interactive plots. #install.packages(&quot;countrycode&quot;) #install.packages(&quot;plotly&quot;) library(countrycode) library(plotly) Loading the business_rankings csv file onto a dataframe and taking a peak at the data: rankings.df &lt;- read.csv(paste(&quot;resources/world_heatmap/business_rankings.csv&quot;, sep=&quot;&quot;)) kable(head(rankings.df)) Economy Ease.of.Doing.Business.Rank Starting.a.Business Dealing.with.Construction.Permits Protecting.Minority.Investors New Zealand 1 1 1 1 Singapore 2 6 10 1 Denmark 3 24 6 19 Hong Kong SAR, China 4 3 5 3 Korea, Rep 5 11 31 13 Norway 6 21 43 9 The package countrycode generates the required country code for each country name. This will be used by the plot_geo function to map each country’s ranking to its location on the map. Thus, we use the package to create the country_code column for our dataframe. rankings.df$code &lt;- countrycode(rankings.df$Economy,&#39;country.name&#39;,&#39;iso3c&#39;) We also create a column of the average ranking of all the different metrics to determine which countries are the best for business overall. rankings.df$avg &lt;- (rankings.df$Ease.of.Doing.Business.Rank+rankings.df$Starting.a.Business+rankings.df$Dealing.with.Construction.Permits+rankings.df$Protecting.Minority.Investors)/4 We now store different visualizaton options in variables. l contains the colour and width of the border between 2 countries. l &lt;- list(color = toRGB(&quot;grey&quot;), width = 0.5) g is used to specify the map options such as whether to show the coast line or not. g &lt;- list( showframe = TRUE, showcoastlines = TRUE, projection = list(type = &#39;Mercator&#39;)) Scale1 contains the options for the colorbar and legend. scale1 &lt;- list( visible=TRUE, showlegend=FALSE, title=&quot;Rank&quot;, reversescale = TRUE ) We now create the code to plot the rankings for the different economies on a map with a dropdown to view the average rankings of the countries, along with their rankings for the different metrics. For this, we use the plot_geo function of plotly to plot the data. For each metric, we create a separate trace which to which we pass the country names along with their country codes, the map plot options (variable named l), and the column for corresponding to that metric is passed to the colorbar. We also specify whether the particular trace is shown by default or not. These traces are then passed to a dropdown where we create the dropdown list - one button per metric, and for each button, we specify the corresponding trace that needs to be displayed. p &lt;- plot_geo(rankings.df) %&gt;% #Code for plotting the maps for the different attributes. add_trace( z = ~avg, color = ~avg,name=&#39;Average of all Rankings&#39;, text = ~Economy, locations = ~code, marker = list(line = l),colorbar=list(title=&#39;Rank&#39;),visible=TRUE ) %&gt;% add_trace( z = ~Ease.of.Doing.Business.Rank, color = ~Ease.of.Doing.Business.Rank,name=&#39;Ease of doing Business&#39;, text = ~Economy, locations = ~code, marker = list(line = l),colorbar=list(title=&#39;Rank&#39;),visible=FALSE ) %&gt;% add_trace( z = ~Starting.a.Business, color = ~Starting.a.Business,name=&#39;Starting a Business&#39;, text = ~Economy, locations = ~code, marker = list(line = l),colorbar=list(title=&#39;Rank&#39;),visible=FALSE ) %&gt;% add_trace( z = ~Dealing.with.Construction.Permits, color = ~Dealing.with.Construction.Permits,name=&#39;Dealing with Construction Permits&#39;, text = ~Economy, locations = ~code, marker = list(line = l),colorbar=list(title=&#39;Rank&#39;),visible=FALSE ) %&gt;% add_trace( z = ~Protecting.Minority.Investors, color = ~Protecting.Minority.Investors,name=&#39;Protecting Minority Investors&#39;, text = ~Economy, locations = ~code, marker = list(line = l),colorbar=list(title=&#39;Rank&#39;),visible=FALSE ) %&gt;% colorbar(title = &#39;Rank&#39;) %&gt;% #Code for the dropdown. layout( title = &quot;Various Business Rankings (Hover over a country for its Rank)&quot;, geo=g, updatemenus = list( list( buttons = list( list(method = &quot;restyle&quot;, args = list(&quot;visible&quot;,list(TRUE, FALSE, FALSE, FALSE, FALSE)), label = &quot;Average of All Rankings&quot;), list(method = &quot;restyle&quot;, args = list(&quot;visible&quot;, list(FALSE, TRUE, FALSE, FALSE, FALSE)), showscale=scale1, label = &quot;Ease of Doing Business&quot;), list(method = &quot;restyle&quot;, args = list(&quot;visible&quot;, list(FALSE, FALSE, TRUE, FALSE, FALSE)), showscale=scale1, label = &quot;Starting a Business&quot;), list(method = &quot;restyle&quot;, args = list(&quot;visible&quot;, list(FALSE, FALSE, FALSE, TRUE, FALSE)), showscale=scale1, label = &quot;Dealing With Construction Permits&quot;), list(method = &quot;restyle&quot;, args = list(&quot;visible&quot;, list(FALSE, FALSE, FALSE, FALSE, TRUE)), showscale=scale1, label = &quot;Protecting Minority Investors&quot;) )) )) We now view the plot. It’s interactive, zoom in to view each country and hover the cursor over to get stats for that country! p 43.3 CONCLUSION There are various ways to tweek this code and use for different purposes. It can be even taken down to lower levels, for appropriate usage, like, it can be used by government to keep a track of poverty in different states, by municipality for number of complaints in different zones of the city. Apart from government, it can be used by researchers, especially geography researchers, to analyse trends in different parts of the region. The list goes on and on. Overall, visualisation can help with analysing of problems and provide a deeper perspective to it by highlighting the trends, which would have been harder to identify otherwise and using the right visualising technique is an important factor to it. Happy visualising! 43.4 REFERENCES To learn more about the libraries visit - Plotly: https://plot.ly/r/ Plot_geo: https://www.rdocumentation.org/packages/plotly/versions/4.9.0/topics/plot_geo Countrycode: https://cran.r-project.org/web/packages/countrycode/countrycode.pdf "],
["spatial-data-links.html", "Chapter 44 Spatial data links 44.1 CartoDB (video) 44.2 Leaflet", " Chapter 44 Spatial data links 44.1 CartoDB (video) Luis Lu and Timothy Huang This tutorial gives a brief overview on getting started with CartoDB, a powerful cloud computing tool that provides geospatial analysis and mapping tools. In this tutorial, we will go over the steps of getting set up on CartoDB, uploading your first dataset, creating your first map visualization, and exploring a few of Carto’s provided geospatial data analysis tools. Link to tutorial video: https://www.youtube.com/watch?v=GxRRXWTMMe8&amp;feature=youtu.be CartoDB 44.2 Leaflet Di Ye and Qiaoge Zhu This project creates a cheatsheet on leaflet package in R. link: https://drive.google.com/file/d/1NgiAYy7kaoheWbCmJRmu9NxI6Ag7s-VT/view?usp=sharing "],
["time-series-cheatsheet.html", "Chapter 45 Time Series Cheatsheet", " Chapter 45 Time Series Cheatsheet Yunjun Xia and Shuyu Huang \\(\\color{blue}{\\textbf{STAT 5702 Community Contribution }}\\) In this project, we have made an R studio cheatsheet about Time Series. There are two related files for this Time Series Cheatsheet. One is a pdf file, and the other one is a keynote file. The two files are uploaded in Yunjun Xia’s personal repository on Github with links as follows: https://github.com/xiayunj/time_series_cheatsheet/blob/master/TimeSeriesCheatsheet.pdf https://github.com/xiayunj/time_series_cheatsheet/blob/master/TimeSeriesCheatsheet.key \\(\\color{blue}{\\textbf{Cheatsheet Description}}\\) In this cheatsheet, we include some commonly used functions of R when dealing with Time Series problems. The first chunck shows three ways to plot a sample time series data. The second chunck describes the definition of Autogression and Moving Average. Then it gives the function to simulate an ARMA process. Then we move on to R functions to deal with filters for time series. In this chunck, we include two kinds of filters which are linear filter and differencing filter. The forth chunk is about auto-correlation and partial auto-correlation of time series. The fifth chunck is about estimating parameters of an ARMA model. Then, the last chunck is about time series forecasting/prediction. Here we also include how to plot the predicted values and the confidence interval. Also, except functions and codes, we include some plots and example outputs for our functions to make this cheatsheet more interpretable. "],
["tutorial-for-multivariable-linear-regression.html", "Chapter 46 Tutorial for Multivariable Linear Regression 46.1 Motivation 46.2 Connection with Single Variable Regression 46.3 Collinearity and Paradox 46.4 Solution Path 46.5 Stepwise Model Selection 46.6 Model Verification", " Chapter 46 Tutorial for Multivariable Linear Regression Yuge Shen, Shengqing Xia 46.1 Motivation We use regression models to describe how one or more factors influence certain response variables. We already learnt how to construct a single variable regression model, but in practice, one often needs to predict or control future responses base on multiple variables or to gain understanding between them. We will make use of basic mathematical ideas and introduce the data visualization, model selection and construction processes in R. For the sake of this tutorial, we use the housing price data kc_house_data.csv from UCI Machine Learning Repository. Notice we will only use this dataset for illustration only, and not going to discuss the underlying mechanism. 46.2 Connection with Single Variable Regression We are familiar with Single Variable Regression \\[y_i = \\beta_0 + \\beta x_i + \\epsilon_i\\] where \\(y_i\\) is the response variable. Similar with Single Variable case, model for multivariable case has quite the same structure only with more variables to consider. We can express the model concept in matrix form: \\[\\pmatrix{Y_1\\\\Y_2\\\\...\\\\Y_n} = \\pmatrix{1&amp;X_{11}&amp;X_{12}&amp;...&amp;X_{1d}\\\\1&amp;X_{21}&amp;X_{22}&amp;...&amp;X_{2d}\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;&amp;\\vdots\\\\1&amp;X_{n1}&amp;X_{n2}&amp;...&amp;X_{nd}}\\pmatrix{\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_d} + \\pmatrix{\\epsilon_1\\\\\\epsilon_2\\\\\\vdots\\\\\\epsilon_n}.\\] Below is an example of model visualization for single and multiple resgressions. The data we are using consists of prices of houses in King County, Washington from sales between May 2014 to May 2015. We found the data on Kaggle website. There are 21,613 observations and 21 variables. There are 19 house features plus the price of the house and id as column fields. ## id date price bedrooms bathrooms sqft_living sqft_lot ## 1 7129300520 20141013T000000 221900 3 1.00 1180 5650 ## 2 6414100192 20141209T000000 538000 3 2.25 2570 7242 ## 3 5631500400 20150225T000000 180000 2 1.00 770 10000 ## 4 2487200875 20141209T000000 604000 4 3.00 1960 5000 ## 5 1954400510 20150218T000000 510000 3 2.00 1680 8080 ## 6 7237550310 20140512T000000 1225000 4 4.50 5420 101930 ## floors waterfront view condition grade sqft_above sqft_basement yr_built ## 1 1 0 0 3 7 1180 0 1955 ## 2 2 0 0 3 7 2170 400 1951 ## 3 1 0 0 3 6 770 0 1933 ## 4 1 0 0 5 7 1050 910 1965 ## 5 1 0 0 3 8 1680 0 1987 ## 6 1 0 0 3 11 3890 1530 2001 ## yr_renovated zipcode lat long sqft_living15 sqft_lot15 ## 1 0 98178 47.5112 -122.257 1340 5650 ## 2 1991 98125 47.7210 -122.319 1690 7639 ## 3 0 98028 47.7379 -122.233 2720 8062 ## 4 0 98136 47.5208 -122.393 1360 5000 ## 5 0 98074 47.6168 -122.045 1800 7503 ## 6 0 98053 47.6561 -122.005 4760 101930 library(scatterplot3d) library(dplyr) data2 &lt;- data %&gt;% as.data.frame() %&gt;% select(sqft_above,sqft_basement,price) p1 &lt;- scatterplot3d(data2, pch = 16, color = &quot;steelblue&quot;, main = &quot;Price vs. above area and basement area&quot;, xlab = &quot;above area&quot;, ylab = &quot;basement area&quot;, zlab = &quot;price&quot;) fit = lm(data2$price~data2$sqft_above+data2$sqft_basement) p1$plane3d(fit) fit2d = lm(data$price~data$sqft_living) plot(data$sqft_living, data$price, pch = 16, col=&quot;steelblue&quot;) abline(fit2d, col = &quot;orange&quot;) 46.3 Collinearity and Paradox One important assumption for the multiple regression model is that we assume independence of all features. So the model would be inaccurate if some of the X values have strong correlation. If this happens, it is called collinearity. In the case of house pricing, it is very possible that the living area is strongly correlated with the area above so if we add both of these 2 factors into the model, the collinearity problem could occur. In order to make accurate decision, we need to have some sort of visualization about the correlation among all x values. The most generally used method is pair matrix plot. By drawing the pairwise correlation of all variables, we can see underlying collinearity pattern and omit some variables that will cause this problem. To show the importance of detecting collinearity, we firstly introduce a paradox: library(ggplot2) type = 1:8 type[1:4]=&quot;type1&quot; type[5:8]=&quot;type2&quot; x=c(1,2,3,4,8,9,10,11) y = c(6,7,8,9,1,2,3,4) model = lm(y~x) df &lt;- data_frame(type,x,y) g &lt;- ggplot(df,aes(x, y)) + geom_line(aes(color = type)) + geom_point()+xlab(&quot;&quot;) + ylab(&quot;&quot;) g+geom_abline(intercept = coef(model)[1],slope = coef(model)[2]) We can see althought the two groups all seem to have positive slope, while because of the multicollinearity, the real regression line has a negative slope, and this is called a paradox. If your goal is to understand how the various X variables impact Y, then multicollinearity is a big problem to the model selection. To better detect the collinearity and select model, we introduce pair matrix firstly. In the housing price dataset, we can choose some variables that we are interested in putting into the model and analyze their patterns. pairs(data = data, price~sqft_living+sqft_above+sqft_basement+bathrooms) 46.4 Solution Path Below is an example of a solution path base on lasso. X = cbind(sqft_living,sqft_above,yr_renovated, zipcode, sqft_living15) library(lars) lasso &lt;- lars(X,price) plot(lasso) Initially all of the coefficients are zero, because when \\[λ = \\infty,log(-λ)=0\\] the penalty is too large, none of the variables can have influence on the regression model. When \\[λ = 0,log(-λ)=1\\] means taht there is no penalty in lasso, so the coefficients are just least square estimators. When a coordinate path (in this case sqft_living) firstly leaves the boundary (drawn in black), the slope of it won’t change until another paths leave the boundary, and repeat the progress, until all coordinates are above the boundary. More specifically, we only include 2 variables so the pattern is more clear and easy to interprete. X = cbind(sqft_living,sqft_above) library(lars) lasso &lt;- lars(X,price) plot(lasso) legend(&quot;topleft&quot;,1, 2, legend = c(&quot;living&quot;, &quot;above&quot;), fill = c(&quot;black&quot;,&quot;red&quot;)) Based on the solution path plot, we can see that when lambda is small, the coefficient for sqft_living is extreme large and it appears first as lambda decreases, while for the sqft_above, it appears when lambda is small and its standardized coefficient is small and close to 0, which suggests that this variable should not be included. Moreover, we can see that when sqft_above appears(becomes larger than 0) in the solution path, the coefficient of sqft_living changes sharply and standardized coefficient of sqft_above decreases, which suggests that there is collinearity between the two variables. Based on the pair matrix we confirm our ideas. 46.5 Stepwise Model Selection In order to choose which predictors to include in our model, we need to test their relative importance. There are 3 major ways to complete the test. 46.5.0.1 Forward selection We start with an empty model, which means no predictors in the model. Then we iteratively add the most affective predictor at each step. In the end, the process terminates when the improvement is no longer statistically significant. 46.5.0.2 Backward selection (or backward elimination) We start with a full model, which means, to include all predictors in the model. Then we repeatedly remove the least contributive predictors. The process terminates when all predictors are statistically significant 46.5.0.3 Stepwise selection (or sequential replacement) A combination of forward and backward selections. You start with no predictors, then sequentially add the most contributive predictors (like forward selection). After adding each new variable, remove any variables that no longer provide an improvement in the model fit (like backward selection). R has a function that perform this process for us. As for this example, we start from an empty model “fitNone” and cast out the stepwise selection fitNone = lm(price~1) step(fitNone, scope = ~sqft_living+sqft_above+sqft_basement+bathrooms+ sqft_lot+ floors+ waterfront+ view+condition+ grade+ yr_built+ yr_renovated+ sqft_living15+ sqft_lot15, direction = &quot;both&quot;) ## Start: AIC=553875.8 ## price ~ 1 ## ## Df Sum of Sq RSS AIC ## + sqft_living 1 1.4356e+15 1.4773e+15 539204 ## + grade 1 1.2976e+15 1.6153e+15 541134 ## + sqft_above 1 1.0682e+15 1.8447e+15 544004 ## + sqft_living15 1 9.9816e+14 1.9148e+15 544810 ## + bathrooms 1 8.0329e+14 2.1096e+15 546904 ## + view 1 4.5978e+14 2.4531e+15 550165 ## + sqft_basement 1 3.0544e+14 2.6075e+15 551484 ## + waterfront 1 2.0668e+14 2.7062e+15 552287 ## + floors 1 1.9209e+14 2.7208e+15 552403 ## + yr_renovated 1 4.6564e+13 2.8664e+15 553529 ## + sqft_lot 1 2.3417e+13 2.8895e+15 553703 ## + sqft_lot15 1 1.9801e+13 2.8931e+15 553730 ## + yr_built 1 8.4977e+12 2.9044e+15 553815 ## + condition 1 3.8514e+12 2.9091e+15 553849 ## &lt;none&gt; 2.9129e+15 553876 ## ## Step: AIC=539203.5 ## price ~ sqft_living ## ## Df Sum of Sq RSS AIC ## + view 1 1.2362e+14 1.3537e+15 537317 ## + grade 1 1.2132e+14 1.3560e+15 537353 ## + waterfront 1 1.1024e+14 1.3670e+15 537529 ## + yr_built 1 9.2854e+13 1.3844e+15 537802 ## + yr_renovated 1 2.2405e+13 1.4549e+15 538875 ## + sqft_living15 1 2.0109e+13 1.4572e+15 538909 ## + condition 1 1.7605e+13 1.4597e+15 538946 ## + sqft_lot15 1 6.4407e+12 1.4708e+15 539111 ## + sqft_lot 1 3.0113e+12 1.4743e+15 539161 ## + sqft_above 1 1.2165e+12 1.4761e+15 539188 ## + sqft_basement 1 1.2165e+12 1.4761e+15 539188 ## + floors 1 2.2991e+11 1.4770e+15 539202 ## + bathrooms 1 1.4719e+11 1.4771e+15 539203 ## &lt;none&gt; 1.4773e+15 539204 ## - sqft_living 1 1.4356e+15 2.9129e+15 553876 ## ## Step: AIC=537316.8 ## price ~ sqft_living + view ## ## Df Sum of Sq RSS AIC ## + grade 1 1.0850e+14 1.2452e+15 535513 ## + yr_built 1 6.3611e+13 1.2900e+15 536278 ## + waterfront 1 4.4742e+13 1.3089e+15 536592 ## + yr_renovated 1 1.3877e+13 1.3398e+15 537096 ## + condition 1 1.2074e+13 1.3416e+15 537125 ## + sqft_living15 1 1.1203e+13 1.3425e+15 537139 ## + sqft_lot15 1 7.7245e+12 1.3459e+15 537195 ## + sqft_lot 1 4.1477e+12 1.3495e+15 537252 ## + floors 1 1.8712e+12 1.3518e+15 537289 ## + sqft_above 1 7.8057e+11 1.3529e+15 537306 ## + sqft_basement 1 7.8057e+11 1.3529e+15 537306 ## &lt;none&gt; 1.3537e+15 537317 ## + bathrooms 1 8.9455e+09 1.3536e+15 537319 ## - view 1 1.2362e+14 1.4773e+15 539204 ## - sqft_living 1 1.0995e+15 2.4531e+15 550165 ## ## Step: AIC=535513 ## price ~ sqft_living + view + grade ## ## Df Sum of Sq RSS AIC ## + yr_built 1 1.5271e+14 1.0924e+15 532687 ## + waterfront 1 4.7212e+13 1.1979e+15 534680 ## + condition 1 2.6995e+13 1.2182e+15 535041 ## + yr_renovated 1 1.7932e+13 1.2272e+15 535202 ## + sqft_lot15 1 5.9098e+12 1.2392e+15 535412 ## + sqft_above 1 5.2796e+12 1.2399e+15 535423 ## + sqft_basement 1 5.2796e+12 1.2399e+15 535423 ## + bathrooms 1 4.7539e+12 1.2404e+15 535432 ## + floors 1 4.1610e+12 1.2410e+15 535443 ## + sqft_lot 1 2.9713e+12 1.2422e+15 535463 ## &lt;none&gt; 1.2452e+15 535513 ## + sqft_living15 1 8.9141e+08 1.2452e+15 535515 ## - grade 1 1.0850e+14 1.3537e+15 537317 ## - view 1 1.1080e+14 1.3560e+15 537353 ## - sqft_living 1 2.0630e+14 1.4515e+15 538824 ## ## Step: AIC=532687.1 ## price ~ sqft_living + view + grade + yr_built ## ## Df Sum of Sq RSS AIC ## + waterfront 1 4.8129e+13 1.0443e+15 531715 ## + bathrooms 1 7.7749e+12 1.0847e+15 532535 ## + floors 1 5.7639e+12 1.0867e+15 532575 ## + sqft_lot15 1 4.2071e+12 1.0882e+15 532606 ## + sqft_lot 1 2.4210e+12 1.0900e+15 532641 ## + yr_renovated 1 1.5703e+12 1.0909e+15 532658 ## + condition 1 1.5232e+12 1.0909e+15 532659 ## + sqft_living15 1 4.6523e+11 1.0920e+15 532680 ## + sqft_above 1 1.7064e+11 1.0923e+15 532686 ## + sqft_basement 1 1.7064e+11 1.0923e+15 532686 ## &lt;none&gt; 1.0924e+15 532687 ## - view 1 6.4285e+13 1.1567e+15 533921 ## - yr_built 1 1.5271e+14 1.2452e+15 535513 ## - grade 1 1.9760e+14 1.2900e+15 536278 ## - sqft_living 1 2.0225e+14 1.2947e+15 536356 ## ## Step: AIC=531715.3 ## price ~ sqft_living + view + grade + yr_built + waterfront ## ## Df Sum of Sq RSS AIC ## + bathrooms 1 7.9640e+12 1.0364e+15 531552 ## + floors 1 4.9620e+12 1.0394e+15 531614 ## + sqft_lot15 1 4.2941e+12 1.0400e+15 531628 ## + sqft_lot 1 2.2513e+12 1.0421e+15 531671 ## + condition 1 1.5965e+12 1.0427e+15 531684 ## + sqft_living15 1 7.5855e+11 1.0436e+15 531702 ## + yr_renovated 1 7.1666e+11 1.0436e+15 531702 ## &lt;none&gt; 1.0443e+15 531715 ## + sqft_above 1 1.9360e+10 1.0443e+15 531717 ## + sqft_basement 1 1.9360e+10 1.0443e+15 531717 ## - view 1 2.2298e+13 1.0666e+15 532170 ## - waterfront 1 4.8129e+13 1.0924e+15 532687 ## - yr_built 1 1.5363e+14 1.1979e+15 534680 ## - grade 1 2.0110e+14 1.2454e+15 535519 ## - sqft_living 1 2.0126e+14 1.2456e+15 535522 ## ## Step: AIC=531551.8 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms ## ## Df Sum of Sq RSS AIC ## + sqft_lot15 1 3.3139e+12 1.0330e+15 531485 ## + floors 1 2.6013e+12 1.0337e+15 531500 ## + sqft_lot 1 1.7011e+12 1.0346e+15 531518 ## + sqft_living15 1 1.3617e+12 1.0350e+15 531525 ## + condition 1 1.3220e+12 1.0350e+15 531526 ## + yr_renovated 1 2.1381e+11 1.0361e+15 531549 ## + sqft_above 1 1.4759e+11 1.0362e+15 531551 ## + sqft_basement 1 1.4759e+11 1.0362e+15 531551 ## &lt;none&gt; 1.0364e+15 531552 ## - bathrooms 1 7.9640e+12 1.0443e+15 531715 ## - view 1 2.1785e+13 1.0581e+15 531999 ## - waterfront 1 4.8318e+13 1.0847e+15 532535 ## - sqft_living 1 1.0668e+14 1.1430e+15 533667 ## - yr_built 1 1.5691e+14 1.1933e+15 534597 ## - grade 1 1.9379e+14 1.2301e+15 535255 ## ## Step: AIC=531484.6 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms + sqft_lot15 ## ## Df Sum of Sq RSS AIC ## + floors 1 2.2075e+12 1.0308e+15 531440 ## + sqft_living15 1 1.6952e+12 1.0313e+15 531451 ## + condition 1 1.3870e+12 1.0316e+15 531458 ## + sqft_above 1 2.7772e+11 1.0328e+15 531481 ## + sqft_basement 1 2.7772e+11 1.0328e+15 531481 ## + yr_renovated 1 2.3857e+11 1.0328e+15 531482 ## &lt;none&gt; 1.0330e+15 531485 ## + sqft_lot 1 5.6626e+08 1.0330e+15 531487 ## - sqft_lot15 1 3.3139e+12 1.0364e+15 531552 ## - bathrooms 1 6.9837e+12 1.0400e+15 531628 ## - view 1 2.2236e+13 1.0553e+15 531943 ## - waterfront 1 4.8384e+13 1.0814e+15 532472 ## - sqft_living 1 1.0999e+14 1.1430e+15 533669 ## - yr_built 1 1.5337e+14 1.1864e+15 534474 ## - grade 1 1.9178e+14 1.2248e+15 535163 ## ## Step: AIC=531440.4 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms + sqft_lot15 + floors ## ## Df Sum of Sq RSS AIC ## + sqft_living15 1 2.0282e+12 1.0288e+15 531400 ## + condition 1 1.9255e+12 1.0289e+15 531402 ## + yr_renovated 1 1.4454e+11 1.0307e+15 531439 ## &lt;none&gt; 1.0308e+15 531440 ## + sqft_above 1 8.1986e+09 1.0308e+15 531442 ## + sqft_basement 1 8.1986e+09 1.0308e+15 531442 ## + sqft_lot 1 6.7056e+07 1.0308e+15 531442 ## - floors 1 2.2075e+12 1.0330e+15 531485 ## - sqft_lot15 1 2.9201e+12 1.0337e+15 531500 ## - bathrooms 1 5.0000e+12 1.0358e+15 531543 ## - view 1 2.2866e+13 1.0537e+15 531913 ## - waterfront 1 4.7787e+13 1.0786e+15 532418 ## - sqft_living 1 1.1194e+14 1.1428e+15 533667 ## - yr_built 1 1.5261e+14 1.1834e+15 534422 ## - grade 1 1.7753e+14 1.2084e+15 534873 ## ## Step: AIC=531399.8 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms + sqft_lot15 + floors + sqft_living15 ## ## Df Sum of Sq RSS AIC ## + condition 1 2.0259e+12 1.0268e+15 531359 ## + yr_renovated 1 1.8644e+11 1.0286e+15 531398 ## + sqft_above 1 1.4099e+11 1.0287e+15 531399 ## + sqft_basement 1 1.4099e+11 1.0287e+15 531399 ## &lt;none&gt; 1.0288e+15 531400 ## + sqft_lot 1 2.7063e+09 1.0288e+15 531402 ## - sqft_living15 1 2.0282e+12 1.0308e+15 531440 ## - floors 1 2.5405e+12 1.0313e+15 531451 ## - sqft_lot15 1 3.2400e+12 1.0320e+15 531466 ## - bathrooms 1 5.4649e+12 1.0343e+15 531512 ## - view 1 2.1315e+13 1.0501e+15 531841 ## - waterfront 1 4.8268e+13 1.0771e+15 532389 ## - sqft_living 1 8.0960e+13 1.1098e+15 533035 ## - grade 1 1.5032e+14 1.1791e+15 534345 ## - yr_built 1 1.5461e+14 1.1834e+15 534424 ## ## Step: AIC=531359.2 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms + sqft_lot15 + floors + sqft_living15 + condition ## ## Df Sum of Sq RSS AIC ## + yr_renovated 1 4.6126e+11 1.0263e+15 531352 ## &lt;none&gt; 1.0268e+15 531359 ## + sqft_above 1 8.3048e+10 1.0267e+15 531359 ## + sqft_basement 1 8.3048e+10 1.0267e+15 531359 ## + sqft_lot 1 5.4527e+09 1.0268e+15 531361 ## - condition 1 2.0259e+12 1.0288e+15 531400 ## - sqft_living15 1 2.1286e+12 1.0289e+15 531402 ## - floors 1 3.1360e+12 1.0299e+15 531423 ## - sqft_lot15 1 3.2777e+12 1.0301e+15 531426 ## - bathrooms 1 4.9874e+12 1.0318e+15 531462 ## - view 1 2.1197e+13 1.0480e+15 531799 ## - waterfront 1 4.8290e+13 1.0751e+15 532351 ## - sqft_living 1 8.0158e+13 1.1069e+15 532982 ## - yr_built 1 1.3349e+14 1.1603e+15 533999 ## - grade 1 1.5061e+14 1.1774e+15 534315 ## ## Step: AIC=531351.5 ## price ~ sqft_living + view + grade + yr_built + waterfront + ## bathrooms + sqft_lot15 + floors + sqft_living15 + condition + ## yr_renovated ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.0263e+15 531352 ## + sqft_above 1 8.6480e+10 1.0262e+15 531352 ## + sqft_basement 1 8.6480e+10 1.0262e+15 531352 ## + sqft_lot 1 6.1742e+09 1.0263e+15 531353 ## - yr_renovated 1 4.6126e+11 1.0268e+15 531359 ## - sqft_living15 1 2.2065e+12 1.0285e+15 531396 ## - condition 1 2.3007e+12 1.0286e+15 531398 ## - floors 1 3.0139e+12 1.0293e+15 531413 ## - sqft_lot15 1 3.3340e+12 1.0296e+15 531420 ## - bathrooms 1 4.5250e+12 1.0308e+15 531445 ## - view 1 2.1000e+13 1.0473e+15 531787 ## - waterfront 1 4.7584e+13 1.0739e+15 532329 ## - sqft_living 1 8.0206e+13 1.1065e+15 532976 ## - yr_built 1 1.1544e+14 1.1418e+15 533653 ## - grade 1 1.5006e+14 1.1764e+15 534299 ## ## Call: ## lm(formula = price ~ sqft_living + view + grade + yr_built + ## waterfront + bathrooms + sqft_lot15 + floors + sqft_living15 + ## condition + yr_renovated) ## ## Coefficients: ## (Intercept) sqft_living view grade yr_built ## 5.976e+06 1.392e+02 4.723e+04 1.256e+05 -3.509e+03 ## waterfront bathrooms sqft_lot15 floors sqft_living15 ## 5.938e+05 3.323e+04 -4.677e-01 2.757e+04 2.424e+01 ## condition yr_renovated ## 1.747e+04 1.229e+01 From the output we can easily see the variable selection process step by step and the result is yield at the end. We omit the sqft_above, sqft_basement and sqft_lot variable and keep the rest of them. 46.6 Model Verification Now if we would like to make sure our model is working well, we need to test if it has a good fit with the original data. Then a diagnostic plot becomes handy. fitTry = lm(formula = price ~ sqft_living + view + grade + yr_built + waterfront + bathrooms + sqft_lot15 + floors + sqft_living15 + condition + yr_renovated) plot(fitTry) Oops, this might not seem to be a well-fit model to the data. The 1st and 3rd plot show that the residuals are not randomly distributed with obvious trends. The 2nd plot, the QQ plot shows non-normalily of the data points, which violates the assumption of the model. Below is a diagnostic plot for a better model on this dataset. Diagnostic Plot for a Potentially Good Model 46.6.1 Outliers and Leverage Plot for detecting outliers. Studentized deleted residuals (or externally studentized residuals) is the deleted residual divided by its estimated standard deviation. Now we firstly introduce \\[di\\]: delete ith observations at a time. refit the regression model on remaining observations, calculate the new fitted value \\[\\hat{yi_i}\\]. examine how much all of the fitted values change when the ith observation is deleted\\[di = yi - \\hat{yi_i}\\]. Then we have \\[StudRes_i = \\frac{di}{SE(di)}\\] Studentized residuals are going to be more effective for detecting outlying Y observations than standardized residuals. If an observation has an externally studentized residual that is larger than 3 (in absolute value) we can call it an outlier. library(olsrr) short_living = sqft_living[4000:4500] short_above = sqft_above[4000:4500] model = lm(short_living~short_above) ols_plot_resid_stud(model) ols_plot_resid_stand(model) Based on the two plots, we can see studentized residuals find less outliers than the standardized residuls and only detect extreme cases. Next, we want to discuss leverage which only depends on X variables. For the simple linear regression case: \\[H_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\] For the multiple case: \\[\\sum_{i=1}^{n}H_{ii} = Trace(H) = Tr(X(X^TX)^{-1}X^T)\\] \\[H_{ii} = X_i(X^TX)^{-1}X_i\\] if\\[H_{ii} &gt; \\frac{2\\sum_{i=1}^{n}H_{ii}}{n}\\], we say the ith observation is a huge leverage. Graph for detecting influential observations. ols_plot_resid_lev(model) "],
["keras-package-tutorial.html", "Chapter 47 Keras Package Tutorial 47.1 Installation 47.2 Obtaining a Dataset 47.3 Building a model", " Chapter 47 Keras Package Tutorial Patrick Stanton Deep Learning has become a cornerstone of modern Machine Learning. The ability to develop and train neural networks is a necessity for any Data Scientist. One of the most prevalent packages that enables us to do so is called Keras. This tutorial will show you how to use this package in R. 47.1 Installation Keras is high-level API that runs atop a more powerful package called Tensor Flow. The installation is pretty straightforward. We simply install the package using CRAN, load into library, and run the installation function. install.packages(&quot;keras&quot;) library(keras) install_keras() ## Creating virtual environment &#39;~/.virtualenvs/r-reticulate&#39; ... ## Using python: /usr/bin/python2.7 ## Using virtual environment &#39;~/.virtualenvs/r-reticulate&#39; ... ## ## Installation complete. 47.2 Obtaining a Dataset A wonderful thing about keras is that it comes with several datasets on it including CIFAR10, MNIST, and many more. For our toy example, we will use the 1970s Boston housing survey dataset. This dataset contains 13 variables regarding various features of 506 neighborhoods in Boston, including crime rates, retail presence, and many others. Using these attributes, we are going to attempt to building a model that predicts the median home value of the neighborhood. The dataset conveniently comes divided into training and testing data. Thus, all we need to do is load them into variables. boston &lt;- dataset_boston_housing() train_x &lt;- boston$train$x train_y &lt;- boston$train$y test_x &lt;- boston$test$x test_y &lt;- boston$test$y 47.3 Building a model We will now design a simple sequential model architecture of three dense layers with 100, 25, and 1 neurons, respectively. For our activation function, we will use relu. This is an extraordinarily popular activation function as it maps non-linear trends very well. Other popular activation function include hyperbolic tangent and sigmoid. This is obviously much too complex of an architecture for this dataset. However, we are merely interested in showing interesting model specifications. Notice that this model takes in a vector with a length of 13 and outputs a scalar value. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 100, activation = &quot;relu&quot;, input_shape = c(13)) %&gt;% layer_dense(units = 25, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 1, activation = &quot;relu&quot;) Now that we have designed our architecture, we must compile the model. In this function, we define the loss function and optimizer for training. Since we are mapping to a single continuous value, mean squared error is a good choice for the loss function. If this were a classification problem, we would probabily use the categorical cross-entropy loss function, or some other more fitting loss function. For our optimizer, we will choose adam, which is the most commonly used optimizer in industry. This code establishes several hyperparameters of our model and prepares it to be trained. compile(model, loss = &quot;mse&quot;, optimizer = optimizer_adam()) Now, the real fun. In this code section, we get to see our model in action. For the purposes of this tutorial, we are going to train on the entire training set. Ordinarily, we would partition off a section of the training set for validation purposes. Instead, we will use the testing set as our validation set. The fit function allows us to select how many times we would like backpropagate the errors through the network as well as the batch size of training examples being fed into the network. The fit function returns the loss of both the training and validation set after each iteration. We will store these values in the history variable for later purposes. history &lt;- fit(model, train_x, train_y, epochs=50, batch_size = 100, validation_data = list(test_x, test_y)) Suppose we would now like to visualize the training and validation losses over the epochs. We simply use the plot function on the history variable. This provides valuable information regarding the fit of our model. plot(history) Congratulations, you have just trained a neural network. As you can see, we have achieved pretty good mean squared loss for both partitions of the dataset. Hence, we have a fairly I hope this tutorial has taught you some of the interesting things you can do with the keras R package. "],
["time-series-modeling-with-arima-in-r.html", "Chapter 48 Time Series Modeling with ARIMA in R 48.1 1. Visualize the time series 48.2 2. Stationarize the Time Series 48.3 3. ACF/PACF 48.4 4. Build the ARIMA Model 48.5 5. Make Predictions 48.6 References/Additional Resources", " Chapter 48 Time Series Modeling with ARIMA in R William Yu This document will give a brief introduction to time series modeling with ARIMA in R. A time series is a set of data points that are indexed by time order. Time series modeling is an especially important topic in data analytics and data science because of its important applications towards various topics. This includes predicting the next day price of a stock, or patterns in the weather. An important concept in time series modeling is ARIMA, or Auto-Regressive Integrated Moving Average. ARIMA is the combination of two models, the auto-regressive and the moving average models. An auto regressive AR(p) component refers to the use of past values in the regression equation for the series Y. The auto-regressive parameter p specifies the number of lags, or past values, to be used in the model. For example, AR(2) is represented as \\[Y_t = c + \\phi_1y_{t-1} + \\phi_2 y_{t-2}+ e_t\\] where φ1, φ2 are parameters for the model. The moving average nature of the model is represented by the “q” value, which is the number of lagged values of the error term. A moving average MA(q) component represents the error of the model as a combination of previous error terms et. The order q determines the number of terms to include in the model \\[Y_t = c + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} +...+ \\theta_q e_{t-q}+ e_t\\] Together, with the differencing variable d, which is used to remove the trend and convert a non-stationary time series to a stationary one, these three parameters define the ARIMA model. Thus, ARIMA is specified by three order parameters: (p, d, q). ARIMA modeling boils down to five parts: Visualize the time series Stationarize the time series Plot ACF/PACF and find optimal parameters Build the ARIMA model Make predictions. This document will provide information for all five. Let’s get started. 48.1 1. Visualize the time series We’ll attempt to predict stock returns by using ARIMA. We’ll be using some financial data from tidyquant: # get historical data for single stock. e.g. google library(tidyquant) jnj = tq_get(&quot;JNJ&quot;, get=&quot;stock.prices&quot;, from=&quot;1997-01-01&quot;) %&gt;% tq_transmute(mutate_fun=to.period,period=&quot;months&quot;) Let’s say we are primarily interested in the closing prices of our stock: library(ggplot2) # showing monthly return for single stock ggplot(jnj, aes(date, close)) + geom_line() We are interested in predicting the returns of JNJ. We compute the log difference of the closing price to stationarize the time series. More on this in the next section. plot(diff(log(jnj$close)),type=&#39;l&#39;, main=&#39;log returns plot&#39;) 48.2 2. Stationarize the Time Series In order to perform any successive modeling on our time series, our time series must be stationary: that is, the mean, variance, and covariance of the series should all be constant with time. Now, there are many reasons as to why we must have a stationary time series, but probably the most important fact as to why we need this is because we literally cannot model a time series any other way; if the mean, variance and covariance vary with time, how will we actually be able to estimate our desired parameter? Returns look stationary in the plot above. Let’s double check with the Dickey Fuller Test of Stationarity: library(tseries) adf.test(diff(log(jnj$close)), alternative=&quot;stationary&quot;, k=0) ## ## Augmented Dickey-Fuller Test ## ## data: diff(log(jnj$close)) ## Dickey-Fuller = -17.966, Lag order = 0, p-value = 0.01 ## alternative hypothesis: stationary The Dickey-Fuller test returns a p-value of 0.01, resulting in the rejection of the null hypothesis and accepting the alternate, that the data is stationary. It is quite common in financial analysis to predict stock returns. By taking the difference between stocks, we are essentially stationarizing the time series. Though not all stock returns are stationary, in many experiments regarding financial analysis, many assume it is. 48.3 3. ACF/PACF ACF stands for Auto-Correlation Function. ACF gives us values of any auto-correlation with its lagged values. In essence, it tells us how the present value in the series is related in terms with its past values. ACF will help us determine the number, or order, of moving-average (MA) coefficients in our ARIMA model. PACF stands for Partial Auto-Correlation Function. Instead of finding correlations of present with lags like ACF, it finds correlation of the residuals with the next lag value. If there is any hidden information in the residual which can be modeled by the next lag, we might get a good correlation and we will keep that next lag as a feature while modeling. PACF helps us identify the number of auto-regression (AR) coefficients in our ARIMA model. In short, ACF and PACF will allow us to determine the order of our parameters for our ARIMA model. acf(diff(log(jnj$close))) pacf(diff(log(jnj$close))) To determine the order of our parameters, we have to look at the difference in lags in both the ACF and PACF graphs. If there is a signficant drop after some lag in the graphs, that suggests the ordered terms we should use for our parameters. For instance, in the ACF graph above, the curve drops significantly after the first lag, so perhaps we should model with one moving average component (MA(1)). While this PACF graph is especially hard to read, let’s say the PACF graph has a significant cut off after 3rd lag, and make it a AR(3) process. 48.4 4. Build the ARIMA Model Our findings in the ACF/PACF section suggest that model ARIMA(1, 0, 1) might be the best fit. Building an ARIMA model is easy with the forecast package; we just call the function ‘arima’, and specify our parameters. library(forecast) (fit &lt;- arima(diff(log(jnj$close)), c(3, 0, 1))) ## ## Call: ## arima(x = diff(log(jnj$close)), order = c(3, 0, 1)) ## ## Coefficients: ## ar1 ar2 ar3 ma1 intercept ## 0.6295 0.0387 -0.0954 -0.7394 0.0056 ## s.e. 0.2644 0.0759 0.0775 0.2636 0.0019 ## ## sigma^2 estimated as 0.002493: log likelihood = 432.35, aic = -852.69 How do we know how well we did? The Akaike information criterion (AIC) score is a good indicator of the ARIMA model accuracy. The lower the AIC score, the better the model performs. Here the model gives us an AIC score of -850.88. But is that really the best we can do? How would we be able to check this? The answer is through iterated experimentation. We have to randomly experiment with the parameters, until we find the parameters that yield the lowest AIC. Let’s check that assumption by comparing the AIC of this model to other models. We will use built-in function in forecast called ‘auto.arima’, which will find the best parameters for our model. A side note: although it can be an easy way out to just use “auto.arima” to find out the best estimated parameters, it is nevertheless a good idea to understand the before steps in order to conduct a prodcutive time series analysis. fitARIMA &lt;- auto.arima(diff(log(jnj$close)), trace=TRUE) ## ## Fitting models using approximations to speed things up... ## ## ARIMA(2,0,2) with non-zero mean : -864.1353 ## ARIMA(0,0,0) with non-zero mean : -851.583 ## ARIMA(1,0,0) with non-zero mean : -850.7393 ## ARIMA(0,0,1) with non-zero mean : -851.8083 ## ARIMA(0,0,0) with zero mean : -850.2716 ## ARIMA(1,0,2) with non-zero mean : -851.5652 ## ARIMA(2,0,1) with non-zero mean : -854.3621 ## ARIMA(3,0,2) with non-zero mean : -857.4924 ## ARIMA(2,0,3) with non-zero mean : -847.7652 ## ARIMA(1,0,1) with non-zero mean : -853.6353 ## ARIMA(1,0,3) with non-zero mean : -852.0785 ## ARIMA(3,0,1) with non-zero mean : -856.983 ## ARIMA(3,0,3) with non-zero mean : Inf ## ARIMA(2,0,2) with zero mean : -864.44 ## ARIMA(1,0,2) with zero mean : -846.3745 ## ARIMA(2,0,1) with zero mean : -851.8578 ## ARIMA(3,0,2) with zero mean : -851.6499 ## ARIMA(2,0,3) with zero mean : -862.3785 ## ARIMA(1,0,1) with zero mean : -848.4336 ## ARIMA(1,0,3) with zero mean : -846.7636 ## ARIMA(3,0,1) with zero mean : -853.7823 ## ARIMA(3,0,3) with zero mean : -855.9012 ## ## Now re-fitting the best model(s) without approximations... ## ## ARIMA(2,0,2) with zero mean : Inf ## ARIMA(2,0,2) with non-zero mean : Inf ## ARIMA(2,0,3) with zero mean : Inf ## ARIMA(3,0,2) with non-zero mean : Inf ## ARIMA(3,0,1) with non-zero mean : -852.3766 ## ## Best model: ARIMA(3,0,1) with non-zero mean We get that the best ARIMA model is achieved with parameters p=3, d=0, q=1. 48.5 5. Make Predictions Before we make predictions, let’s see how our model fitted with our training data. plot(as.ts(diff(log(jnj$close))) ) lines(fitted(fitARIMA), col=&quot;red&quot;) Moving on, we can make a prediction of future stock returns with the forecast.Arima function. We predict the returns on the next 5 months: futurVal &lt;- forecast(fitARIMA,h=5, level=c(99)) #confidence level 99% plot(forecast(futurVal)) # 5 predicted values futurVal$mean ## Time Series: ## Start = 275 ## End = 279 ## Frequency = 1 ## [1] 0.009774685 0.007420760 0.005455234 0.005188938 0.005169821 And we’re done! We can perhaps better determine our model accuracy by using past data and split it into a test and training set, but I wanted to use this as an example of the capabilities of ARIMA in Time Series Modeling. To conclude, we talked about time series modeling in R. We went over how to stationarize our data, how to determine the order parameters from ACF/PACF, and ultimately how to build our ARIMA model and predict with it. As you can see, time series modeling is a difficult subject, especially in the world of finance, but it can be extremely rewarding as well. Below I have more resources that go more in depth about the various topics I talked about. 48.6 References/Additional Resources Chatterjee, Subhasree. “Time Series Analysis Using ARIMA Model In R.” DataScience+, 5 Feb. 2018, https://datascienceplus.com/time-series-analysis-using-arima-model-in-r/ Dalinina, Ruslana. “Introduction to Forecasting with ARIMA in R.” Oracle Data Science, https://blogs.oracle.com/datascience/introduction-to-forecasting-with-arima-in-r Nua, Bob. “Identifying the Numbers of AR or MA Terms in an ARIMA Model.” Identifying the Orders of AR and MA Terms in an ARIMA Model, https://people.duke.edu/~rnau/411arim3.htm. Paradkar, Milind. “Forecasting Stock Returns Using ARIMA Model.” R-Bloggers, 9 Mar. 2017, www.r-bloggers.com/forecasting-stock-returns-using-arima-model. More info on ARIMA: https://people.duke.edu/~rnau/411arim.htm More info on time series stationarity: https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322 More info on ACF/PACF: https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8 More info on AIC: https://www.statisticshowto.datasciencecentral.com/akaikes-information-criterion/ "],
["modeling-links.html", "Chapter 49 Modeling links 49.1 Exploring Financial Models 49.2 Overview of the t-SNE algorithm", " Chapter 49 Modeling links 49.1 Exploring Financial Models Shreyas Jadhav(sj3006), Andrei Sipos(ags2202), Gideon Teitel(gt2288) We made a presentation on exploration and visualization of Financial models and presented it in class on 28th October 2019. The presentation involved analysis of 3 models namely: Modern Portfolio Theory (Markowitz Model) Capital Asset Pricing Model (CAPM) Brownian Motion and Bates Model - Jump Diffusion We have analysed and implemented the models in python and R. Following are the links for the same: LINKS: Code for: Modern Portfolio Theory (Markowitz Model) and Capital Asset Pricing Model (CAPM) : https://github.com/shreyasj3006/Exploring-Financial-Models Link for the entire presentation : https://drive.google.com/open?id=0B9K8qs96dJgzZGhkNHRvS051dmlDc2dwcWl6enNIQzdXcHhJ Note: Please use your lionmail id to access this link. Code for Simple Brownian Motion Simulation: miu=0.01; sigma=0.03; T=1/12; n=1000; P0=100; dt=T/n t=seq(0,T,by=dt) Price=c(P0,miu*dt+sigma*sqrt(dt)*rnorm(n,mean=0,sd=1)) Price=cumsum(Price) plot(t,Price,type=&#39;l&#39;,ylab=&quot;Price P(t)&quot;,xlab=&quot;Time t&quot;,main = &quot;Brownian Motion&quot;) #install.packages(&quot;sde&quot;) library(sde) nt=10; t=seq(0,T,by=dt) X=matrix(rep(0,length(t)*nt), nrow=nt) for (i in 1:nt) { X[i,]= GBM(x=P0,r=miu,sigma=sigma,T=T,N=n) } plot(t,X[1,],t=&#39;l&#39;,ylim=c(min(X), max(X)), col=1, ylab=&quot;Price P(t)&quot;,xlab=&quot;Time t&quot;,main = &quot;Geometric Brownian Motion&quot;) for(i in 2:nt){lines(t,X[i,], t=&#39;l&#39;,ylim=c(min(X), max(X)),col=i)} # #install.packages(&quot;ESGtoolkit&quot;) library(ESGtoolkit) eps0 &lt;- simshocks(n = 10, horizon = 50, frequency = &quot;quart&quot;) sim.GBM &lt;- simdiff(n = 10, horizon = 50, frequency = &quot;quart&quot;, model = &quot;GBM&quot;, P0, theta1 = 0.03, theta2 = 0.1, eps = eps0) matplot(time(sim.GBM), sim.GBM, type = &#39;l&#39;, ylab=&quot;Price P(t)&quot;,xlab=&quot;Time t&quot;,main = &quot;Bates Model - Jump Diffusion Simulation&quot;) 49.2 Overview of the t-SNE algorithm Arjun Dhillon For my community contribution, I’ll be delivering a 5-mintue lightning presentation on t-distributed stochastic neighbor embeddings (t-SNE). The t-SNE algorithm is useful in representing high-dimensional data in 2 or 3 dimensions. The process works by mapping higher dimensional data points to 2 or 3 dimesional datapoints in such a way that similar data points have higher probability of being near one another. It is especially useful for visualizing latent reprsentations of data in neural networks. The main idea of t-SNE is to minimize the Kullback-Leibler divergence of the distribution \\(Q\\) of points in the remapped space from the distribution of points in \\(P\\) the original space: \\[KL(P||Q) = \\sum_{i \\ne j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\\], where \\(p_{ij}\\) represents the similarity of two points in the original space: \\[p_{ij} = \\frac{\\exp( - \\lVert x_i - x_j \\rVert^2 / 2 \\sigma_i^2)}{\\sum_{k \\ne i} \\exp( - \\lVert x_i - x_j \\rVert^2 / 2 \\sigma_i^2)}\\] and \\(q_{ij}\\) represents the similarity of the mappings \\(y_i\\): \\[q_{ij} = \\frac{(1 + \\lVert y_i - y_j \\rVert^2)^{-1}}{\\sum_{k \\ne i} (1 + \\lVert y_i - y_j \\rVert^2)^{-1}}\\] "],
["rmarkdown-tutorial.html", "Chapter 50 Rmarkdown tutorial 50.1 1. Overview 50.2 2. Getting started 50.3 3. Markdown syntax 50.4 4. Embeding code 50.5 5. Rendering", " Chapter 50 Rmarkdown tutorial Author: Shaofeng Wu, Mingrui Liu 50.1 1. Overview R Markdown provides a tidy framework for data science. A markdown file can normally help us: - save and execute code that you wrote - generate high quality reports that can be shared with an audience R Markdown documents are fully reproducible and support dozens of static and dynamic output formats. This link provide a quick tour of what’s possible with R markdown.-links 50.1.1 1.1 What is R Markdown? Rmd files · An R Markdown(.Rmd) file is a record of your project. It contains the code that your audience needs to reproduce your work as well as all the scripts that your audience can understand what you did. Reproducible Research · You can use Knit to rerun the code in an R Markdown file to reproduce your work and export the results to be a well-formatted report. Dynamic Documents · There are a lot of ways to export your report. The formats include html, pdf, MS Word, or RTF documents; html or pdf based slides, Notebooks, and more. 50.1.2 1.2 Workflow Open a new .Rmd file at File ▶ New File ▶ R Markdown. Write document by editing template Knit document to create report; use knit button or render() to knit Preview Output in IDE window Publish (optional) to web server Examine build log in R Markdown console Use output file that is saved along side .Rmd 50.2 2. Getting started 50.2.1 2.1. Install the package You can use the following command to install the required library. install.packages(&#39;rmarkdown&#39;) 50.2.2 2.2. Open file You can create a new file or open existed from the directory that you choose. 50.2.3 2.3. output format The Rmarkdown can render any Rmd file into a format that Rmarkdown supports. For example, the code below renders OutputExample.Rmd to a Microsoft Word document. library(rmarkdown) render(&quot;resources/rmarkdown_tutorial/OutputExample.Rmd&quot;, output_format = &quot;word_document&quot;) Following is a table of all the formats that you can choose: Output value table output value creates html_document html pdf_document pdf (requires Tex ) word_document Microsof Word (.docx) odt_document OpenDocument Text rtf_document Rich Text Format md_document Markdown github_document Github compatible markdown ioslides_presentation ioslides HTML slides slidy_presentation slidy HTML slides beamer_presentation Beamer pdf slides (requires Tex) You can choose to render to the format that you want by clicking the dropdown menu beside the knit button: 50.3 3. Markdown syntax Rmarkdown has lots of fancy syntax so that you can produce an ordered and beautiful document. We will give the syntax here that we regularly use a lot. plain text plain text italics and bold *italics* and **bold** italics and bold list * unordered list + sub-item 1 + sub-item 2 - sub-sub-item 1 unordered list sub-item 1 sub-item 2 sub-sub-item 1 1. ordered list 2. item 2 i) sub-item 1 A. sub-sub-item 1 ordered list item 2 sub-item 1 A. sub-sub-item 1 headers # Header1 {#anchor} ## Header 2 {#css_id} ### Header 3 {.css_class} #### Header 4 ##### Header 5 ###### Header 6 Caption hyperlink &lt;http://www.rstudio.com&gt; [link](www.rstudio.com) Jump to [Header 1](#anchor) http://www.rstudio.com link Jump to Header 1 table | Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Right Left Default Center 12 12 12 12 123 123 123 123 1 1 1 1 equation $$E = mc^{2}$$ \\[E = mc^{2}\\] 50.4 4. Embeding code 50.4.1 4.1. Inline code You can surround code with back ticks and r. R will replace inline code with its results. For example: one plus one equals `r 1+1` The output is: one plus one equals 2 50.4.2 4.2. Code chunks In this way, ou need to start a chunk with ``` {r} and end a chunk with ``` For example: print(&quot;Hello, world!&quot;) ## [1] &quot;Hello, world!&quot; 50.4.3 4.3. Display options There are also a lot of options to display your codes and results. For example, you can choose eval as TRUE or False in the output to decide whether to evaluate the code and include its results. Here is the difference between the two options: With eval = TRUE: print(&quot;Hi there!&quot;) ## [1] &quot;Hi there!&quot; With eval = FALSE: print(&quot;Hi there!&quot;) Here is a table that includes the options that we normally use: option default effect eval TRUE Whether to evaluate the code and include its results echo TRUE Whether to display code along with its results warning TRUE Whether to display warnings error FALSE Whether to display errors message TRUE Whether to display messages tidy FALSE Whether to reformat code in a tidy way when displaying it cache FALSE Whether to cache results for future renders comment “##” Comment character to preface results with 50.5 5. Rendering first way You can run rmarkdown::render(&quot;&lt;file path&gt;&quot;) in the console. second way You can click the Knit button in the top pane and choose the output format that you want. "],
["python-in-rmarkdown.html", "Chapter 51 Python in Rmarkdown", " Chapter 51 Python in Rmarkdown Karan Rao This is a quick tutorial for running Python code in an R Markdown file. This is especially useful for final projects where students may want to use Python for data cleaning, string manipulation, and even machine learning. This guide isn’t exhaustive - it’s intended to help you get started immediately and fix some common issues you might find. Link to GitHub document: https://github.com/raokaran/python-rmd/blob/master/PythonR.md "],
["rstudio-vs-jupyterlab-talk.html", "Chapter 52 RStudio vs JupyterLab (talk)", " Chapter 52 RStudio vs JupyterLab (talk) Dhananjay Deshpande This presentation introduces RStudio and JupyterLab as Data Science tools to the audience and compares the salient features of each tool. It later summarizes the findings and makes recommendations for Data Scientists based on current features available. RStudio vs JupyterLab for Data Science "],
["bookdown-workshop.html", "Chapter 53 bookdown (workshop)", " Chapter 53 bookdown (workshop) Weixi Yao and Wangzhi Li This introductory workshop on bookdown is designed to give a complete guide to the bookdown package. The workshop is split into two parts. The first part covers the basic information, including what is bookdown, why use bookdown and what are the other options avaiable. The second part serves as a practice session, and each attendee will try to build their own books using the instruction we provide. If the attendees want to know more about bookdown, they can always go back to our slides for reference. We have uplodaed our complete slide to the repo (url: https://github.com/yweixi/EDAV-community-contribution.git; file name: EDAV Community Contribution.pdf). Also, all the materials we use during the workshop can be found in a seperate repo (url: https://github.com/SafeguardLi/SafeguardLi.github.io.git). "],
["the-first-step-to-analyse-a-dataset.html", "Chapter 54 The first step to analyse a dataset 54.1 Introduction 54.2 A glimpse at the dataset 54.3 Dive into one column 54.4 Advanced patterns about a data set", " Chapter 54 The first step to analyse a dataset Weitao Chen and Jianing Li 54.1 Introduction Oftentimes we get a large dataset to conduct data analysis or to prepare the data to be further used in our model. A common question data analysts hear is “where do I even start in my analysis?”. We will introduce serveral functions to help you take a glance at your giant dataset so you can have a fundemental undetstanding of your data. 54.2 A glimpse at the dataset 54.2.1 How does the data look like? When you come across a new dataset, you may ask: How does the dataset look like? “To see is to know. Not to see is to guess.” To answer this question, you just need to view its content. View works not only in RStudio, but also in R terminal on Windows. However, its output can’t be knitted into html or pdf. Though, when the dataset is huge, you would not really like to open and view the entire dataset. Then what? Just take a sample of it. The first function that comes to your mind? head! ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 NA 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 NA 0.4 setosa head returns the first 5 rows of a data frame by default. You can also specify how many rows you want. head(iris,3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 NA 1.3 0.2 setosa tail is similar to it, but it retrives the rows from the bottom. tail(iris,2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 149 6.2 3.4 5.4 NA virginica ## 150 5.9 3.0 5.1 1.8 &lt;NA&gt; As you can see, we get only the flowers of species “setosa” with head and of “virginica” with tail. That’s because the original dataset is ordered by the “Species”, and its head and tail are homogeneous. What if we want to get a heterogeneous view? You may use sample_n instead. It selects random rosw from a table for you. library(dplyr) sample_n(iris, 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.3 2.5 5.0 1.9 virginica ## 2 6.3 3.3 4.7 1.6 versicolor ## 3 6.1 3.0 4.9 1.8 virginica ## 4 5.2 3.4 1.4 0.2 setosa ## 5 5.3 3.7 1.5 0.2 setosa When you want to take a glimpse at a huge dataset, it’s fairly better choice to use sample_n. However, unlike head, tail or View, it doesn’t work on vector or types other than data frame. Also, you have to library dplyr to use sample_n, while other 3 functions come in baser. 54.2.2 Retrive the metadata For a data frame, one of the first property we want to learn about it would be its size. dim(iris) ## [1] 150 5 The first value in the result is the number of observations and the second is of variables. Sometimes we don’t really want to know the details of a dataset. We simply need a summary. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.400 Median :1.300 ## Mean :5.823 Mean :3.057 Mean :3.835 Mean :1.182 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## NA&#39;s :9 NA&#39;s :5 NA&#39;s :7 NA&#39;s :8 ## Species ## setosa :47 ## versicolor:47 ## virginica :48 ## NA&#39;s : 8 ## ## ## When used on a data frame, summary gives you the metadata of the dataset. That is, the summaries for each vairable in it. For numerical column, you will see the percentiles and mean. For categorical ones, you can get the frequency of each factors. A simpler alternative is str. Note that str here is not an abbreviation for “string”, but for “structure”. str “compactly displays the structure of an arbitrary R object”, according to the document. As for data frame, it displays the data type and first few values of each columns. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 NA 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 NA 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 54.3 Dive into one column 54.3.1 Summarise a numerical variable If you would like to examine some columns more thoroughly, you can use summarise to customize your summary. It takes the data frame as the first parameter. Name-value pairs of summary functions are followed, indicating the output titles and content. Available summary functions are as followed: Center: mean(), median() Spread: sd(), IQR(), mad() Range: min(), max(), quantile() Position: first(), last(), nth(), Count: n(), n_distinct() Logical: any(), all() library(dplyr) iris %&gt;% summarize(mean.1 = mean(Petal.Length, na.rm=TRUE), sd.1 = sd(Petal.Length, na.rm=TRUE), mean.2 = mean(Sepal.Length, na.rm=TRUE), sd.2 = sd(Sepal.Length, na.rm=TRUE)) ## mean.1 sd.1 mean.2 sd.2 ## 1 3.834965 1.751227 5.823404 0.8196374 54.3.2 Understand a categorical variable You may learn about the frequency about a categorical column in a data frame using summary. summary(iris$Species) ## setosa versicolor virginica NA&#39;s ## 47 47 48 8 Though, it doesn’t work when the column is presented not as a factor, but as characters. iris$Characters &lt;- as.character(iris$Species) summary(iris$Characters) ## Length Class Mode ## 150 character character Under such circumstances, we may use unique and table instead. unique tells you about all the unique values in a vector, and table shows their frequency. unique(iris$Characters) ## [1] &quot;setosa&quot; NA &quot;versicolor&quot; &quot;virginica&quot; table(iris$Characters) ## ## setosa versicolor virginica ## 47 47 48 54.4 Advanced patterns about a data set 54.4.1 Locate the missing values Sometime there are missing values or NA in our dataset. It can be caused by a number of reasons such as observations that were not recorded and data corruption. Or it can be left as NA on purpose to indicate something. The important thing is how to find the missing values in our dataset. In this block we will introduce several functions to find the missing values Check missing values by columns colSums(is.na(iris)) %&gt;% sort(decreasing=TRUE) ## Sepal.Length Petal.Width Species Characters Petal.Length Sepal.Width ## 9 8 8 8 7 5 Check missing values by row rowSums(is.na(iris)) %&gt;% sort(decreasing=TRUE) ## [1] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [149] 0 0 Use a heatmap to check missing values library(ggplot2) tidyiris &lt;- iris %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, -id) %&gt;% mutate(missing = ifelse(is.na(value), &quot;yes&quot;, &quot;no&quot;)) ggplot(tidyiris, aes(x = key, y = fct_rev(id), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;iris with NAs added&quot;) + scale_fill_viridis_d() + # discrete scale theme_bw() if(&quot;mi&quot; %in% rownames(installed.packages()) == TRUE) { library(mi) x &lt;- missing_data.frame(iris) image(x) } ## NOTE: The following pairs of variables appear to have the same missingness pattern. ## Please verify whether they are in fact logically distinct variables. ## [,1] [,2] ## [1,] &quot;Species&quot; &quot;Characters&quot; 54.4.2 Find the outlier for numeric values An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.[3] An outlier can cause serious problems in statistical analyses. Here we find the outliers in each columns. tidyiris &lt;- clean.iris[1:4] %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, -id) ggplot(data=tidyiris)+ geom_boxplot(mapping=aes(x=key,y=value)) Thus we can see that there are 4 outliers in Sepal.Width variable. 54.4.3 Find out the correlations among variables For our numeric data, it is important to find the correlations between variables, since it provides us extra information about the dataset. library(GGally) ggpairs(clean.iris[1:4]) "],
["tinder-self-reflection.html", "Chapter 55 Tinder self-reflection 55.1 Introduction 55.2 Analysis 55.3 Conclusion 55.4 Final Thoughts", " Chapter 55 Tinder self-reflection Benjamin Livingston 55.1 Introduction When I told my friends I was doing this, they laughed. After I showed this to my friends, they laughed again. I laughed, too. Our Tinder data is a disturbingly accurate window into our romantic selves. It traces so many of our dating tendencies, from pickiness, to obsession, to desperation, to pushiness. I gained tremendous insight into my romantic habits from this exercise, and I hope you will enjoy it as much as I did. Most importantly, I’ve constructed this in a way that will allow you to easily do this analysis for yourself, too. 55.1.1 For The Taken / Non-Millennial Folk You’re probably going to look at every statistic and graph here and wonder, “what the heck is all this?” Tinder is a dating app that launched in 2012, available from any web browser or smartphone. You create a profile, select your preferred gender, age, and locational proximity for a potential partner, and Tinder provides you a sequence of other users that fit your criteria. Every time a user’s profile pops up, you can either “swipe left” and pass on them, or “swipe right” and like them. If (and only if) you and the other user both swipe right on each other, you are deemed a “match”, and you gain ability to talk to one another. (via Innovation Is Everywhere) User habits vary: some users swipe right on everyone they see, while some users are very picky. There is very little explicit feedback from the app, so the user is forced to form their own conclusions from their personal data, which Tinder allows you to download. 55.1.2 Replicating This Analysis For Yourself I’ve made it possible for you to create all these statistics and graphs for yourself at the click of a button. Your Tinder data can be downloaded at this link. In this GitHub repository, you will find a file called grabyourtinder.R. If you download your Tinder data as instructed, you will receive a zipped file. In that file, there is a JSON labeled “data.JSON”. This is your Tinder data - namely, all your messages and daily statistics. The code I wrote for this project allows any user to extract all of their daily usage statistics from this JSON without the need for additional software. If you extract and copy data.JSON to your R working directory and run the code in grabyourtinder.R, you will be able to create all these graphs and statistics for yourself, and generate a .csv of your Tinder data. Try this!!!! I did all the legwork for you. I’d love to hear what you come up with. If you don’t like what you see, you can throw your laptop out the window and the evidence will disappear forever. A note for non-R users: If you haven’t learned R, this is the perfect time to. It’s free, extremely easy to use, fun to play with, and very powerful. Two recommended free resources if you’d like to try it out: Hadley Wickham’s R For Data Science Roger D. Peng’s R Programming For Data Science 55.1.3 Protecting The Innocent (and Not-So-Innocent) Since my Tinder data JSON file also contains my message data, it will unfortunately not be made available with this project. As you will see soon, there are a lot of messages in there, and thus a plethora of personally identifiable information (for myself and others) that can’t be posted on the internet. Hope you understand. In lieu of this, I have included a .csv file with my daily usage statistics in the GitHub repository, which was extracted from the JSON using my script. 55.1.4 A Fun Twist I will be plotting my Tinder usage over time, and I’m going to add an extra feature to spice it up. This data covers 2014-2015 until the present (we will explain why the start of this range is indefinite later). In Fall 2016, I moved from Pittsburgh to Philadelphia, and then in Summer 2019, I moved from Philadelphia to New York. We will mark those moves in our graphs, and see if we discover any geographic trends as we conduct our analysis. 55.2 Analysis 55.2.1 Our Fun New Tinder Statistics: “Amourmetrics” Opens - the number of times I opened the Tinder app Messages - messages exchanged on the app (split by sent vs. received where stated, combined otherwise) Likes - the number of times I swiped right (a.k.a. “liked” a user) Passes - the number of times I swiped left (a.k.a. “passed” on a user) Swipes - the total number of times I swiped, equal to likes + passes 55.2.2 All-Time Statistics &amp; A Demographical Discovery Let’s start by examining my messaging habits. print(paste0(&#39;Total messages sent: &#39;,sum(bentinder$messages_sent))) ## [1] &quot;Total messages sent: 23047&quot; print(paste0(&#39;Total messages received: &#39;,sum(bentinder$messages_received))) ## [1] &quot;Total messages received: 19156&quot; print(paste0(&#39;Total messages: &#39;,sum(bentinder$messages_sent)+sum(bentinder$messages_received))) ## [1] &quot;Total messages: 42203&quot; I’m a talkative person, so this isn’t particularly surprising. What’s most interesting about this talk-versus-listen trend is how it has varied over time, which we’ll get to in a bit. Of course, your reaction may be a more primal “FORTY TWO THOUSAND MESSAGES?!?!”. If that’s the case, wait until you see my all-time totals across all Tinder statistics. messages = bentinder %&gt;% select(date,messages_sent,messages_received) %&gt;% mutate(message_differential = messages_received - messages_sent) bentinder = bentinder %&gt;% mutate(messages = messages_sent + messages_received) %&gt;% select(-c(messages_sent,messages_received)) bentinder = bentinder %&gt;% mutate(swipes=likes+passes) sapply(bentinder[-1],sum) ## opens likes passes matches messages swipes ## 25081 75404 214505 8777 42203 289909 289,909 swipes! 289,909! This is all mind-blowing… but 289,909?? This could make you laugh, cry, drop your jaw, or just rub your temples and shake your head. But there’s a deeper meaning to this number that I’d like to explore - because considering that I only date men, it’s completely incomprehensible. Think about this for a moment. According to a 2006 study by UCLA’s Gary J. Gates (the most recent readily-available, exhaustive empirical estimate of metropolitan area LGBT populations), the 2005 LGBT populations of the metropolitan areas I’ve lived in were approximately as follows: Pittsburgh: 50,994 Philadelphia: 179,459 New York: 568,903 Furthermore, the LGBT population of Pennsylvania as a whole was 323,454. While these numbers have likely grown in the last decade-and-a-half, they don’t seem to have skyrocketed significantly based on more recent city LGBT population estimates, nor does it make intuitive sense that the number of gay men would have grown astronomically in the last 15 years. Surely, more people are openly LGBT in 2019 than in 2005, but we are making an important distinction between LGBT and openly LGBT (or identifying as LGBT) here. In other words, telling a survey-taker that you are LGBT and being LGBT (and seeking same-sex partners on Tinder) can be two very different things. A quick calculation finds that 273,682 of these swipes happened before I moved to New York. The combined LGBT adult populations of the Pittsburgh and Philadelphia areas is estimated at 230,453, and the entire state only has an estimated 323,454 LGBT adults. If we make a loose assumption that about half of the LGBT residents are male, that would leave about 115,000 gay men in the Philadelphia and Pittsburgh areas and about 162,000 gay men in Pennsylvania at large. I swiped 273,000 times while I lived in Pennsylvania. That means I swiped more than twice the number of available people in my cities and over 1.5 times the number of gay men in my state. Considering I typically set Tinder to only show me people close to my age (almost exclusively within five years), this doesn’t make any sense. This makes me wonder if these LGBT population estimates are even close to accurate. I swiped a lot while out of town (or while using Tinder’s Passport feature) and with visitors from other places, and while I can’t definitively state exactly how much of my swiping was done with people from other metropolitan areas, it probably isn’t enough to explain this trend. Even if only 50,000 of my swipes were done with people residing in my metropolitan area (which would be less than 20% of my overall swipes), these numbers still don’t add up. Tinder typically doesn’t suggest the same person twice, so we can probably rule that out as a major factor. It seems very likely I saw at least 200,000 unique people, and we will make a low-end estimate that 50,000 of them lived in the Philadelphia and Pittsburgh areas. It seems extremely unlikely that that 50,000 of the estimated 115,000 LGBT adult males in those areas are Tinder users close to my age. These numbers suggest there are (and have probably long been) many more gay men in these cities than the aforementioned research stated in 2006, and other self identification-based research has stated since. The linked Gallup article states that “Estimate of LGBT Population Rises to 4.5%”. My data casts serious doubt on the validity of these estimates. In no way am I claiming definitive proof that these figures are wrong, but even a cursory glance at my numbers makes them seem like poor estimates. There are many potential mitigating factors here that prevent any sort of sound empirical proof of this assertion. However, the most parsimonious, plausible explanation is that the true number of gay men in America (or at the very least, in Pennsylvania) hasn’t been anywhere close to properly enumerated in studies that rely on self-identification. Of course… perhaps all this analysis is simply a ploy on my part to deflect from the fact that I have swiped two hundred and eighty-nine thousand times on Tinder. I can’t wrap my head around that number any more than you can. Let’s continue this dive into insanity by examining my all-time daily maximums. sapply(bentinder[-1],max) ## opens likes passes matches messages swipes ## 172 1632 3548 91 509 5144 I don’t need any advanced statistical data analysis to tell you that opening Tinder 172 times in one day and swiping 5144 times in a day is… well I’ll let you pick a word for it. I’m curious though… what was happening on those days? Let’s check the records and find out. bentinder %&gt;% filter(opens==172|likes==1632|passes==3548|matches==91|messages==509|swipes==5144) %&gt;% mutate(day = wday(date,label = T)) ## date opens likes passes matches messages swipes day ## 1 2016-04-10 135 1632 3512 91 289 5144 Sun ## 2 2016-04-12 91 1231 3548 65 241 4779 Tue ## 3 2016-04-13 117 528 1897 72 509 2425 Wed ## 4 2017-02-04 172 1357 3324 81 425 4681 Sat It’s strange: there was nothing remarkable about these days. A quick study of my Google Maps timeline shows that I didn’t go anywhere remotely interesting on any of these days, other than work. I had expected that my record usage would come with travel, but it seems that it just came with boredom. A quick look back at the photos I took on those days confirms the sheer mundanity of my record-setting Tinder marathons. I swiped 12,250 times on the days I took those photos. I will never stop laughing at this. 55.2.3 “It’s Like Batting Average, But For Tinder” Next, we will debut my favorite new statistic from this analysis: the swipe right percentage print(&#39;Swipe right percentage:&#39;) ## [1] &quot;Swipe right percentage:&quot; 100 * (sum(bentinder$likes) / (sum(bentinder$likes) + sum(bentinder$passes))) ## [1] 26.00954 I swipe right on (a.k.a. “like”) only 26% of users. At first, I thought this felt low, and that I was being too picky. Then, I wondered if this might actually be high, since I have no baseline to judge it against. So, let’s answer another simple question: what percentage of users I swiped right on (or “liked”) swiped right on me (“liked” me back)? print(&#39;Match percentage:&#39;) ## [1] &quot;Match percentage:&quot; 100 * sum(bentinder$matches) / sum(bentinder$likes) ## [1] 11.63997 This number is much lower - only 11.6%! I like 26% of users, but only 11.6% of those users like me back. However, it’s important to note that 11.6% of users like me among users that I liked. For the general population, it’s likely a higher percentage, perhaps equal to or greater than 26%. Unfortunately, Tinder does not provide data on how other users swiped you, and we cannot derive this value using probability theory without further information. Still, it’s fascinating to know that of the people I like, only about 1 in 11 like me back. Perhaps I am too picky! For good measure, let’s calculate the percentage of swipes that have yielded a match. print(&#39;Swipes per match:&#39;) ## [1] &quot;Swipes per match:&quot; 100 * sum(bentinder$matches) / sum(bentinder$swipes) ## [1] 3.027502 To be fair, this isn’t quite as bad as I thought. I can deal with swiping 33 times (which takes a minute or two) to get a match. Had this number been 100, I would have felt very differently. We will incorporate these variables into the rest of our analysis as follows. First, we add a swipe right rate, which is equal to the number of times I swipe right divided by my total number of swipes. Second, we add a match rate, a log-adjusted variable that gets higher as more users return my swipes right in kind, and lower as more of the users I liked pass on me. Additional details for math people: To be more specific, we will take the ratio of matches to swipes right, parse any zeros in the numerator or the denominator to 1 (necessary for generating real-valued logarithms), and then take the natural logarithm of this value. This statistic itself won’t be particularly interpretable, but the comparative overall trends will be. 55.2.4 Where &amp; When Did My Swiping Habits Change? We will start our graphing by examining my match rate and swipe right rate over time. bentinder = bentinder %&gt;% mutate(swipe_right_rate = (likes / (likes+passes))) %&gt;% mutate(match_rate = log( ifelse(matches==0,1,matches) / ifelse(likes==0,1,likes))) rates = bentinder %&gt;% select(date,swipe_right_rate,match_rate) match_rate_plot = ggplot(rates) + geom_point(size=0.2,alpha=0.5,aes(date,match_rate)) + geom_smooth(aes(date,match_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=-0.5,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=-0.5,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=-0.5,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(-2,-.4)) + ggtitle(&#39;Match Rate Over Time&#39;) + ylab(&#39;&#39;) swipe_rate_plot = ggplot(rates) + geom_point(aes(date,swipe_right_rate),size=0.2,alpha=0.5) + geom_smooth(aes(date,swipe_right_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=.345,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=.345,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=.345,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(.2,0.35)) + ggtitle(&#39;Swipe Right Rate Over Time&#39;) + ylab(&#39;&#39;) grid.arrange(match_rate_plot,swipe_rate_plot,nrow=2) Match rate fluctuates very wildly over time, and there clearly isn’t any sort of annual or monthly trend. It’s cyclical, but not in any obviously traceable manner. My best guess here is that the quality of my profile photos (and perhaps general dating prowess) varied significantly over the last five years, and these peaks and valleys trace the periods when I became more or less attractive to other users. The jumps on the curve are significant, corresponding to users liking me back anywhere from about 20% to 50% of the time. Perhaps this is evidence that the perceived “hot streaks” or “cold streaks” in one’s dating life are a very real thing. Swipe right rate stays much more consistent. There are fewer peaks and valleys, and there’s less overall variation. However, there is a very noticeable dip in Philadelphia. As a native Philadelphian, the implications of this frighten me. We have routinely been derided as having some of the least attractive residents in the nation. I passionately reject that implication. I refuse to accept this as a proud native of the Delaware Valley. That being the case, I’m going to write this off as being a product of disproportionate sample sizes and leave it at that. The uptick in New York is abundantly clear across the board, though. I used Tinder very little in Summer 2019 while preparing for graduate school, which causes many of the usage rate dips we’ll see in 2019 - but there is a huge jump to all-time highs across the board when I move to New York. If you’re an LGBT millennial using Tinder, it’s difficult to beat New York. 55.2.5 A Problem With Dates If you study these tables, you’ll notice the same issue I did - missing data for messages and app opens. bentinder[1:20,-c(8,9)] ## date opens likes passes matches messages swipes ## 1 2014-11-12 0 24 40 1 0 64 ## 2 2014-11-13 0 8 23 0 0 31 ## 3 2014-11-14 0 3 18 0 0 21 ## 4 2014-11-16 0 12 50 1 0 62 ## 5 2014-11-17 0 6 28 1 0 34 ## 6 2014-11-18 0 9 38 1 0 47 ## 7 2014-11-19 0 9 21 0 0 30 ## 8 2014-11-20 0 8 13 0 0 21 ## 9 2014-12-01 0 8 34 0 0 42 ## 10 2014-12-02 0 9 41 0 0 50 ## 11 2014-12-05 0 33 64 1 0 97 ## 12 2014-12-06 0 19 26 1 0 45 ## 13 2014-12-07 0 14 31 0 0 45 ## 14 2014-12-08 0 12 22 0 0 34 ## 15 2014-12-09 0 22 40 0 0 62 ## 16 2014-12-10 0 1 6 0 0 7 ## 17 2014-12-16 0 2 2 0 0 4 ## 18 2014-12-17 0 0 0 1 0 0 ## 19 2014-12-18 0 0 0 2 0 0 ## 20 2014-12-19 0 0 0 1 0 0 print(&#39;----------skipping rows 21 to 169----------&#39;) ## [1] &quot;----------skipping rows 21 to 169----------&quot; bentinder[170:190,-c(8,9)] ## date opens likes passes matches messages swipes ## 170 2015-09-07 5 11 18 1 0 29 ## 171 2015-09-08 4 36 96 3 0 132 ## 172 2015-09-09 8 7 11 2 0 18 ## 173 2015-09-10 2 4 7 0 0 11 ## 174 2015-09-11 3 22 60 3 0 82 ## 175 2015-09-14 2 24 56 0 0 80 ## 176 2015-09-15 1 10 16 1 0 26 ## 177 2015-09-17 0 0 0 1 0 0 ## 178 2015-09-18 2 0 0 0 0 0 ## 179 2015-09-19 1 32 87 2 0 119 ## 180 2015-09-20 1 0 0 1 1 0 ## 181 2015-09-21 2 0 2 1 0 2 ## 182 2015-09-22 1 1 3 0 0 4 ## 183 2015-09-25 1 41 105 3 0 146 ## 184 2015-09-26 0 0 0 2 0 0 ## 185 2015-09-27 0 0 0 1 0 0 ## 186 2015-09-28 0 0 0 1 0 0 ## 187 2015-09-29 9 35 94 3 23 129 ## 188 2015-09-30 11 15 25 3 8 40 ## 189 2015-10-01 2 1 4 0 3 5 ## 190 2015-10-04 0 0 0 0 1 0 bentinder = bentinder %&gt;% select(-c(likes,passes,swipe_right_rate,match_rate)) bentinder = bentinder[-c(1:186),] messages = messages[-c(1:186),] We clearly cannot compile any useful averages or trends using those categories if we’re factoring in data collected before Sep 29, 2015. Therefore, we will restrict our data set to all dates since Sep 29, 2015 moving forward, and all inferences will be made using data from that date on. 55.2.6 Overall Trends Now that we’ve redefined our data set and removed our missing values, let’s examine the relationships between our remaining variables. ggduo(bentinder[2:5], types=list(continuous = wrap(&quot;smooth_loess&quot;, alpha = 0.4,size=0.2))) + tinder_theme() It’s abundantly obvious how much outliers affect this data. Nearly all the points are clustered in the lower left-hand corner of every graph. We can see general long-term trends, but it’s hard to make any sort of deeper inference. There are a lot of very extreme outlier days here, as we can see by studying the boxplots of my usage statistics. tidyben = bentinder %&gt;% gather(key = &#39;var&#39;,value = &#39;value&#39;,-date) ggplot(tidyben,aes(y=value)) + coord_flip() + geom_boxplot() + facet_wrap(~var,scales = &#39;free&#39;,nrow=5) + tinder_theme() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + ggtitle(&#39;Daily Tinder Stats&#39;) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank()) A handful of extreme high-usage dates skew our data, and will make it difficult to view trends in graphs. Thus, henceforth, we will “zoom in” on graphs, displaying a smaller range on the y-axis and hiding outliers in order to better visualize overall trends. 55.2.7 Playing Hard To Get Let’s start zeroing in on trends by “zooming in” on my message differential over time - the daily difference between the number of messages I get and the number of messages I receive. ggplot(messages) + geom_point(aes(date,message_differential),size=0.2,alpha=0.5) + geom_smooth(aes(date,message_differential),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=6,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=0.2) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=6,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=6,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.44) + tinder_theme() + ylab(&#39;Messages Sent/Received In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Differential Over Time&#39;) + coord_cartesian(ylim=c(-7,7)) The left side of this graph probably doesn’t mean much, since my message differential was closer to zero when I barely used Tinder early on. What’s interesting here is I was talking more than the people I matched with in 2017, but over time that trend eroded. Either I’m talking less, people are talking to me more, or both. Let’s view messages sent and messages received separately and study the trend a little closer. tidy_messages = messages %&gt;% select(-message_differential) %&gt;% gather(key = &#39;key&#39;,value = &#39;value&#39;,-date) ggplot(tidy_messages) + geom_smooth(aes(date,value,color=key),size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=29,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=.3) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=29,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.2) + tinder_theme() + ylab(&#39;Msg Received &amp; Msg Sent In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Rates Over Time&#39;) There are a number of possible conclusions you might draw from this graph, and it’s hard to make a definitive statement about it - but my takeaway from this graph was this: I talked way too much in 2017, and over time I learned to send fewer messages and let people come to me. As I did this, the lengths of my conversations eventually reached all-time highs (after the usage dip in Phiadelphia that we’ll discuss in a second). Sure enough, as we’ll see soon, my messages peak in mid-2019 more precipitously than any other usage stat (although we will discuss other potential explanations for this). Learning to push less - colloquially known as playing “hard to get” - appeared to work much better, and now I get more messages than ever and more messages than I send. Again, this graph is open to interpretation. For instance, it’s also possible that my profile simply got better over the last couple years, and other users became more interested in me and started messaging me more. Whatever the case, clearly what I am doing now is working better for me than it was in 2017. 55.2.8 Playing The Game ggplot(tidyben,aes(x=date,y=value)) + geom_point(size=0.5,alpha=0.3) + geom_smooth(color=tinder_pink,se=FALSE) + facet_wrap(~var,scales = &#39;free&#39;) + tinder_theme() + ggtitle(&#39;Daily Tinder Stats Over Time&#39;) mat = ggplot(bentinder) + geom_point(aes(x=date,y=matches),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=matches),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=13,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=13,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=13,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,15)) + ylab(&#39;Matches&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Matches Over Time&#39;) mes = ggplot(bentinder) + geom_point(aes(x=date,y=messages),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=messages),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=55,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=55,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,60)) + ylab(&#39;Messages&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Messages Over Time&#39;) opns = ggplot(bentinder) + geom_point(aes(x=date,y=opens),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=opens),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=32,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=32,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=32,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,35)) + ylab(&#39;App Opens&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Tinder Opens Over Time&#39;) swps = ggplot(bentinder) + geom_point(aes(x=date,y=swipes),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=swipes),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=380,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=380,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=380,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,400)) + ylab(&#39;Swipes&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Swipes Over Time&#39;) grid.arrange(mat,mes,opns,swps) Even though my swipe right rate went down in Philadelphia, my usage went up (at least at first). This is probably due to Philadelphia having a much larger population than Pittsburgh, but it could also be a product of having a new dating pool after moving. That always causes a flurry of new Tinder activity. The massive dips during the second half of my time in Philadelphia undoubtedly correlates with my preparations for graduate school, which started in early 2018. Then there’s a surge upon arriving in New York and having a month off to swipe, and a significantly larger dating pool. Notice that when I move to New York, all the usage stats peak, but there is an especially precipitous rise in the length of my conversations. Sure, I had more time on my hands (which feeds growth in all these measures), but the relatively large surge in messages suggests I was making more meaningful, conversation-worthy connections than I had in the other cities. This could have something to do with New York, or maybe (as mentioned earlier) an improvement in my messaging style. 55.2.9 “Swipe Night, Part 2” Overall, there is some variation over time with my usage stats, but how much of this is cyclical? We don’t see any evidence of seasonality, but perhaps there’s variation based on the day of the week? Let’s investigate. There isn’t much to see when we compare months (cursory graphing confirmed this), but there’s a clear pattern based on the day of the week. by_day = bentinder %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(messages=mean(messages),matches=mean(matches),opens=mean(opens),swipes=mean(swipes)) colnames(by_day)[1] = &#39;day&#39; mutate(by_day,day = substr(day,1,2)) ## # A tibble: 7 x 5 ## day messages matches opens swipes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 39.7 8.43 21.8 256. ## 2 Mo 34.5 6.89 20.6 190. ## 3 Tu 30.3 5.67 17.4 183. ## 4 We 29.0 5.15 16.8 159. ## 5 Th 26.5 5.80 17.2 199. ## 6 Fr 27.7 6.22 16.8 243. ## 7 Sa 45.0 8.90 25.1 344. by_days = by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) rates_by_day = rates %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(swipe_right_rate=mean(swipe_right_rate,na.rm=T),match_rate=mean(match_rate,na.rm=T)) colnames(rates_by_day)[1] = &#39;day&#39; mutate(rates_by_day,day = substr(day,1,2)) ## # A tibble: 7 x 3 ## day swipe_right_rate match_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 0.303 -1.16 ## 2 Mo 0.287 -1.12 ## 3 Tu 0.279 -1.18 ## 4 We 0.302 -1.10 ## 5 Th 0.278 -1.19 ## 6 Fr 0.276 -1.26 ## 7 Sa 0.273 -1.40 rates_by_days = rates_by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(rates_by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) Tinder recently labeled Sunday its “Swipe Night”, but for me, that title goes to Saturday. I use the app most then, and the fruits of my labor (matches, messages, and opens that are presumably related to the messages I’m receiving) slowly cascade over the course of the week. I wouldn’t make too much of my match rate dipping on Saturdays. It can take a day or five for a user you liked to open the app, see your profile, and like you back. These graphs suggest that with my increased swiping on Saturdays, my immediate conversion rate goes down, probably for this exact reason. We’ve captured an important feature of Tinder here: it is seldom immediate. Instantaneous responses are rare on Tinder. It’s an app that involves a lot of waiting. You need to wait for a user you liked to like you back, wait for one of you to see the match and send a message, wait for that message to be returned, and so on. This can take a while. It can take days for a match to happen, and then days for a conversation to ramp up. As my Saturday numbers suggest, this often doesn’t happen the same night. So perhaps Tinder is better at looking for a date sometime this week than looking for a date later tonight. 55.2.10 For My Fellow Data Nerds, Or People Who Just Like Graphs Here’s a parallel coordinate plot that allows you to play with the outliers in my usage categories, and see how my luck varied on my high-usage days. Perhaps you’ll notice a trend that I missed. nodates = select(bentinder,-date) parcoords(nodates, rownames = F, brushMode = &quot;1D-axes&quot;, alpha = .4, reorderable = T, queue = T, color = tinder_pink) 55.3 Conclusion 55.3.1 Dubious Demographics The most profound takeaway here is that these numbers cast serious doubt upon many empirical estimates of LGBT populations. We can say with reasonable confidence that my swiping numbers are implausible if estimates of Pennsylvania’s LGBT population are to be believed. 55.3.2 Love Is Bored My Tinder usage peaked when I was doing very little, and it hits its weekly high-water mark on Saturdays. The busier I got in 2019, the more my usage plummeted. Having more time for Tinder clearly leads to more Tinder usage, and having less time for Tinder clearly leads to less Tinder usage. This seems intuitive, but it suggests that romantic obsessions may have as much to do with having nothing to do as they do with actual romance. 55.3.3 Does Location Matter? Well, Maybe. I expected larger differences between localities, but it was difficult to make precise geographic inferences. There were too many other life changes in this period to make any sweeping statements. The drop in swipe right rate upon moving to Philadelphia does stand out (much to my dismay), but certainly not as much as the peaks across the board in New York. Granted, I had a lot of time to swipe when I arrived in New York since I had a month off before school, but it seems that both the quantity and quality of my connections surged. When you’re gay, living in a big city is great for cultural reasons - but we don’t talk enough about how much overall population matters. Clearly, New York was an incredible place to swipe. Location can mean everything when you’re gay. I’ll be curious to see how these New York numbers evolve over time, and if the sheer population of the city allows me to sustain them better than I did in Philadelphia. 55.3.4 The Cinderella Effect My match rate fluctuated very wildly over time, which implies that users’ interest in me varied over time. We can think of this a couple ways - either my representation of myself changed in my profile, or I myself changed and become more or less attractive to other users. Either way, we can interpret this as a sign that we aren’t static, and a person can always get better (or worse) at dating - especially online. 55.3.5 “Playing Hard To Get” May A Be Real Thing The less I dominated my conversations, the longer they got. We can’t prove causality here, but my message differential charts make it appear that a more relaxed, succinct approach to conversation benefitted me. This brings me to my next question… 55.3.6 Can We Solve Dating Using Machine Learning? It would be fascinating to see how my success rates - namely message differential and match rate - are affected by how I use the app. Not using the app isn’t a recipe for dating success, but overdoing it and obsessively swiping and messaging isn’t a good strategy, either. So where’s the middle ground? If we build a model using this data, we may be able to answer these questions, and find an ideal way to use Tinder to maximize success. That’s beyond the scope of this study, but it’s something I’d like to explore down the line. 55.4 Final Thoughts I probably use Tinder too much. I think we’ve established that. Still though, the fruits of my time invested are abundantly evident. I’ve made 8,777 connections, 8,500 of which I probably never would have made otherwise, and some of which became very meaningful to me. We are seldom fortunate enough to have a true quantification of the number of people we’ve interacted with over the years - so as hilariously excessive as all this seems, it’s pretty cool to have such a definitive empirical trace of my 20’s. I’ve learned a lot about myself through this analysis, and I strongly encourage you to run my script and do the same for yourself with your own data. I’d love to hear what you find. Until then, swipe on, my friends. "],
["ice-cream-survey.html", "Chapter 56 Ice Cream Survey 56.1 Overview 56.2 Loading packages and reading in data 56.3 Understanding what cleaning is required 56.4 Cleaning and prepping the data 56.5 Visualizing the data 56.6 Takeaways", " Chapter 56 Ice Cream Survey Jake Stamell 56.1 Overview 56.1.1 Description For my community contribution, I sent out a short (4 question) survey to the class on their ice cream preferences. I asked age, country of origin, cup or cone, and favorite flavor. Country and flavor were open text responses, which I hoped would cause some variation in input requiring cleaning (misspellings, alternative flavor names, etc.). 56.1.2 Goals of this community contribution Share the ice cream preferences of our class! Demonstrate how to clean messy text responses in order to ease analysis and visualization Provide an example of visualizing multiple categorical variables 56.2 Loading packages and reading in data library(googlesheets) # For accessing the responses library(tidyverse) # For data cleaning and visualizing library(data.table) # Alternative to dplyr library(countrycode) # For handling country names library(gridExtra) # For visualizations # This key should allow anyone to access the raw results ice_cream_key &lt;- &quot;18uYxylZazzedLVzo4hSLEemQNKZ0uNse_V2aVyARvj8&quot; ice_cream &lt;- gs_key(ice_cream_key) ice_cream_responses &lt;- ice_cream %&gt;% gs_read(ws=&quot;Form Responses 1&quot;) setDT(ice_cream_responses) # using data.table instead of dplyr names(ice_cream_responses) &lt;- c(&quot;Timestamp&quot;,&quot;Age&quot;,&quot;Country&quot;,&quot;Method&quot;,&quot;Flavor&quot;) 56.3 Understanding what cleaning is required For country, respondents were allowed to input whatever they wanted. This caused issues with USA and Mexico, where the former was submitted in multiple formats and the latter included the name with and without accents. A bigger issue is the number of countries with only one respondent, indicating that we will need to combine responses in some way. ice_cream_responses[,.N,by=Country][order(-N)] ## Country N ## 1: India 9 ## 2: China 9 ## 3: USA 6 ## 4: Belgium 1 ## 5: Korea 1 ## 6: France 1 ## 7: Thailand 1 ## 8: Japan 1 ## 9: Indonesia 1 ## 10: México 1 ## 11: Bangladesh 1 ## 12: Usa 1 ## 13: Mexico 1 ## 14: United States 1 For flavor, again respondents could input any text. As expected, this caused many variations on the same flavors and even one misspelling. Even after accounting for this, the same problem remains of having many categories with few responses. The approach I will take for summarizing this will be to group similar flavors (e.g. chocolate chip and chocolate chip cookie dough). I consider myself somewhat of an ice cream expert (making it is a hobby of mine); I will leverage this knowledge in grouping ice cream flavors. ice_cream_responses[,.N,by=Flavor][order(-N)] ## Flavor N ## 1: Chocolate 11 ## 2: Vanilla 6 ## 3: Strawberry 2 ## 4: Chocolate chip cookie dough 1 ## 5: Mint Chocolate Chip 1 ## 6: vanilla 1 ## 7: chocalate 1 ## 8: Black Sesame 1 ## 9: Mint chocolate 1 ## 10: Vanilla macadamia 1 ## 11: Butter pecan 1 ## 12: Vanilla Chocolate Chip 1 ## 13: matcha 1 ## 14: Strawberry Cheesecake 1 ## 15: Chocolate Chip 1 ## 16: Mint 1 ## 17: Coffee 1 ## 18: Chocolate with nuts 1 ## 19: Mint Chocolate 1 Lastly, there are no issues with the data entry for age; however, we need to group it in some way as well. ice_cream_responses[order(Age),.N,by=Age] ## Age N ## 1: 21 2 ## 2: 22 16 ## 3: 23 3 ## 4: 24 3 ## 5: 25 5 ## 6: 26 2 ## 7: 28 1 ## 8: 33 1 ## 9: 34 1 ## 10: 38 1 56.4 Cleaning and prepping the data 56.4.1 Country We start by removing the accent in Mexico. Then, we can take care of two issues at once: duplicate country names and too many countries with small number of responses. By leveraging the countrycode package, we can group countries by continent. This leaves a small number of categories, which will ease our visualizations. Unfortunately, I did not have too many European respondents for this survey so I will create a secondary continent variable that groups them with the Americas. ice_cream_responses[,country_encoding := Encoding(Country)] ice_cream_responses[country_encoding==&quot;UTF-8&quot;, Country := iconv(Country,from=&quot;UTF-8&quot;,to=&quot;ASCII//TRANSLIT&quot;)] ice_cream_responses[,country_encoding := NULL] ice_cream_responses[,Continent := fct_infreq(countrycode(sourcevar= Country, origin= &quot;country.name&quot;, destination= &quot;continent&quot;))] ice_cream_responses[Continent==&quot;Asia&quot;, Continent2 := &quot;Asia&quot;] ice_cream_responses[Continent!=&quot;Asia&quot;, Continent2 := &quot;Americas\\nand Europe&quot;] ice_cream_responses[,Continent2 := fct_infreq(Continent2)] ice_cream_responses[,.N,by=Continent][order(-N)] ## Continent N ## 1: Asia 23 ## 2: Americas 10 ## 3: Europe 2 56.4.2 Flavor For flavor, we start by converting everything to lower case and making sure there is no extra whitespace. The approach for grouping flavors is to use regex to find specific strings and rename the flavors accordingly. Note the order used in “flavors_to_identify”. Some of these will be matched multiple times (e.g. choc and chocolate chip will match chocolate chip cookie dough). Therefore, I have ordered it so that the last match is the one I want to assign and will be the one used. Also, note the use of “choc” to deal with the mispelling of “chocolate” in one response. (This is a little bit of a quick workaround so as to not have to explicitly deal with the issue.) I again create a secondary flavor variable with only 3 categories to compare chocolate-based ice cream against vanilla-based. ice_cream_responses[,Flavor := str_squish(str_to_lower(Flavor))] flavors_to_identify &lt;- c(&#39;strawberry&#39;, &#39;choc&#39;, &#39;vanilla&#39;, &#39;chocolate chip&#39;, &#39;mint&#39;) flavor_names &lt;- c(&#39;Strawberry&#39;, &#39;Chocolate&#39;, &#39;Vanilla&#39;, &#39;Choc chip&#39;, &#39;Mint&#39;) flavor_names2 &lt;- c(&#39;Other&#39;, &#39;Chocolate&#39;, &#39;Vanilla&#39;, &#39;Vanilla&#39;, &#39;Other&#39;) for(i in seq(flavors_to_identify)){ ice_cream_responses[str_which(Flavor, flavors_to_identify[i]), `:=` (Flavor_group=flavor_names[i], Flavor_group2=flavor_names2[i])] } ice_cream_responses[is.na(Flavor_group), `:=` (Flavor_group=&quot;Other&quot;, Flavor_group2=&quot;Other&quot;)] ice_cream_responses[, `:=`(Flavor_group= fct_relevel(fct_infreq(Flavor_group),&quot;Other&quot;,after=Inf), Flavor_group2= fct_relevel(fct_infreq(Flavor_group2),&quot;Other&quot;,after=Inf))] ice_cream_responses[,.N,by=Flavor_group][order(-N)] ## Flavor_group N ## 1: Chocolate 13 ## 2: Vanilla 8 ## 3: Mint 4 ## 4: Other 4 ## 5: Choc chip 3 ## 6: Strawberry 3 56.4.3 Age This is quick to clean as we can just split into 2 groups (basically recent grads and those who have some work experience). ice_cream_responses[Age &lt; 23, Age_group := &quot;&lt;23&quot;] ice_cream_responses[Age &gt;= 23, Age_group := &quot;23+&quot;] ice_cream_responses[,Age_group := factor(Age_group, levels = c(&quot;&lt;23&quot;,&quot;23+&quot;))] 56.5 Visualizing the data 56.5.1 Getting an overview We start with a basic plot to understand the distribution of the transformed variables. Now using the simplified categories. 56.5.2 Ice cream preferences by continent and age Unforunately, even with the simplified bucketing of the variables, splitting the data by all 4 variables of interest reduces each category to a very small count. ice_cream_responses[,.N,by=.(Age_group,Continent2,Method,Flavor_group2)] ## Age_group Continent2 Method Flavor_group2 N ## 1: 23+ Americas\\nand Europe Cup Vanilla 3 ## 2: 23+ Asia Cup Other 2 ## 3: &lt;23 Asia Cup Vanilla 2 ## 4: &lt;23 Asia Cup Chocolate 2 ## 5: 23+ Asia Cone Other 4 ## 6: &lt;23 Asia Cone Chocolate 4 ## 7: 23+ Americas\\nand Europe Cone Vanilla 2 ## 8: &lt;23 Asia Cup Other 1 ## 9: &lt;23 Americas\\nand Europe Cone Chocolate 2 ## 10: &lt;23 Asia Cone Vanilla 2 ## 11: &lt;23 Americas\\nand Europe Cone Other 2 ## 12: 23+ Asia Cone Vanilla 1 ## 13: &lt;23 Asia Cone Other 2 ## 14: 23+ Asia Cone Chocolate 3 ## 15: &lt;23 Americas\\nand Europe Cone Vanilla 1 ## 16: 23+ Americas\\nand Europe Cone Chocolate 2 Therefore, we will need to compare 2 or 3 variables a at a time to find the interesting patterns. Even breaking down the responses by continent produces buckets that are a little too small for comparison. Still, it looks like respondents from Asia are more likely to favor “Other” flavor than the classics as compared to respondents from Americas/Europe. Additionally, &lt;23/Asia and 23+/Americas,Europe may like cups more than the others. However, maybe there is something else at play? Turns out people who like vanilla are more likely to also prefer a cup! Let’s dig in to age now Age doesn’t apear to play a major role in preferences. Maybe this isn’t too surprising since I chose an arbitrary split for the two groups! 56.6 Takeaways N = 35 is very small when you have 4 variables of interest with multiple categories each. Analyzing respones required bucketing each variable of interest into 2 or 3 categories. Chocolate and vanilla based ice creams are about equally preferred. Cones are much more popular than cups. Respondents who liked vanilla were more likely to prefer cups. Age did not play a major role in preferences, likely due to a small age range of 17 years. "],
["ask-a-manager-salary-survey-dataset.html", "Chapter 57 “Ask A Manager” salary survey dataset 57.1 Obtaining the dataset 57.2 Description of fields 57.3 Data cleanup process", " Chapter 57 “Ask A Manager” salary survey dataset Kliment Mamykin A survey posted in a popular blog “Ask A Manager”https://www.askamanager.org asked the readers to anonymously fill out how much money they were making. After the blog post was published, it went viral, and 31407 people took time and responded to the survey. A Google sheet was used to aggregate the responses. The data is interesting, since the self-reported compensation data is hard to come by, and useful for anyone on the job market. However the analysis of this dataset is made difficult by the lack of normalization in the responses. The survey uses freeform entered responses, with little standartization on the values allowed (probably to minimize the time/friciton of filling out the form). The goal for this project is to clean up the data and make it available for community for further analysis. Most of the work was done using OpenRefine, with the Industries list normalized with a machine learning model. The repository is https://github.com/kmamykin/askamanager_salary_survey 57.1 Obtaining the dataset Version 1 of the cleaned up dataset is located at (https://github.com/kmamykin/askamanager_salary_survey/raw/master/data/v1/Ask-A-Manager-Salary-Survey-2019.csv) salary_survey &lt;- readr::read_csv(&quot;https://github.com/kmamykin/askamanager_salary_survey/raw/master/data/v1/Ask-A-Manager-Salary-Survey-2019.csv&quot;) salary_survey %&gt;% drop_na(Industry) %&gt;% group_by(Industry) %&gt;% summarise(Freq = n()) %&gt;% ggplot(aes(x=fct_reorder(Industry, Freq), y=Freq)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;cornflowerblue&quot;) + geom_text(aes(label = Freq), position = position_dodge(width = 1), hjust = -0.2, size = 2) + coord_flip() + ylim(0, 3500) + xlab(&quot;Industry&quot;) + ylab(&quot;Responses&quot;) + ggtitle(&quot;Responses by Industry&quot;) + theme_bw() 57.2 Description of fields Timestamp - timestamp of the submittd response Age - age bracket: “18-24”, “25-34”, “35-44”, … One of the few fields that was a choice and did not need a cleanup Industry - This is the field that took the most time to normalize. While there are many different industry classification taxonomies, the industry list in https://www.webspidermount.com/features/generic-job-taxonomy/ looked short, simple and targeted for a job search domain. The values were normalized based on Industry (Original) using KNN classifier (see below for details). JobTitle - user entered field, clustered to remove small variations (e.g. “sr.engineer” vs “Sr Engineer”). There is still too much variation and this field need more normalization work. City, State, Country - these fields were normalized from Location (Original) field by calling Google Locations API and further manually tweaking the data inside OpenRefine. Location - normalized location in format “Nashville, TN, USA”. Only USA, Australia, Canada locations contain states in this dataset, for the other countries the field contains , , e.g. “Manchester, UK” Experience - experience in the responder, e.g. “8 - 10 years”, “11 - 20 years” etc. Also did not need normalization. Currency - currency of the Base pay, normalized from a freeform entry. Base - base salary (in specified Currency). This is the main metric. A lot of parsing and regex transforms went to this field to extract the numbers from freeform AnnualSalary (Original). Extra perks got separated into Extras field. For the records where it was detected that the compensation was at an hourly rate, the rate was extracted to HourlyRate and the Base was calculated with HourlyRate * 1650 hours/year (this is a big assumption here). You can filter those records out where HourlyRate is not NA. HourlyRate - extracted hourly rate from AnnualSalary (Original). Only some records contain values (hourly rate workers), otherwise the value is empty. Extras - extracted extra perks/comments from AnnualSalary (Original) Notes - user provided notes AnnualSalary (Original) - original field Location (Original) - original field JobTitle (Original) - original field Industry (Original) - original field Industry (Clustered) - Industry (Original) clustered with OpenRefine to remove small variations of spelling/capitalization str(salary_survey) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 31381 obs. of 22 variables: ## $ JobTitle : chr &quot;Talent Management Asst. Director&quot; &quot;Operations Director&quot; &quot;Market Research Assistant&quot; &quot;Senior Scientist&quot; ... ## $ Job Seniority : chr &quot;Director&quot; &quot;Director&quot; NA NA ... ## $ Job Area : chr &quot;Talent Management Asst.&quot; &quot;Operations&quot; NA NA ... ## $ Timestamp : chr &quot;4/24/2019 11:43:21&quot; &quot;4/24/2019 11:43:26&quot; &quot;4/24/2019 11:43:27&quot; &quot;4/24/2019 11:43:27&quot; ... ## $ Age : chr &quot;35-44&quot; &quot;25-34&quot; &quot;18-24&quot; &quot;25-34&quot; ... ## $ Industry : chr &quot;Government/Military&quot; &quot;Environmental&quot; &quot;Science/Research&quot; &quot;Pharmaceutical/Bio-tech&quot; ... ## $ Currency : chr &quot;USD&quot; &quot;USD&quot; &quot;USD&quot; &quot;GBP&quot; ... ## $ City : chr &quot;Nashville&quot; &quot;Madison&quot; &quot;Las Vegas&quot; &quot;Cardiff&quot; ... ## $ State : chr &quot;TN&quot; &quot;WI&quot; &quot;NV&quot; NA ... ## $ Country : chr &quot;USA&quot; &quot;USA&quot; &quot;USA&quot; &quot;UK&quot; ... ## $ Location : chr &quot;Nashville, TN, USA&quot; &quot;Madison, WI, USA&quot; &quot;Las Vegas, NV, USA&quot; &quot;Cardiff, UK&quot; ... ## $ Experience : chr &quot;11 - 20 years&quot; &quot;8 - 10 years&quot; &quot;2 - 4 years&quot; &quot;5 - 7 years&quot; ... ## $ Base : num 75000 65000 36330 34600 55000 ... ## $ HourlyRate : num NA NA NA NA NA NA NA NA NA NA ... ## $ Extras : chr NA NA NA NA ... ## $ Notes : chr NA NA NA NA ... ## $ JobTitle (Original) : chr &quot;Talent Management Asst. Director&quot; &quot;Operations Director&quot; &quot;Market Research Assistant&quot; &quot;Senior Scientist&quot; ... ## $ AnnualSalary (Original): chr &quot;75000&quot; &quot;65,000&quot; &quot;36,330&quot; &quot;34600&quot; ... ## $ AnnualSalary : chr &quot;75000&quot; &quot;65000&quot; &quot;36330&quot; &quot;34600&quot; ... ## $ Location (Original) : chr &quot;Nashville, TN&quot; &quot;Madison, Wi&quot; &quot;Las Vegas, NV&quot; &quot;Cardiff, UK&quot; ... ## $ Industry (Original) : chr &quot;Government&quot; &quot;Environmental nonprofit&quot; &quot;Market Research&quot; &quot;Biotechnology&quot; ... ## $ Industry (Clustered) : chr &quot;Government&quot; &quot;Environmental non-profit&quot; &quot;Market Research&quot; &quot;Biotechnology&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. JobTitle = col_character(), ## .. `Job Seniority` = col_character(), ## .. `Job Area` = col_character(), ## .. Timestamp = col_character(), ## .. Age = col_character(), ## .. Industry = col_character(), ## .. Currency = col_character(), ## .. City = col_character(), ## .. State = col_character(), ## .. Country = col_character(), ## .. Location = col_character(), ## .. Experience = col_character(), ## .. Base = col_double(), ## .. HourlyRate = col_double(), ## .. Extras = col_character(), ## .. Notes = col_character(), ## .. `JobTitle (Original)` = col_character(), ## .. `AnnualSalary (Original)` = col_character(), ## .. AnnualSalary = col_character(), ## .. `Location (Original)` = col_character(), ## .. `Industry (Original)` = col_character(), ## .. `Industry (Clustered)` = col_character() ## .. ) 57.3 Data cleanup process OpenRefine was used for most of the heavy lifting of data transformations. OpenRefine (previously Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data. 57.3.1 Industry classification The initial attempt to manually cleanup the original Industry (Original) field proved to be taking to long, and in the middle I decided to change the taxonomy. To automate this process a simple machine learning model was used to classify the input user entered industry into a target normalized set of industry taxonomy values. For both sets (the target industries and the input industries) I scraped Google search result and the resulting organic web sites, creating a rich text document describing each input or target industry. Using bag of words document representation and KNN classifier (k=1), each input industry was classified into a target industry based on their document vector similarity (using eucledian metric) See Processing.ipynb and Classification.ipynb for further details. 57.3.2 Job Title classification TODO - transform the freeform entered job titles into Job Seniority field and Job Area field, e.g. “Senior Data Scientist” would be transformed to “Senior” seniority and “Data Science” job area. 57.3.3 Contributing Contributions are welcome! If you notice an error in the dataset, or would like to add further cleanup - please follow this process: Download the dataset and create a project in OpenRefine. Perform desired data operations. Switch to “Undo/Redo” tab and “Extract…” transformation file in JSON. Submit your changes in a Pull Request with transformation file attached. "],
["forecast-of-the-2020-senate-election.html", "Chapter 58 Forecast of the 2020 senate election", " Chapter 58 Forecast of the 2020 senate election James DeAntonis In 2020, the US will hold its biannual senate election, featuring 35 seats up for grabs. The goal of this project is to project an expectation of the results. Overview of US government: As seen above, the US government is split up into three brances: the legislative, executive and judicial. The legislative brach is broken into two chambers: the senate and the house of representatives. The house of representatives is currently under solid democratic control and likely will not be flipped in 2020. We focus on the senate. Current breakdown of US senate: The current US senate sees a 53-47 Republican majority. As seen above, this republican majority is driven by high representation in the southeast and upper mountain states. Breakdown of seats up for re-election in 2020: In 2020, there is a disproportionately high number of republican seats up for grabs (23) compared to their democratic counterparts (12). We see this in the above map, which shows many seats up in the middle of the country. Above is a visualization of a Cook Report analysis, which broke out each incumbent and categorized its probability of retaining its seat (toss-up, lean, likely, solid). For example, we see that there are 12 incumbent republicans with solid chance of retaining seat and 8 such democrats. Similarly, three republicans are considered toss-ups to retain, compared to one democrat. The model used will treat each of these categories as independent binomial random variables and compute an expectation of the share of seats will be retained in the party. The model expectation points that most of the solid seats will be retained, the toss-ups will go 50/50, and the likely and lean seats will be in between. We see that, of all seats in the election, 20 will yield republican victories while 15 will yield democratic victories. Since the breakdown of incumbents in these seats is 23-12, this implies a three-seat gain for the democrats. In conclusion, the model projects a 50-50 party split after the 2020 senate election. "],
["intro-to-stringr-包入门详解.html", "Chapter 59 Intro to stringr 包入门详解 59.1 stringr 包的安装与调用 59.2 字符串匹配函数(Detect Matches) 59.3 字符串的截取函数(Subset Strings) 59.4 字符串长度编辑函数(Manage Lengths) 59.5 字符串变换与编辑函数(Mutate Strings) 59.6 字符串分割与拼接函数(Join and Split) 59.7 字符串排序(Order Strings) 59.8 字符串的编译格式与显示格式修改函数(Encode and Visualize Strings) 59.9 正则表达式(Regular Expression) 59.10 参考文献(Reference)", " Chapter 59 Intro to stringr 包入门详解 Yichi Zhang and Mingfang Chang In this file, we will make an introduction of functions in stringr package with detailed examples in Chinese. 59.1 stringr 包的安装与调用 59.1.1 安装 #从CRAN下载string 发行版本： install.packages(&quot;stringr&quot;) #从Github下载： devtools::install_github(&quot;tidyverse/stringr&quot;) 59.1.2 调用 library(stringr) 59.2 字符串匹配函数(Detect Matches) 59.2.1 str_detect(string, pattern) 检测字符串中是否包含匹配字符，返回TRUE或FALSE。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_detect(fruit, &quot;a&quot;) ## [1] TRUE TRUE TRUE TRUE str_detect(fruit, &quot;b&quot;) ## [1] FALSE TRUE FALSE FALSE str_detect(&quot;this is an example&quot;, &quot;an&quot;) ## [1] TRUE 59.2.2 str_which(string, pattern) 查找字符串中匹配字符的索引，返回索引。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_which(fruit, &quot;a&quot;) ## [1] 1 2 3 4 str_which(fruit, &quot;p&quot;) ## [1] 1 3 4 str_which(&quot;example&quot;, &quot;a&quot;) ## [1] 1 59.2.3 str_count(string, pattern) 计数字符串匹配次数，返回计数。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_count(fruit, &quot;a&quot;) ## [1] 1 3 1 1 str_count(fruit, &quot;p&quot;) ## [1] 2 0 1 3 str_count(&quot;example&quot;, &quot;am&quot;) ## [1] 1 59.2.4 str_locate(string, pattern) 找到字符串中第一个匹配字符的位置，返回位置，如无匹配则返回NA。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_locate(fruit, &quot;a&quot;) ## start end ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 5 5 str_locate(fruit, &quot;p&quot;) ## start end ## [1,] 2 2 ## [2,] NA NA ## [3,] 1 1 ## [4,] 1 1 str_locate(&quot;example&quot;, &quot;x&quot;) ## start end ## [1,] 2 2 59.2.5 str_locate_all(string, pattern) 找到字符串中所有匹配字符的位置，返回位置。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_locate_all(fruit,&quot;p&quot;) ## [[1]] ## start end ## [1,] 2 2 ## [2,] 3 3 ## ## [[2]] ## start end ## ## [[3]] ## start end ## [1,] 1 1 ## ## [[4]] ## start end ## [1,] 1 1 ## [2,] 6 6 ## [3,] 7 7 str_locate_all(&quot;example&quot;, &quot;e&quot;) ## [[1]] ## start end ## [1,] 1 1 ## [2,] 7 7 59.3 字符串的截取函数(Subset Strings) 59.3.1 str_sub(string, start index, end index) 用于直接通过索引来分割字符串，并返回起始索引到结束索引中的全部字符。 示例： x &lt;-&#39;Hello World&#39; str_sub(x,1,3) ## [1] &quot;Hel&quot; str_sub(x,-7,-2) ## [1] &quot;o Worl&quot; 函数的输入也可以是一串字符串。 x &lt;-c(&#39;Apple&#39;,&#39;Banana&#39;,&#39;Strawberry&#39;) str_sub(x,2,4) ## [1] &quot;ppl&quot; &quot;ana&quot; &quot;tra&quot; 59.3.2 str_subset(string,pattern) 输入一个字符串或者一系列字符串，并给定一种模式，返还拥有这种模式的字符串。 示例： x &lt;-&#39;Hello World&#39; str_subset(x,&#39;ell&#39;) ## [1] &quot;Hello World&quot; y &lt;-c(&#39;Apple&#39;,&#39;Banana&#39;,&#39;Strawberry&#39;) str_subset(y,&#39;ana&#39;) ## [1] &quot;Banana&quot; z &lt;-&#39;Hello World&#39; str_subset(x,&#39;zzz&#39;) ## character(0) 59.3.3 str_extract(string,pattern) 输入一个字符串或者一串字符串，给定一种模式,返回在每个字符串中第一个符合模式的子字符串，如果某个字符串中并没有我们输入的模式，则返回空值NA，这个函数的输出类型是一个包含多个字符串的向量。 示例： string1 &lt;- c(&#39;fruit&#39;, &#39;Test Score:89&#39;, &#39;Test Score:170&#39;,&#39;tiger&#39;) pattern1 &lt;-&#39;ui&#39; str_extract(string1,pattern1) ## [1] &quot;ui&quot; NA NA NA 对于这个函数使用正则表达式来表达字符串的模式会更有意义，我们将会在最后具体讨论正则表达式的写法。 pattern2 &lt;- &quot;[A-Z][a-z]*[:]\\\\d*&quot; str_extract(string1,pattern2) ## [1] NA &quot;Score:89&quot; &quot;Score:170&quot; NA 我们也可以使用函数str_extract_all(string， pattern) 返回每个字符串当中所有符合的模式的子字符串（并不只是第一个）。 示例： string2 &lt;- c(&#39;fruit&#39;, &#39;Test Score:89 Ratescore:77&#39;, &#39;Test Score:170&#39;,&#39;tiger&#39;) pattern2 &lt;- &quot;([A-Z][a-z]*[:])(\\\\d*)&quot; str_extract_all(string2,pattern2) ## [[1]] ## character(0) ## ## [[2]] ## [1] &quot;Score:89&quot; &quot;Ratescore:77&quot; ## ## [[3]] ## [1] &quot;Score:170&quot; ## ## [[4]] ## character(0) 59.3.4 str_match(string, pattern) 输入一个字符串或者一串字符串，给定一种模式,返回在每个字符串中第一个符合模式的子字符串，如果某个字符串中并没有我们输入的模式，则返回空值NA,但是不同于str_extract()函数， 此函数返回一个包含多个字符串的矩阵。 示例： str_match(string2,pattern2) ## [,1] [,2] [,3] ## [1,] NA NA NA ## [2,] &quot;Score:89&quot; &quot;Score:&quot; &quot;89&quot; ## [3,] &quot;Score:170&quot; &quot;Score:&quot; &quot;170&quot; ## [4,] NA NA NA 我们也可以用str_match_all()函数去返回所有符合模式的子字符串,在返回的矩阵中，第一列代表一个符合模式的完整的子字符串，后面的每一列代表了完整的子字符串中的每一个组，这个组是在正则表达式中用括号分割开来的（例如“([A-Z][a-z][:])(\\d)” 中， 第一个组就是([A-Z][a-z][:])， 第二个组则是(\\d)）。 str_match_all(string2,pattern2) ## [[1]] ## [,1] [,2] [,3] ## ## [[2]] ## [,1] [,2] [,3] ## [1,] &quot;Score:89&quot; &quot;Score:&quot; &quot;89&quot; ## [2,] &quot;Ratescore:77&quot; &quot;Ratescore:&quot; &quot;77&quot; ## ## [[3]] ## [,1] [,2] [,3] ## [1,] &quot;Score:170&quot; &quot;Score:&quot; &quot;170&quot; ## ## [[4]] ## [,1] [,2] [,3] 59.4 字符串长度编辑函数(Manage Lengths) 59.4.1 str_length(string) 返回字符串长度，即所含字符个数。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_length(fruit) ## [1] 5 6 4 9 str_length(&quot;example&quot;) ## [1] 7 str_length(&quot; &quot;) ## [1] 1 str_length(&quot;&quot;) ## [1] 0 59.4.2 str_pad((string, width, side = c(“left”, “right”,“both”), pad = &quot; &quot;) 用所给字符填充字符串至所给长度，可总左边或右边或两边填充，返回填充后字符串。默认值为从左边以空格填充至所给长度。如果所给长度比字符串长度短，则返回原字符串。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_pad(fruit, 17) ## [1] &quot; apple&quot; &quot; banana&quot; &quot; pear&quot; ## [4] &quot; pineapple&quot; str_pad(fruit, 17, &quot;both&quot;, &quot;*&quot;) ## [1] &quot;******apple******&quot; &quot;*****banana******&quot; &quot;******pear*******&quot; ## [4] &quot;****pineapple****&quot; str_pad(fruit, 17, &quot;left&quot;, &quot; &quot;) ## [1] &quot; apple&quot; &quot; banana&quot; &quot; pear&quot; ## [4] &quot; pineapple&quot; str_pad(fruit, 17, &quot;right&quot;, &quot;-&quot;) ## [1] &quot;apple------------&quot; &quot;banana-----------&quot; &quot;pear-------------&quot; ## [4] &quot;pineapple--------&quot; str_pad(&quot;example&quot;, 11, &quot;both&quot;) ## [1] &quot; example &quot; str_pad(&quot;another&quot;, 3, &quot;both&quot;,&quot;*&quot;) ## [1] &quot;another&quot; 59.4.3 str_trunc(string, width, side = c(“right”, “left”,“center”), ellipsis = “…”) 从所给方向截取字符串并替换为所给字符至所给长度，可以从左边或右边或中间截取，默认为从左边截取，默认替换字符为“…”，返回编辑后字符串。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_trunc(fruit, 3,&quot;left&quot;, &quot;-&quot;) ## [1] &quot;-le&quot; &quot;-na&quot; &quot;-ar&quot; &quot;-le&quot; str_trunc(fruit, 4,&quot;center&quot;, &quot;*&quot;) ## [1] &quot;ap*e&quot; &quot;ba*a&quot; &quot;pear&quot; &quot;pi*e&quot; str_trunc(fruit, 5,&quot;right&quot;, &quot;%&quot;) ## [1] &quot;apple&quot; &quot;bana%&quot; &quot;pear&quot; &quot;pine%&quot; str_trunc(&quot;example&quot;, 5) ## [1] &quot;ex...&quot; 59.4.4 str_trim(string, side = c(“both”, “left”, “right”)) 去掉字符串开头或/和结尾的空格，可以去掉左边或右边或两边的空格，默认为去掉两边空格，返回去掉空格后的字符串。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_trim(fruit) ## [1] &quot;apple&quot; &quot;banana&quot; &quot;pear&quot; &quot;pineapple&quot; str_trim(&quot; example1 &quot;) ## [1] &quot;example1&quot; str_trim(&quot; example2 &quot;, &quot;left&quot;) ## [1] &quot;example2 &quot; str_trim(&quot; example3 &quot;, &quot;right&quot;) ## [1] &quot; example3&quot; str_trim(&quot; example4 &quot;, &quot;both&quot;) ## [1] &quot;example4&quot; 59.5 字符串变换与编辑函数(Mutate Strings) 59.5.1 str_sub(string,start index,end index) 可用于返回所有输入的字符串中，从起始索引到结束索引的子字符串。 示例： f &lt;-c(&#39;Apple&#39;,&#39;Banana&#39;,&#39;Strawberry&#39;) str_sub(f,1,3) ## [1] &quot;App&quot; &quot;Ban&quot; &quot;Str&quot; 也可用于替换1所有输入的字符串中，从起始索引到结束索引的子字符串，用于替换的字符串取决的使用者自己定义并指向这个函数的字符串。 示例： f &lt;-c(&#39;Apple&#39;,&#39;Banana&#39;,&#39;Strawberry&#39;) str_sub(f,1,3)&lt;-&#39;hello&#39; print(f) ## [1] &quot;hellole&quot; &quot;helloana&quot; &quot;helloawberry&quot; 59.5.2 str_replace(string,pattern,replacement) 找个输入法的字符串中第一个符合我们定义的模式的子字符串并用我们输入的替换字符串替换。 示例： string3 &lt;- &#39;exploratary data annlysis&#39; str_replace(string3,&#39;exploratary&#39;,&#39;Visulization&#39;) ## [1] &quot;Visulization data annlysis&quot; string2 &lt;- c(&#39;fruit&#39;, &#39;Test Score:89 Ratescore:77&#39;, &#39;Test Score:170&#39;,&#39;tiger&#39;) pattern2 &lt;- &quot;([A-Z][a-z]*[:])(\\\\d*)&quot; str_replace(string2,pattern2,&#39;Replacement&#39;) ## [1] &quot;fruit&quot; &quot;Test Replacement Ratescore:77&quot; ## [3] &quot;Test Replacement&quot; &quot;tiger&quot; 59.5.3 str_replace_all(string,pattern,replacement) 找个输入法的字符串中所有符合我们定义的模式的子字符串并用我们输入替换字符串替换,如果这个字符串中不存在我们定义的模式，直接返还原本的字符串。 示例： string3 &lt;- &#39;exploratary data annlysis and data Visulization&#39; str_replace_all(string3,&#39;data&#39;,&#39;Visulization&#39;) ## [1] &quot;exploratary Visulization annlysis and Visulization Visulization&quot; string2 &lt;- c(&#39;fruit&#39;, &#39;Test Score:89 Ratescore:77&#39;, &#39;Test Score:170&#39;,&#39;tiger&#39;) pattern2 &lt;- &quot;([A-Z][a-z]*[:])(\\\\d*)&quot; str_replace_all(string2,pattern2,&#39;Replacement&#39;) ## [1] &quot;fruit&quot; &quot;Test Replacement Replacement&quot; ## [3] &quot;Test Replacement&quot; &quot;tiger&quot; 59.5.4 str_to_lower(string) 将一个字符串或者一个字符串的向量全部变为小写格式。 示例： upper &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) str_to_lower(upper) ## [1] &quot;fff&quot; &quot;task&quot; &quot;community contribution&quot; 由于不同的国家有不同的大小写规则，我们可以用参数locale去规定在这个小写转变格式中所使用的规则（每个国家的规则有国家的英文缩写表示，具体课查看维基百科https://zh.wikipedia.org/wiki/ISO_639-1%E4%BB%A3%E7%A0%81%E8%A1%A8 语言ISO639表格）。 upper &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) str_to_lower(upper,locale = &#39;en&#39;) ## [1] &quot;fff&quot; &quot;task&quot; &quot;community contribution&quot; 59.5.5 str_to_upper(string) 将一个字符串或者一个字符串的向量全部变为大写格式。 示例： lower1 &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) str_to_upper(lower1) ## [1] &quot;FFF&quot; &quot;TASK&quot; &quot;COMMUNITY CONTRIBUTION&quot; str_to_upper(lower1,locale = &#39;en&#39;) ## [1] &quot;FFF&quot; &quot;TASK&quot; &quot;COMMUNITY CONTRIBUTION&quot; 59.5.6 str_to_title(string) 将一个字符串或者一个字符串组成的向量变换称标题格式（每个单词的首字母大写）。 示例： str_to_title(lower1) ## [1] &quot;Fff&quot; &quot;Task&quot; &quot;Community Contribution&quot; str_to_title(lower1,locale = &#39;en&#39;) ## [1] &quot;Fff&quot; &quot;Task&quot; &quot;Community Contribution&quot; 59.6 字符串分割与拼接函数(Join and Split) 59.6.1 str_c(…, sep = &quot;&quot;, collapse = NULL) 将多个字符串拼接成单个字符串，字符串间可添加分割字符，默认分割字符为空字符，返回拼接后字符串。 示例： str_c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example1&quot;, sep = &quot;&quot;, collapse = NULL) ## [1] &quot;thisisanexample1&quot; str_c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example2&quot;, sep = &quot; &quot;, collapse = NULL) ## [1] &quot;this is an example2&quot; str_c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example3&quot;) ## [1] &quot;thisisanexample3&quot; 59.6.2 str_c(…, sep = &quot;“, collapse =”&quot;) 将一个字符串向量拼接为单个字符串，字符串间可添加分割字符，默认分割字符为空字符，返回拼接后字符串。 示例： str_c(c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example1&quot;), sep = &quot;&quot;, collapse = &quot;&quot;) ## [1] &quot;thisisanexample1&quot; str_c(c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;example2&quot;), sep = &quot;&quot;, collapse = &quot;*&quot;) ## [1] &quot;this*is*an*example2&quot; 59.6.3 str_dup(string, times) 多次复制字符串，返回复制后字符串。 示例： fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_dup(fruit, 3) ## [1] &quot;appleappleapple&quot; &quot;bananabananabanana&quot; ## [3] &quot;pearpearpear&quot; &quot;pineapplepineapplepineapple&quot; str_dup(&quot;example&quot;, 7) ## [1] &quot;exampleexampleexampleexampleexampleexampleexample&quot; 59.6.4 str_split_fixed((string, pattern, n) 以所给字符将字符串分割成所给数量的字符串，返回分割后的结果。 示例： fruit &lt;- c(&quot;app le&quot;, &quot;bana na&quot;, &quot;p ear&quot;, &quot;pinea pple&quot;) str_split_fixed(fruit, &quot; &quot;, 2) ## [,1] [,2] ## [1,] &quot;app&quot; &quot;le&quot; ## [2,] &quot;bana&quot; &quot;na&quot; ## [3,] &quot;p&quot; &quot;ear&quot; ## [4,] &quot;pinea&quot; &quot;pple&quot; str_split_fixed(&quot;this is an example2&quot;, &quot; &quot;, 3) ## [,1] [,2] [,3] ## [1,] &quot;this&quot; &quot;is&quot; &quot;an example2&quot; 59.6.5 str_glue(…, .sep = &quot;&quot;, .envir = parent.frame()) 在字符串内替换变量，变量可在函数内定义，可以连接多个字符串并以所给分隔字符分隔，返回替换后字符串。 示例： str_glue(&quot;This is {apple}&quot;, &quot; and this is not {banana}.&quot;, .sep=&quot;,&quot;,apple = &quot;apple&quot;, banana = &quot;banana&quot;) ## This is apple, and this is not banana. pineapple&lt;-&quot;pineapple&quot; str_glue(&quot;This is {pineapple}&quot;) ## This is pineapple 59.6.6 str_glue_data(.x, …, .sep = &quot;“, .envir = parent.frame(), .na =”NA&quot;) 在字符串内替换变量，变量可在函数内以环境、列表、数据框等形式定义，可以连接多个字符串并以所给分隔字符分隔，返回替换后字符串。 示例： str_glue_data(list(a=&quot;apple&quot;, b=&quot;banana&quot;), &quot;This is {a}&quot;, &quot;this is not {b}&quot;, .sep=&quot;,&quot;) ## This is apple,this is not banana fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) 59.7 字符串排序(Order Strings) 59.7.1 str_sort(string) 根据输入的参数，给输入的字符串向量排序。 示例： order &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) #给名叫order的字符串向量按照字母顺序从前到后排序 str_sort(order) ## [1] &quot;Community CONTRIBUTION&quot; &quot;FFF&quot; &quot;Task&quot; #给名叫order的字符串向量按照字母顺序从后到前排序 str_sort(order,decreasing = TRUE) ## [1] &quot;Task&quot; &quot;FFF&quot; &quot;Community CONTRIBUTION&quot; orderNA &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;,NA) #给名叫orderNA的字符串向量按照字母顺序从前到后的排序并且把空值排在最后面 str_sort(orderNA,na_last = TRUE) ## [1] &quot;Community CONTRIBUTION&quot; &quot;FFF&quot; &quot;Task&quot; ## [4] NA 59.7.2 str_order(string) 此函数与str_sort()的排序方法和使用方法基本一致，两个函数的主要区别在于str_order()返回的是索引的顺序而不是一个排序过后的新字符串向量。 示例： order2 &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) str_sort(order2) ## [1] &quot;Community CONTRIBUTION&quot; &quot;FFF&quot; &quot;Task&quot; str_order(order2) ## [1] 3 1 2 order2 &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;) #排序的顺序变为字母表的顺序从后到前 str_sort(order2,decreasing = TRUE) ## [1] &quot;Task&quot; &quot;FFF&quot; &quot;Community CONTRIBUTION&quot; str_order(order2, decreasing = TRUE) ## [1] 2 1 3 orderNA &lt;- c(&#39;FFF&#39;,&#39;Task&#39;,&#39;Community CONTRIBUTION&#39;,NA) #将NA排在所有字符串的最后面 str_sort(orderNA,na_last = TRUE) ## [1] &quot;Community CONTRIBUTION&quot; &quot;FFF&quot; &quot;Task&quot; ## [4] NA str_order(orderNA, na_last =TRUE) ## [1] 3 1 2 4 59.8 字符串的编译格式与显示格式修改函数(Encode and Visualize Strings) 59.8.1 str_conv(string, encoding) 更改当前字符串的编码格式。 示例： x &lt;- charToRaw(&#39;武汉欢迎你&#39;) #将武汉欢迎你变成字符节 print(x) ## [1] e6 ad a6 e6 b1 89 e6 ac a2 e8 bf 8e e4 bd a0 #将原始的字符节x用UTF-8的编码格式从新编码 str_conv(x, &quot;UTF-8&quot;) ## [1] &quot;武汉欢迎你&quot; #使用stringi::stri_enc_list()能返回整个r中完整的编码格式表 59.8.2 str_view(string, pattern) 通过html的形式显示我们输入的模式在字符串中第一个出现的位置的位置。 示例： str_view(&quot;hello world&quot;, &quot;world&quot;) str_view(&quot;hello world world&quot;, &quot;world&quot;) 我们同时也可以使用函数str_view_all(string, pattern)去显示我们输入的字符串中所有和我们所选择的模式匹配的子字符串的位置。 示例： str_view_all(&quot;hello world world&quot;, &quot;world&quot;) str_view_all(&#39;Test Score:89 Ratescore:77&#39;,&quot;([A-Z][a-z]*[:])(\\\\d*)&quot;) 59.8.3 str_wrap(string,width,indent,exdent) 改变字符串的显示格式。 示例： data &lt;- &#39;Data analysis is a process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today\\&#39;s business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.&#39; # the text above come from wikipedia #width规定了字符串每一行的长度，这里我们让字符串data每隔50个字符就换一次行 cat(str_wrap(data, width = 50, indent = 0, exdent = 0),&#39;\\n&#39;) ## Data analysis is a process of inspecting, ## cleansing, transforming and modeling data with the ## goal of discovering useful information, informing ## conclusion and supporting decision-making. Data ## analysis has multiple facets and approaches, ## encompassing diverse techniques under a variety of ## names, and is used in different business, science, ## and social science domains. In today&#39;s business ## world, data analysis plays a role in making ## decisions more scientific and helping businesses ## operate more effectively. #indent决定了字符串第一行的缩进长度，这里我们选择缩进长度10，也就是第一行在一个空格之后开始 cat(str_wrap(data, width = 50, indent = 10, exdent = 0),&#39;\\n&#39;) ## Data analysis is a process of inspecting, ## cleansing, transforming and modeling data with the ## goal of discovering useful information, informing ## conclusion and supporting decision-making. Data ## analysis has multiple facets and approaches, ## encompassing diverse techniques under a variety of ## names, and is used in different business, science, ## and social science domains. In today&#39;s business ## world, data analysis plays a role in making ## decisions more scientific and helping businesses ## operate more effectively. #exdent决定了字符串除了第一行所有其他行的缩进长度，这里我们选择缩进长度15，也就是除了第一行，每行前面都有15个空格 cat(str_wrap(data, width = 50, indent = 0, exdent = 15),&#39;\\n&#39;) ## Data analysis is a process of inspecting, ## cleansing, transforming and modeling data with the ## goal of discovering useful information, informing ## conclusion and supporting decision-making. Data ## analysis has multiple facets and approaches, ## encompassing diverse techniques under a variety of ## names, and is used in different business, science, ## and social science domains. In today&#39;s business ## world, data analysis plays a role in making ## decisions more scientific and helping businesses ## operate more effectively. 59.9 正则表达式(Regular Expression) 59.9.1 字符匹配 在stringr 函数中，pattern参数可以正则表达式的形式表示。在R语言中，正则表达式以字符串形式表示。 ‘a’ 在正则表达式中单个字母代表的就是一个字母本身 \\.   \\?   \\(   \\}  等 当我们想表示一个符号的时候，以符号本身表示一个符号并在前面加一个斜杠 \\n 代表换行 \\t 代表一个缩进，即跳格 \\s 代表任意空格 \\S 代表非空格字符 \\d 代表任意数字字符 \\D 代表任意非数字字符 \\w 代表任意字字符 \\W 代表任意非字字符 \\b 代表词边界 \\B 代表非词边界 [:digit]  代表0-9之间的任意数字 [:alpha]  代表a-z,和A-Z之间的任意字母 [:lower:]  代表a-z之间的任意字母 [:upper:]  代表A-Z之间的任意字母 [:xdigit:]  代表16进制中任意的数字(0-9和A-F) [:alumn:]  代表任意数字或字母 [:graph:]  代表任意数字,字母,或者符号 [:print:]  代表任意数字,字母,符号,或者空格 [:spcae:]  代表空格,用法与\\s相同 [:blank:]  代表空格或者跳格(缩进)但不包括换行 .  代表任意字符除了换行 由于很多常规字符不能直接以字符串的形式来表达，所以我们需要在带有斜杠的正则表达式前面额外加上一个斜杠，使相对应的正则表达式能以字符串的形式储存 例如： \\n 在字符串中的表达为’\\\\n’ \\s 在字符串中的表达为’\\\\s’ \\w 在字符串中的表达为’\\\\w’ 等等 59.9.2 替换(Alternates) ‘|’ 表示或者(or)， 例如 ‘ab|d’表示’ab’ 或’d’ 都能被匹配。 #&#39;wk|kkk&#39;表示字符串中所有的&#39;wk&#39; 或&#39;kkk&#39; 都能被匹配。 str_view_all(&#39;w.asdgwkad.dfkkk.qwwerthj&#39;,&#39;wk|kkk&#39;) ‘[ ]’ 表示其中一个，例如’[abd]’表示任意’a’或’b’或’d’都能被匹配。 #&#39;[wk]&#39;表示字符串中所有的&#39;w&#39; 或&#39;k&#39; 都能被匹配。 str_view_all(&#39;w.asdgwkad.dfkkk.qwwerthj&#39;,&#39;[wk]&#39;) ‘[^]’ 表示除此之外，例如’[^abd]’ 表示除’abd’之外的的子字符串都能被匹配。 #&#39;[^wk]&#39;表示字符串中除了&#39;w&#39; 和&#39;k&#39; ，其他所有子字符串都能被匹配。 str_view_all(&#39;w.asdgwkad.dfkkk.qwwerthj&#39;,&#39;[^wk]&#39;) ‘[-]’ 表示范围，例如’[a-c]‘表示从a至c，即单个子字符串’a’,’b’或者’c’能被匹配。 #&#39;[a-f]&#39;表示字符串中所有的从a到f的单个子字符串都能被匹配。 str_view_all(&#39;w.asdgwkad.dfkkk.qwwerthj&#39;,&#39;[a-f]&#39;) 59.9.3 锚点(Anchors) ‘^’ 表示字符串开头，例如’^a’表示以a开头的字符串为有效匹配。 #匹配字符串开头为j的首字母，如果字符串的首字母不是j，则为无有效匹配。 str_view_all(&#39;jdjdjdjdj&#39;,&#39;^j&#39;) str_view_all(&#39;djdjdjdj&#39;,&#39;^j&#39;) ‘$’ 表示字符串结尾，例如’a$’表示以a结尾的字符串为有效匹配。 #匹配字符串尾字母j，如果字符串的最后一个字母不是j，则为无有效匹配。 str_view_all(&#39;jdjdjdjdj&#39;,&#39;j$&#39;) str_view_all(&#39;djdjdjdj&#39;,&#39;j$&#39;) 59.9.4 查找(Look Arounds) ‘char1(?=char2)’ 表示匹配char1后一个为char2的char1字符，例如‘a(?=c)’表示匹配a后一个为c的a。 #匹配后一个字符为n的u str_view_all(&#39;Columbia university&#39;,&#39;u(?=n)&#39;) ‘char1(?!char2)’ 表示匹配char1后一个不为char2的char1字符，例如‘a(?!c)’表示匹配a后一个不为c的a。 #匹配后一个字符为不为n的u str_view_all(&#39;Columbia university&#39;,&#39;u(?!n)&#39;) ‘(?&lt;=char2)char1’ 表示匹配char1前一个为char2的char1字符，例如’(?&lt;=b)a’表示匹配a前一个为b的a。 #匹配前一个字符为空格的u str_view_all(&#39;Columbia university&#39;,&#39;(?&lt;= )u&#39;) ‘(?&lt;!char2)char1’ 表示匹配char1前一个不为char2的char1字符，例如’(?&lt;!b)a’表示匹配a前一个不为b的a #匹配前一个字符不为空格的u str_view_all(&#39;Columbia university&#39;,&#39;(?&lt;! )u&#39;) 59.9.5 数量词的使用(Quantifiers) ? 代表前一个正则表达式出现零次或者一次 #寻找字符串ab中有所有由一个a或者空字符组成的模式(pattern) str_view_all(&#39;ab&#39;,&#39;a?&#39;) #寻找字符串.wkwkwkk中有所有由一个k或者空字符组成的模式(pattern) str_view_all(&#39;.wkwkwkk&#39;,&#39;k?&#39;) * 代表前一个正则表达式出现零次或多次 #寻找字符串abaa中所有由一个a或者多个a或者空字符组成的模式(pattern) str_view_all(&#39;abaa&#39;,&#39;a*&#39;) #寻找字符串.wkkkkwkwkk中所有由一个k或者多个k或者空字符组成的模式(pattern) str_view_all(&#39;.wkkkkwkwkk&#39;,&#39;k*&#39;) #可以看到k*和k？的主要区别在于k？会把‘kkkk’当作4个单个k组成的模式(pattern),而k*会把字符串&#39;kkkk&#39;视为一个由4个k组成的模式(pattern) + 代表前一个正则表达式出现一次或多次 #寻找字符串abaa中有所有由一个a或者多个a组成的模式(pattern) str_view_all(&#39;abaa&#39;,&#39;a+&#39;) #寻找字符串.wkkkkwkwkk中所有由一个k或者多个k组成的模式(pattern) str_view_all(&#39;.wkkkkwkwkk&#39;,&#39;k+&#39;) #数量词加号不再将空字符视为一个模式(pattern),加号匹配前一个字符串出现至少一次的模式(pattern) {n} 代表前一个正则表达式出现n次 #a{n} 将会匹配字符串中连续出现了n个a的模式(pattern) #寻找字符串abaa中所有由2个a组成的模式(pattern) str_view_all(&#39;abaa&#39;,&#39;a{2}&#39;) #寻找字符串.wkkkkwkwkk中所有由4个k组成的模式(pattern) str_view_all(&#39;.wkkkkwkwkk&#39;,&#39;k{4}&#39;) {n,} 代表前一个正则表达式出现n次或者更多 #a{n,} 将会匹配字符串中连续出现了n个a或者更多的模式(pattern) #寻找字符串abaaab.bbaaaa.aaa中所有由2个或2个以上a组成的模式(pattern) str_view_all(&#39;abaab.bbaaaa.aaa&#39;,&#39;a{2,}&#39;) #寻找字符串.wkkkkwkwkk.32.qwdsfkkkkkkkk.wd中所有由4个或者4个以上k组成的模式(pattern) str_view_all(&#39;.wkkkkwkwkk.32.qwdsfkkkkkkkk.wd&#39;,&#39;k{4,}&#39;) {n,m} 代表前一个正则表达式出现的次数在n到m之间(包括n和m) #a{n,m} 将会匹配字符串中连续出现了n到m个a的模式(pattern) #寻找字符串abaaab.bbaaaa.aaa中所有由2到4个a组成的模式(pattern) str_view_all(&#39;abaab.bbaaaa.aaa&#39;,&#39;a{3,4}&#39;) #寻找字符串.wkkkkwkwkk.32.qwdsfkkkkkkkk.wd中所有由4到6个k组成的模式(pattern) str_view_all(&#39;.wkkkkwkwkk.32.qwdsfkkkkkkkk.wd&#39;,&#39;k{4,6}&#39;) 59.9.6 括号划分表达式并用转义号码替换 ( ) 将表达式中的一个部分用括号包含代表着括号中的子表达式自成一个组，系统在匹配表达式时,也会根据括号的顺序来匹配 例如: (a)(bb)(cab) 此正则表达式中，第一组子表达式是a,第二组子表达式是bb,第三组表达式是cab,系统在匹配的时候,匹配的循序也就是先匹配一个a,然后两个b,最后再是cab 我们可以通过在数字前加上双斜杠来直接表示对应数字的子表达式 例如: \\\\1 在正则表达式中代表第一组子正则表达式 \\\\\\3 在正则表达式中代表第三组子正则表达式 等等 表达式 ‘(a)(b)(c)(c)(b)(a)’ 和 ‘(a)(b)(c)\\3\\2\\1’是相同的 表达式’(aab)(cde)(c)(aab)(c)(cde)‘和’(aab)(cde)(c)\\1\\3\\2’是相同的 str_view_all(&quot;abccbaab&quot;, &#39;(a)(b)(c)\\\\3\\\\2\\\\1&#39;) str_view_all(&quot;abccbaab&quot;, &#39;(a)(b)(c)(c)(b)(a)&#39;) 59.10 参考文献(Reference) http://edrub.in/CheatSheets/cheatSheetStringr.pdf "],
["likert-package.html", "Chapter 60 Likert package", " Chapter 60 Likert package Lingrui Luo and Zijing Wang 工具包 ‘likert’ 中文翻译版本 Chinese Version 类型   工具包 标题   分析与可视化李克特(likert)选项 版本   1.3.5 日期   2016-12-26 作者   Jason Bryer &lt;jason@bryer.org&gt;,Kimberly Speerschneider &lt;kimspeer@gmail.com&gt; 维护者   Jason Bryer &lt;jason@bryer.org&gt; 网址   http://jason.bryer.org/likert, http://github.com/jbryer/likert 错误报告  https://github.com/jbryer/likert/issues 描述说明  一种着重与可视化的分析李克特(likert)选项的方法。堆栈条形图(stacked bar plot)是展现李克特数据结果的优先选择。表格数据结果也可以与密度图相结合，帮助研究者决定李克特选项的回答是否可以进行定量使用而不是定性使用。以下函数可以作为一个好的开篇：likert(), summary.likert(), plot.likert(). 许可证   GPL 延迟加载  是 插图生成器  utils 工具   R (&gt;=3.0), ggplot2, xtable 导入工具包  psych, reshape2, gridExtra, grid, plyr 建议工具包   devtools, shiny RoxygenNote   5.0.1 需要汇编   不需要 知识库   CRAN 日期/发布日期  2016-12-31 01:37:00 索引 likert-package         李克特分析与可视化 描述     李克特分析与可视化 作者     &lt;jason@bryer.org&gt; abs_formatter         连续性数值的绝对值格式 描述     打印出坐标标签的绝对值。对于堆栈条形图中以负数表示负分组的非负百分比十分有用。 使用方式     abs_formatter(x) 参数 - x     需要被改变格式的值 数值     x的绝对值 align.plots 改编于ggExtra工具包(该工具包已不可以用)。 与实验性mlpsa图有关，mlpsa图将两个单独分布结合在环形图(circular plot)中 描述     改编于ggExtra工具包(该工具包已不可以用)。与实验性mlpsa图有关，mlpsa图将两个单独分布结合在环形图(circular plot)中 使用方式     align(g1,...) 参数 - g1     grid.layout - ...     需要添加的其他画图元素 参考     http://groups.google.com/group/ggplot2/browse_thread/thread/1b859d6b4b441c90 gap     虚构数据集，数据机包括五个不同的办公室的重要程度与满意度结果。 描述     此数据集用于GapAnalysis样本中，用于展示likert工具包如何处理差距分析 格式     68行观察结果、11个变量的数据框结构 label_wrap_mod         包装标签文本 描述     包装标签文本 使用方式     label_wrap_mod(value,width=25) 参数     - value     需要被包装的向量(用as.character转换成char类型) - width     每一行字母的最大宽度，改编于https://github.com/hadley/ggplot2/wiki/labeller likert         分析李克特类型选项 描述     此函数提供多种关于李克特类的统计数据。 使用方式     likert(items, summary, grouping = NULL, factors = NULL, importance, nlevels = length(levels(item[,1]))) 参数 - items     存储李克特类选项的数据框。数据框中的变量类型应为“因子”数据类型 - summary     预总结的数据框。第一列必须是李克特选项，剩下的列是李克特选项的量表(例如：强烈反对，反对等等) - grouping     (可选的)结果应总结为所给的分组变量。 - factors     定义每一列所属因子length(factors)==ncol(items)的向量。值对应因子标签 - importance     与李克特选项有相同维度的数据框，包含对应选项的重要程度。 - nlevels     可能的量表量化选项的数量。只在存在缺失值的时候有必要标明。 细节 - results     此数据框会包含一列Item,Group(如果明确了一个分组变量，并且每列对应每一个量化选项(例如:同意，不同意等。)。每一个单元格的值对应每一个李克特回答的百分比) - items     原李克特选项的复制数据框 - grouping     原分组向量的复制 - nlevels     用于计算的量化选项的数量 值     包括以下元素的李克特类别：结果，选项，分组，量化选项数量，总结。 可查阅     - plot.likert - summary.likert 例子     data(pisaitems) items29 &lt;- pisaitems[, substr(names(pisaitems), 1, 5) == &#39;ST25Q&#39;] names(items29) &lt;- c(&quot;Magazines&quot;, &quot;Comic books&quot;, &quot;Fiction&quot;, &quot;Non-fiction books&quot;, &quot;Newspapers&quot;) l29 &lt;- likert(items29) summary(l29) plot(l29) liker.bar.plot         李克特选项的条形图 描述     李克特结果的条形图 使用方式     likert.bar.plot(l, group.order, center = (l$nlevels-1)/2 + 1, ...) 参数     - l     李克特选项的结果 - group.order     组别(分组选项)或者选项(非分组选项)的顺序 - center     指定作为中心的量化等级。例如，center=3表示用第三等级作为中心，center=3.5表示明确中心点，但是小于等于3为低等级，大于等于4为高等级(i.e. 用于强制选择的选项或者没有中立选择的选项)。这也会影响从低到高等级的颜色分割位置。 - ...     传到likert.options的参数 - likert     李克特类型对象 可查阅     - plot.likert - likert.heat.plot - likert.bar.plot - likert.density.plot likert.density.plot         创建一个李克特选项密度图 描述     此函数创建一个将李克特选项当作连续性变量处理的可视化 使用方式     likert.density.plot(likert, facet = TRUE, bw = 0.5, legend, ...) 参数     - likert     李克特类型对象 - facet     对于非分组的选项，每一个密度分布应该画在不同的平面内 - bw     平滑宽度。经常设置成标准偏差，但对于李克特类型选项来说经常是不足够的。由于两个相邻等级是不同的，所以使用0.5作为bw的值。 - legend     图标的标题 - ...     传给density的参数 可查阅     - plot.likert likert.heat.plot         内部方法 描述     内部方法 使用方式     likert.heat.plot(likert, low.color = &quot;white&quot;, high.color = &quot;blue&quot;, text.color = &quot;black&quot;, text.size = 4, wrap = 50, ...) 参数     - likert     李克特类型对象 - low.color     用于表示低数值的颜色 - high.color     用于表示高数值的颜色 - text.color     文本属性的字体颜色 - text.size     文本属性的字体大小 - wrap     用于非分组李克特对象的包装标签文本的宽度 - ...     目前尚未使用 可查阅     - plot.likert - likert.bar.plot likert.histogram.plot         李克特回答数量的直方图 描述     绘画一个包括每一个选项和分组(如果存在指定分组)对应的回答数量的直方图。 负数值(默认用红棕色)表明选项和分组的缺失值数量。 使用方式     likert.histogram.plot(l, xlab = &quot;n&quot;, plot.missing = TRUE, bar.color = &quot;grey70&quot;, missing.bar.color = &quot;maroon&quot;, label.completed = &quot;Completed&quot;, label.missing = &quot;Missing&quot;, legend.position = &quot;bottom&quot;, wrap = ifelse(is.null(l$grouping), 50, 100), order, group.order, panel.arrange = &quot;v&quot;, panel.strip.color = &quot;#F0F0F0&quot;, text.size = 2.5, ...) 参数     - l     李克特的结果 - xlab     用于x轴的标签 - plot.missing     如果是TRUE，缺失值会被画在x轴的左边 - bar.color     柱形条的颜色 - missing.bar.color     缺失值的柱形条的颜色 - label.completed     在图标中表示完整值总量的标签 - label.missing     在图标中表示缺失值总量的标签 - legend.position     图标位置 - wrap     包装文本到面板条之前的字母数量 - order     李克特选项的顺序 - group.order     组别(分组选项)或者选项(非分组选项)的顺序 - panel.arrange     v表示垂直位置，h表示水平位置 - panel.strip.color     面板颜色 - text.size     文本字体大小 - ...     其他ggplot2参数 likert.matrix.plot         矩阵图(实验的) 描述     矩阵图(实验的) 使用方法     likert.matrix.plot(likert, nSample = nrow(likert$items), ...) 参数     - likert     李克特选项的结果 - nSample     全部行的随机样本。此函数对于大数据集的运行时间会长一些(包括pisaitems数据)。可以绘画一个随机副样本以便于快速开发。 - ...     传给pairs.ordered.categorical的参数 likert.options         建立一个用于绘画李克特结果的有可选项的对象 描述    建立一个用于绘画李克特结果的有可选项的对象 使用方法     likert.options(low.color = &quot;#D8B365&quot;, high.color = &quot;#5AB4AC&quot;, neutral.color = &quot;grey90&quot;, neutral.color.ramp = &quot;white&quot;, colors = NULL, plot.percent.low = TRUE, plot.percent.high = TRUE, plot.percent.neutral = TRUE, plot.percents = FALSE, text.size = 3, text.color = &quot;black&quot;, centered = TRUE, include.center = TRUE, ordered = TRUE, wrap = 50, wrap.grouping = 50, legend = &quot;Response&quot;, legend.position = &quot;bottom&quot;, panel.arrange = &quot;v&quot;, panel.strip.color = &quot;#F0F0F0&quot;, ...) 参数     - low.color     用于表示低数值的颜色 - high.color     用于表示高数值的颜色 - neutral.color     用于表示中间数值的颜色(如果等级数为奇数) - neutral.color.ramp     当访问colorRamp用于low.color和high.color定义调色板时使用的第二种颜色 - colors     指定使用颜色的向量。向量长度或元素数量一定等于李克特等级的数量 - plot.percent.low     是否绘画低百分比 - plot.percent.high     是否绘画高百分比 - plot.percent.neutral     是否绘画中立百分比 - plot.percents     是否标记每一个种类/柱形条 - text.size     文本属性的文字大小 - text.color     文本属性的文字颜色 - centered     如果值为TRUE，条形图将被放在0的中心位置以便于低等级被放置与负值区域。 - include.center     如果值为TRUE，将中心等级包括在图中(不然中心等级会被忽视/排除) - ordered     将选项从高到低重新排序 - wrap     用于李克特选项标签的包装标签文本的宽度 - wrap.grouping     用于分组李克特选项标签的包装标签文本的宽度 - legend     图标的标题 - legend.position     图标的位置(“left”, “right”, “bottom”, “top”, 或者两个元素的数字向量) - panel.arrange     用于分组李克特选项的面板安排。可能的值为v(垂直位置,默认值), h(水平位置), 和NULL(自动选择水平和垂直) - panel.strip.color     面板标签的背景颜色 - ...     用于未来的扩展 mass         来自一个管理部门的Math Anxiety比例问卷调查的结果 描述     在一门统计学课程中对20名学生进行Math Anxiety比例问卷调查结果的数据框。 此数据框包括了原始数据，可以用于核实预先总结的步骤。 格式     14行和6列的数据框 参考文献     Bai, H., Wang, L., Pan, W., &amp; Frey, M. (2009). Measuring mathematics anxiety: Psychometric analysis of a bidimensional affective scale. Journal of Instructional Psychology, 36 (3), 185- 193. MathAnxiety         管理部门提供的 Math Anxiety比例问卷调查的预先总结的结果 描述     在一门统计学课程中，对20名学生进行了MathAnxiety比例问卷调查的预先总结结果的数据框。 格式     14行6列的数据框架。 参考文献     Bai, H., Wang, L., Pan, W., &amp; Frey, M. (2009). Measuring mathematics anxiety: Psychometric analysis of a bidimensional affective scale. Journal of Instructional Psychology, 36 (3), 185- 193. MathAnxietyGender         按照性别区分的, 管理部门提供的 Math Anxiety比例问卷调查的预先总结结果 描述     在一门统计学课程中，按照性别对20名学生进行了MathAnxiety比例问卷调查的预先总结结果的数据框。 格式     28行7列的数据框架。 参考文献     Bai, H., Wang, L., Pan, W., &amp; Frey, M. (2009). Measuring mathematics anxiety: Psychometric analysis of a bidimensional affective scale. Journal of Instructional Psychology, 36 (3), 185- 193. pisaitems         国际学生评估计划 描述     北美(即加拿大、墨西哥和美国)根据经济合作与发展组织(OECD)提供的2009年国际学生评估项目(PISA)的结果。更多信息请参见http://www.pisa.oecd.org/，包括代码手册。 格式     包含来自北美的81个变量的66690个观测值的数据框。 来源     经济合作与发展组织。 plot.likert         绘制一组李克特项 描述     这是S3 plot通用函数的一个实现。这个函数将根据类型参数调用likert.bar.plot, likert.heat.plot 或者 likert.density.plot。有关这些函数的自定义图形外观的所有可用参数，请参阅帮助页面。虽然这些函数可以直接绘制，但我们建议调用通用plot函数。 使用方式 #S3 method for class &#39;likert&#39; plot(x, type = c(&quot;bar&quot;, &quot;heat&quot;, &quot;density&quot;), include.histogram = FALSE, panel.widths = c(3, 1), panel.arrange = &quot;v&quot;, panel.strip.color = &quot;#F0F0F0&quot;, legend.position = &quot;bottom&quot;, group.order, panel.background = element_rect(size = 1, color = &quot;grey70&quot;, fill = NA), ...) 参数 - x     要绘制的李克特项。 - type     要创建的图形类型，当前值为bar和heat。 - include.histogram     如果为TRUE，回答数量的直方图也会被绘制。 - panel.widths     如果include.histogram为 TRUE, 这个长度为2的向量指定了左右面板的比例。 - panel.arrange     如何为按组分的李克特项安排面板。可能的值是v(垂直，默认值)、h(水平)和NULL(自动填充水平和垂直)。 - panel.strip.color     面板标签的背景色。 - legend.position     图例的位置(“左”、“右”、“下”、“上”或两元数字向量)。 - group.order     分组(分组项)或项(非分组项)的绘制顺序。 - panel.background     定义图的背景。见theme。 - 其他参数传递给 likert.bar.plot or likert.heat.plot。 可查阅 - likert.bar.plot - likert.heat.plot - likert.density.plot - likert.histogram.plot plot.likert.gap         绘制一组李克特项 描述     这是S3 plot通用函数的一个实现。这个函数将根据类型参数调用likert.bar.plot, likert.heat.plot 或者 likert.density.plot。有关这些函数的自定义图形外观的所有可用参数，请参阅帮助页面。虽然这些函数可以直接绘制，但我们建议调用通用plot函数。 使用方式     ## S3 method for class &#39;likert.gap&#39; plot(x, type = c(&quot;bar&quot;, &quot;density&quot;), include.histogram = FALSE, panel.widths = c(3, 1), panel.arrange = &quot;v&quot;, panel.strip.color = &quot;#F0F0F0&quot;, legend.position = &quot;bottom&quot;, panel.background = element_rect(size = 1, color = &quot;grey70&quot;, fill = NA), satisfaction.label = &quot;Satisfaction&quot;, importance.label = &quot;Importance&quot;, legend, ...) 参数 - x     要绘制的李克特项 。 - type     要创建的图形类型，当前值为bar和heat。 - include.histogram     如果为TRUE，回答数量的直方图也会被绘制。 - panel.widths     如果include.histogram为 TRUE, 这个长度为2的向量指定了左右面板的比例。 - panel.arrange     如何为按组分的李克特项安排面板。可能的值是v(垂直，默认值)、h(水平)和NULL(自动填充水平和垂直)。 - panel.strip.color     面板标签的背景色。 - legend.position     图例的位置(“左”、“右”、“下”、“上”或两元数字向量)。 - group.order     分组(分组项)或项(非分组项)的绘制顺序。 - panel.background     定义图的背景。见theme。 - satisfaction.label     满意度项的标签。 - importance.label     重要性项的标签。 - legend     说明框的标题。 - 其他参数传递给 likert.bar.plot or likert.heat.plot。 可查阅 - likert.bar.plot - likert.heat.plot - likert.density.plot - likert.histogram.plot print.likert         输出结果表 描述     输出结果表 使用方式     ## S3 method for class &#39;likert&#39; print(x, ...) 参数 - x     要打印的李克特类。 - 传递给print.data.frame的参数。 print.likert.bar.plot likert.bar.plot的输出方法。 主要目的是抑制ggplot2打印的“当 ymin != 0时堆叠没有很好定义”警告 该警告针对的是具有负条形(即居中图)的条形图。 描述     likert.bar.plot的输出方法。主要目的是抑制ggplot2对具有负条形(即中间的条形图)的条形图打印的“ymin != 0时堆叠没有很好定义”警告。 使用方式     ## S3 method for class &#39;likert.bar.plot&#39; print(x, ...) 参数 - x     likert.bar.plot生成的图。 - 传递给ggplot2的参数。 print.likert.gap         打印结果表 描述     打印结果表 使用方式     ## S3 method for class &#39;likert.gap&#39; print(x, ...) 参数 - x     要打印的李克特类。 - 传递给print.data.frame的参数。 print.likert.heat.plot         likert.heat.plot的打印方法 描述     likert.heat.plot的打印方法 使用方式     ## S3 method for class &#39;likert.heat.plot&#39; print(p, ...) 参数 - p     likert.heat.plot的图。 - 传递给ggplot2的参数。 print.xlikert         打印xtable.likert的结果 描述     打印xtable.likert的结果 使用方式     ## S3 method for class &#39;xlikert&#39; print(x, tabular.environment = &quot;longtable&quot;, floating = FALSE, ...) 参数 - x     xlikert的结果。 - tabular.environment     见 print.xtable。 - floating     见 print.xtable。 - 传递给print.xtable的参数。 recode         重新编码向量 描述     这个实用函数将重新编码带着新值的原始字符或因子向量的值。 使用方式     recode(x, from, to, to.class = NULL) 参数 - x     将被重新编码的向量。 - from     x中将被重新编码的旧值。 - to     新的值。 - to.class     一个“as.”函数代表所需的向量类型(例如as.character, as.numeric,as.logical, as.numeric) 值     一个长度与x一样的包含重新编码值的向量。 例子 test &lt;- letters[sample(5, 10, replace=TRUE)] recode(test, from=letters[1:5], to=paste(&#39;Letter&#39;, letters[1:5])) reverse.levels         反转一个因子的级别分类 描述     反转一个因子的级别分类。 使用方式     reverse.levels(x) 参数 - x     一个级别分类将被反转的一个因子或者数据框。 例子 mylevels &lt;- c(&#39;Strongly Disagree&#39;, &#39;Disagree&#39;, &#39;Neither&#39;, &#39;Agree&#39;, &#39;Strongly Agree&#39;) test &lt;- factor(sample(mylevels[1:5], 10, replace=TRUE)) cbind(test, as.integer(test), as.integer(reverse.levels(test))) sasr         学术自律调查(SASR)的结果。 描述     学术自我调节(SASR)调查由六个因素组成:自我调节、内在动机、外在动机、自我效能感、元认知和个人关联与控制。 格式     包含63个变量，860个观察值的数据框架。 参考文献     Dugan, R., &amp; Andrade, H. (2011). Exploring the construct validity of academic self-regulation using a new self-report questionnaire. The International Journal of Educational and Psychological Assessment, 7(1). shinyLikert         李克特包的Shiny应用。 描述     这将启动一个包含显示的许多功能李克特包的Shiny应用程序包。 使用方式 shinyLikert() 参考文献     http://rstudio.com/shiny summary.likert         打印李克特分析的汇总表。 描述     summary函数返回一个提供附加信息的数据框架。它包含类似结果数据框的“Item”和“Group”列,以及一列“low”对应总和水平低于中性的分类,一列“high”对应的总和高于中性的分类,和列“mean”和“sd”对应的平均值和标准偏差分别的结果。数值化结果取决于as.numeric后的因数的值。 - 使用方式 ## S3 method for class &#39;likert&#39; summary(object, center = (object$nlevels - 1)/2 + 1,ordered = TRUE, ...) 参数 - object     要总结的李克特类。 - center     指定应该将哪个级别作为中心。例如，center = 3将使用第三个级别作为中心，而center = 3.5将表示没有特定的级别为中心，但&lt;= 3是低级别，&gt;= 4是高级别(即用于强制选择项或没有中立选项的项)。 - ordered     是否应该对结果排序。目前不支持分组分析。 summary.likert.gap         打印李克特分析的汇总表。 描述     summary函数返回一个提供附加信息的数据框架。它包含类似结果数据框的“Item”和“Group”列,以及一列“low”对应总和水平低于中性的分类,一列“high”对应的总和高于中性的分类,和列“mean”和“sd”对应的平均值和标准偏差分别的结果。数值化结果取决于as.numeric后的因数的值。 使用方式     ## S3 method for class &#39;likert.gap&#39; summary(object, ...) 参数 - object     要总结的李克特类。 - 参数传递给summary.likert。 值 包含两个数据框架的列表，其中分别汇总了满意度和重要性数据的结果。 xtable.likert         打印一个LaTex表的李克特项目。 描述     建立LaTeX或HTML表的likert结果。 使用方式 ## S3 method for class &#39;likert&#39; xtable(x, caption = NULL, label = NULL, align = NULL, digits = NULL, display = NULL, auto = FALSE, include.n = TRUE, include.mean = TRUE, include.sd = TRUE, include.low = TRUE, include.neutral = (x$nlevels%%2 != 0), include.high = TRUE, include.levels = TRUE, include.missing = TRUE, center = (x$nlevels - 1)/2 + 1, ordered = TRUE, ...) 参数 - x     李克特类的对象。 - caption     表格的标题。 - label     表格的标签。 - align     列的序列方向。 - digits     数字类列的小数位的数量。 - display     列的格式。 - auto     逻辑，指示在没有向align、digit或display传递值时是否应用自动格式(有关更多信息，请参阅xtable)。 - include.n     包含n的选项。 - include.mean o     包含平均数的选项。 - include.sd     包含sd的选项。 - include.low     包含low的选项。 - include.neutral     包含neutral的选项。 - include.high     包含high的选项。 - include.levels     包含级别的选项。 - include.missing     包含缺失值的选项。 - center     指定应该将哪个级别作为中心。例如，center = 3将使用第三个级别作为中心，而center = 3.5将表示没有特定的级别为中心，但&lt;= 3是低级别，&gt;= 4是高级别(即用于强制选择项或没有中立选项的项)。这也影响哪些层被总结在低和高组。 - ordered     是否应该对结果排序。见summary.likert。 - ...     参数传递给xtable。 可查阅 - xtable - print.xtable "],
["rvest-package-1.html", "Chapter 61 rvest package 1", " Chapter 61 rvest package 1 Sunny Lee and Jinrong Cao In this page you will find the translated version of the Rvest package in Madarin Chinese. Rvest package is a powerful tool for extracting information on webpages.The original package can be found here: https://cran.r-project.org/web/packages/rvest/rvest.pdf. rvest 程序包 2019年5月15日 標題：簡單方便地提取網頁上的信息與數據 版本： 0.3.4 描述：为了便于下载和操作HTML和XML而设计的, 并且基于‘xml2’和’httr’上的包装 許可：GPL-3 網址：http://rvest.tidyverse.org/，https://github.com/tidyverse/rvest 報錯網址：https://github.com/tidyverse/rvest/issues\\ 依赖包：R (&gt;= 3.2), xml2 输入： httr (&gt;= 0.5), magrittr, selectr 建議：covr，knitr，png，rmarkdown，spelling，stringi (&gt;=版本0.3.1)，testthat 插圖生成器：knitr 編碼：UTF-8 語言：英語-美國 LazyData: 是 RoxygenNote 6.1.1 编辑需要：否 作者：Hadley Wickham，RStudio 維護者：Hadley Wickhamhadley@rstudio.com 仓库： CRAN 日期/出版：2019-05-15 20:10:30 UTC R主题记录 encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 google_form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 html . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3 html_form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 4 html_nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 html_session . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .. . . . . . . . .. . . 6 html_table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. 7 html_text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 8 jump_to . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . 9 pluck . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . 10 session_history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 set_values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . 11 submit_form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 目錄 —————————————————————————————————————————— Encoding           猜测并修正错误的字符编码 —————————————————————————————————————————— 描述 这些函数帮助你应对显示错误编码的网页。你可以使用guess_encoding找到正确的编码（并将结果作为参数提供给html的encoding），或者在找到正确信息之后使用repair_encoding纠正字符向量。 使用 guess_encoding(x) repair_encoding(x, from = NULL) 參數 x            一個字符型向量 from      字符串實際所在編碼。如果為 NULL，guess_coding會被使用 stringi 這些功能是極好的stringi程序包的周圍工具，所以你必須事先安裝好stringi程序包 例子 # 在程序包中包含一個不完善的編碼 path &lt;- system.file(&quot;html-ex&quot;, &quot;bad-encoding.html&quot;, package = &quot;rvest&quot;) x &lt;- read_html(path) x %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() guess_encoding(x) # 以下提供兩個有效的編碼，只有一個正確的 read_html(path, encoding = &quot;ISO-8859-1&quot;) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() read_html(path, encoding = &quot;ISO-8859-2&quot;) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() —————————————————————————————————————————— google_form            得到id之后创建一个google form的链接 —————————————————————————————————————————— 描述 得到id之后创建一个google form的链接 使用 google_form(x) 参数 x            唯一的字符向量 例子 google_form(&quot;1M9B8DsYNFyDjpwSK6ur_bZf8Rv_04ma3rmaaBiveoUI&quot;) —————————————————————————————————————————— html            对一个HTML网页进行爬虫 —————————————————————————————————————————— 描述 html被弃用时：请使用read_html()代替 使用 html(x, …, encoding = “”) class ‘session’ 的S3方法: read_xml(x, …, as_html = FALSE) 参数 x              一个url, 一个本地路径, 一个包含html的字符串, 或是一个httr请求后的回应 …              如果x為一個URL，額外的參數會被傳送給httr::GET() encoding     详述文件的编码。参照iconvlist()里的完整列表。如果在判断正确编码的过程中遇到问题，尝试stringi::stri_enc_detect() as_html      可以選擇如同爬html檔一般地爬一個xml檔 例子 # 通過url: google &lt;- read_html(&quot;http://google.com&quot;, encoding = &quot;ISO-8859-1&quot;) google %&gt;% xml_structure() google %&gt;% html_nodes(&quot;div&quot;) # 通過字符序列： (最少要是html 5文件) # http://www.brucelawson.co.uk/2010/a-minimal-html5-document/ minimal &lt;- read_html(&quot;&lt;!doctype html&gt; &lt;meta charset=utf-8&gt; &lt;title&gt;blah&lt;/title&gt; &lt;p&gt;I&#39;m the content&quot;) minimal minimal %&gt;% xml_structure() # 通過httr請求 google2 &lt;- read_html(httr::GET(&quot;http://google.com&quot;)) —————————————————————————————————————————— html_form            对一个网页里的表格进行爬虫 —————————————————————————————————————————— 描述 爬取網頁上的表單 使用 html_form(x) 参数 x 一个节点，节点的集合或一个文件 可参照 HTML 4.01 form specification: http://www.w3.org/TR/html401/interact/forms.html 例子 html_form(read_html(&quot;https://hadley.wufoo.com/forms/libraryrequire-quiz/&quot;)) html_form(read_html(&quot;https://hadley.wufoo.com/forms/r-journal-submission/&quot;)) box_office &lt;- read_html(&quot;http://www.boxofficemojo.com/movies/?id=ateam.htm&quot;) box_office %&gt;% html_node(&quot;form&quot;) %&gt;% html_form() —————————————————————————————————————————— html_nodes            从HTML文件里抓取节点 —————————————————————————————————————————— 描述 使用XPath與CSS選擇器更輕易地從html檔案獲取信息。當與https://selectorgadget.com/  一同使用時CSS選擇器特別有效：因為這能讓你輕易地找到你該使用哪個選擇器。若是你之前沒有使用過CSS選擇器，跟著這個有趣的教學操作一遍：http://flukeout.github.io/ 使用 html_nodes(x, css, xpath) html_node(x, css, xpath) 参数 x            一个文件，一个节点的集合或一个节点 css, xpath            要选择的节点。取决于要使用CSS或XPath 1.0选择器，提供一个css或xpath html_node 與 html_nodes 比較 html_node就像是 “[[”，它總是只會獲取剛好一個元素。當給予一列的節點，html_node總是會給出同樣長度的列表，而html_nodes給出的長度可能會更長或是更短。 CSS选择器支持 CSS选择器被selectr包翻译成XPath选择器，而这个选择器是python cssselect库的一个端口, https://pythonhosted.org/cssselect/. 它执行大多数CSS3里的选择器，参照描述：http://www.w3.org/TR/2011/REC-css3-selectors-20110929/. 例外情况如下： - 需要交互的虚拟选择器被忽略：:hover, :active, :focus, :target, :visited - 如下的虚拟classes无法和通配符元素运作：:first-of-type, :last-of-type, :nth-of-type, :nth-last-of-type, :only-of-type - 它支持：:contains(text) - 你可以使用!=, [foo!=bar] 等同于 :not([foo=bar]) - :not() 接受一个由简单选择器构成的序列，而不仅仅是一个简单选择器 例子 # CSS 選擇器---------------------------------------------- ateam &lt;- read_html(&quot;http://www.boxofficemojo.com/movies/?id=ateam.htm&quot;) html_nodes(ateam, &quot;center&quot;) html_nodes(ateam, &quot;center font&quot;) html_nodes(ateam, &quot;center font b&quot;) # 但是 html_node 最好與 magrittr程序包內的 %&gt;% 一同使用 # 你可以使用chain subsetting連鎖子集合: ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;td&quot;) ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;font&quot;) td &lt;- ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;td&quot;) td # 當使用在一序列的節點上，html_nodes()將給出所有節點，並將結果壓縮成一個 # 新的节点列表对象(nodelist) td %&gt;% html_nodes(&quot;font&quot;) # html_node()給出第一個符合的節點. 若是找不到符合的節點，它會給出一個”missing”節點 if (utils::packageVersion(&quot;xml2&quot;) &gt; &quot;0.1.2&quot;) { td %&gt;% html_node(&quot;font&quot;) } # 若要選出一個在特定位置的元素，你該使用magrittr::extract2，這是”[[“的一個別名 library(magrittr) ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% extract2(1) %&gt;% html_nodes(&quot;img&quot;) ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(1) %&gt;% html_nodes(&quot;img&quot;) # 找出前兩個表格中的所有圖像 ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[`(1:2) %&gt;% html_nodes(&quot;img&quot;) ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% extract(1:2) %&gt;% html_nodes(&quot;img&quot;) # XPath 選擇器 --------------------------------------------- # 和XPath連鎖(chaining)有點棘手 - 你可能需要變更使用中的字首 # 不論你現在在檔案中的哪裡，應該都是從根節點(root node)中做選擇 ateam %&gt;% html_nodes(xpath = &quot;//center//font//b&quot;) %&gt;% html_nodes(xpath = &quot;//b&quot;) —————————————————————————————————————————— html_session            模拟一个html浏览器的session —————————————————————————————————————————— 描述 模拟一个html浏览器的session 使用 html_session(url, …) is.session(x) 参数 url            开始 session的位置 …            任何在session中使用的其他httr配置 x            一个用于测试是否是session的对象 方法 一个session对httr和html方法的组合进行回应：使用httr::cookies(), httr::headers()和httr::status_code()获取要求的属性；使用html_nodes()获取html 例子 # http://stackoverflow.com/questions/15853204\\ s &lt;- html_session(&quot;http://hadley.nz&quot;) s %&gt;% jump_to(&quot;hadley-wickham.jpg&quot;) %&gt;% jump_to(&quot;/&quot;) %&gt;% session_history() s %&gt;% jump_to(&quot;hadley-wickham.jpg&quot;) %&gt;% back() %&gt;% session_history() s %&gt;% follow_link(css = &quot;p a&quot;) —————————————————————————————————————————— html_table            将一个html表格爬虫为数据框 —————————————————————————————————————————— 描述 将一个html表格爬虫为数据框 使用 html_table(x, header = NA, trim = TRUE, fill = FALSE, dec = “.”) 参数 x            一个节点，节点的集合或一个文件 header        是否将第一行作为header? 如果是NA, 并且如果包含&lt; th &gt;标签，那么使用第一行 trim            是否要去除每一个单元格内开头和结尾的空格？ fill            如果是，自动将每一行内少于最大列数的单元格填充为NAs dec            用作小数符号的字符 假设 html_table目前基于以下假设：            没有任何单元格跨行            标题在第一行里 例子 sample1 &lt;- minimal_html(&quot;&lt;table&gt; &lt;tr&gt;&lt;th&gt;Col A&lt;/th&gt;&lt;th&gt;Col B&lt;/th&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;z&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&quot;) sample1 %&gt;% html_node(&quot;table&quot;) %&gt;% html_table() # 在合併格中的值會被複製 sample2 &lt;- minimal_html(&quot;&lt;table&gt; &lt;tr&gt;&lt;th&gt;A&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;th&gt;C&lt;/th&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td colspan=&#39;2&#39;&gt;7&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&quot;) sample2 %&gt;% html_node(&quot;table&quot;) %&gt;% html_table() # 如果表格格式不整齊、每個縱列中的行數不同，使用`fill = TRUE`來填滿缺失值 sample3 &lt;- minimal_html(&quot;&lt;table&gt; &lt;tr&gt;&lt;th&gt;A&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;th&gt;C&lt;/th&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;3&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&quot;) sample3 %&gt;% html_node(&quot;table&quot;) %&gt;% html_table(fill = TRUE) —————————————————————————————————————————— html_text            从html里提取属性，文本和标签名 —————————————————————————————————————————— 描述 从html里提取属性，文本和标签名 使用 html_text(x, trim = FALSE) html_name(x) html_children(x) html_attrs(x) html_attr(x, name, default = NA_character_) 参数 x            一个文件，节点，或节点的集合 trim            如果是，那么去除每一个单元格内开头和结尾的空格 name            需要检索的属性的名字 default            当属性不存在于每一个节点时，默认使用的字符串 值 html_attr、html_tag、和html_text：一個字符型向量 html_attrs： 一個列表 例子 movie &lt;- read_html(&quot;http://www.imdb.com/title/tt1490017/&quot;) cast &lt;- html_nodes(movie, &quot;#titleCast span.itemprop&quot;) html_text(cast) html_name(cast) html_attrs(cast) html_attr(cast, &quot;class&quot;) —————————————————————————————————————————— jump_to            链接到一个新的url —————————————————————————————————————————— 描述 jump_to接受一个url(相对或绝对)；follow_link接受一个指向目前页面上链接(一个&lt; a &gt;标签)的表达 使用 jump_to(x, url, …) follow_link(x, i, css, xpath, …) 参数 x             一个session url            一个用于链接的url, 无论相对或绝对 …            任何用于这个请求的其他httr配置 i              你可以选择：            一个整数: 选择第i个链接            一个字符串：第一个包含此文本的链接（区分大小写） css            選擇的節點。提供css與xpath中的一種（視情況而定：看你想要使用CSS或是 XPath 1.0 選擇器 xpath        選擇的節點。提供css與xpath中的一種（視情況而定：看你想要使用CSS或是XPath 1.0 選擇器 例子 s &lt;- html_session(&quot;http://hadley.nz&quot;) s &lt;- s %&gt;% follow_link(&quot;github&quot;) s &lt;- s %&gt;% back() s %&gt;% follow_link(&quot;readr&quot;) —————————————————————————————————————————— pluck            用位置抓取列表内的元素 —————————————————————————————————————————— 描述 用位置抓取列表内的元素 使用 pluck(x, i , type) 参数 x         一个列表 i         一个字符串或一个整数 type         結果的種類（若知曉） —————————————————————————————————————————— session_history            用于操作历史记录的工具 —————————————————————————————————————————— 描述 用于操作历史记录的工具 使用 session_history(x) back(x) 参数 x          一个session —————————————————————————————————————————— set_values            在一个表格中设定数值 —————————————————————————————————————————— 描述 在一个表格中设定数值 使用 set_values(form, …) 参数 form          要調整的表格 …          要被調整的內容（名稱與值配對） 值 表格內被更新的物件 例子 search &lt;- html_form(read_html(&quot;http://www.google.com&quot;))[[1]] set_values(search, q = &quot;My little pony&quot;) set_values(search, hl = &quot;fr&quot;) ## 沒有運行: set_values(search, btnI = &quot;blah&quot;) —————————————————————————————————————————— submit_form            将一个表格提交回服务器 —————————————————————————————————————————— 描述 将一个表格提交回服务器 使用 submit_form(session, form, submit = NULL, …) 参数 session          将表格提交到的session form          要提交的表格 submit          用于提交的按钮名字。如果没有提供，默认为表格里第一个提交的按钮（包含一条消息） …          任何传递给 httr::GET()或httr::POST()的其他参数   值  如果请求成功，选择爬虫html的回应。如果http请求失败，报错。如果要获取回应里的其他元素，自己用submit_request结果里的元素进行搭建。 例子 test &lt;- google_form(&quot;1M9B8DsYNFyDjpwSK6ur_bZf8Rv_04ma3rmaaBiveoUI&quot;) f0 &lt;- html_form(test)[[1]] f1 &lt;- set_values(f0, entry.564397473 = &quot;abc&quot;) "],
["rvest-package-2.html", "Chapter 62 rvest package 2", " Chapter 62 rvest package 2 Tianyi Wang and Xue Xia 62.0.1 Description: Our group do a translation for some useful resource. We translated the package description of ‘rvest’ and the cheatsheet for data transformation. The ‘rvest’ package is especially useful in obtaining the information we need from websites quickly. Also, data transformation is an efficient way to do multiple things in single line of code. So we decided to translate these two into Chinese so that more people can benefit from it. 62.0.2 Source https://rstudio.com/resources/cheatsheets/ https://cran.r-project.org/web/packages/rvest/rvest.pdf 62.0.3 Cheatsheet 62.0.3.1 ‘rvest’爬虫包 62.0.3.1.1 简介 通过’xml2’ and ‘httr’ 包对网页进行爬虫，读取网页和节点内容 62.0.3.1.2 主页 http://rvest.tidyverse.org/, https://github.com/tidyverse/rvest 62.0.3.1.3 英文版 https://cran.r-project.org/web/packages/rvest/rvest.pdf 62.0.4 Encoding（乱码处理） 62.0.4.0.1 介绍： 可以用此类函数帮助应对网页中的乱码：可以用guess_encoding得到正确的代码，然后用repair_encoding 修复字符型向量。 62.0.4.0.2 函数定义： guess_encoding(x) repair_encoding(x, from = NULL) 62.0.4.0.3 参数列表： x 字符形向量 from 字符字符的实际编码格式；如果=NULL，即转用guess_encoding 62.0.4.0.4 应用： # # 读入包内乱码的一个文件 # path &lt;- system.file(&quot;html-ex&quot;, &quot;bad-encoding.html&quot;, package = &quot;rvest&quot;) # x &lt;- read_html(path) # x %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() # guess_encoding(x) # 两条可用的编码格式，其中只有一个是正确的 # read_html(path, encoding = &quot;ISO-8859-1&quot;) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() # read_html(path, encoding = &quot;ISO-8859-2&quot;) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() 62.0.5 google_form 62.0.5.0.1 介绍： 从已知识别码制作通往谷歌表格的链接 62.0.5.0.2 函数定义： google_form(x) 62.0.5.0.3 参数列表： x 表格的唯一识别码 62.0.5.0.4 应用： # google_form(&quot;1M9B8DsYNFyDjpwSK6ur_bZf8Rv_04ma3rmaaBiveoUI&quot;) 62.0.6 HTML 62.0.6.0.1 介绍： 用read_html() 读取网页 62.0.6.0.2 函数定义： html(x, …, encoding = &quot;&quot;) 62.0.6.0.3 参数列表： x 可以是url，本地路径，包含html的字符串，或者来自httr的请求 … 如果x是URL参数就传递给httr::GET() encoding 文档的编码形式，完整列表请查看iconvlist()。如果不能准确确定编码方式，可以尝试stringi::stri_enc_detect()。 62.0.6.0.4 应用： # # 从url读取： # google &lt;- read_html(&quot;http://google.com&quot;, encoding = &quot;ISO-8859-1&quot;) # google %&gt;% xml_structure() # google %&gt;% html_nodes(&quot;div&quot;) # # 从字符串读取： # # http://www.brucelawson.co.uk/2010/a-minimal-html5-document/ # minimal &lt;- read_html(&quot;&lt;!doctype html&gt; # &lt;meta charset=utf-8&gt; # &lt;title&gt;blah&lt;/title&gt; # &lt;p&gt;I&#39;m the content&quot;) # minimal # minimal %&gt;% xml_structure() # # 从httr请求读取： # google2 &lt;- read_html(httr::GET(&quot;http://google.com&quot;)) 62.0.7 html_form （提取表单） 62.0.7.0.1 介绍： 解析网页中的数据表格 62.0.7.0.2 函数定义： html_form(x) 62.0.7.0.3 参数列表： x 一个节点，节点集合，或者文档 可参考： http://www.w3.org/TR/html401/interact/forms.html 62.0.7.0.4 应用： # html_form(read_html(&quot;https://hadley.wufoo.com/forms/libraryrequire-quiz/&quot;)) # html_form(read_html(&quot;https://hadley.wufoo.com/forms/r-journal-submission/&quot;)) # box_office &lt;- read_html(&quot;http://www.boxofficemojo.com/movies/?id=ateam.htm&quot;) # box_office %&gt;% html_node(&quot;form&quot;) %&gt;% html_form() 62.0.8 html_nodes （提取网页中指定部分） 62.0.8.0.1 介绍： 可以用XPath和CSS更简单地集取HTML片段。CSS选择器在与http://selectorgadget.com/关联时尤为有用：他可以使找到正在用的选择器变得尤其简单。如果你以前没有用过CSS，可以在http://flukeout.github.io/找到攻略。 62.0.8.0.2 函数定义： html_nodes(x, css, xpath) html_node(x, css, xpath) 62.0.8.0.3 参数列表： x 可以是完整的文档，节点列表，或者单一的节点 css, xpath 要收集的节点 62.0.8.0.4 html_node 与html_nodes对比： html_node和向量中的[[相似，只输出一个元素。当给了一个节点列表时，html_node将返回一个长度相等的列表；而html_nodes可能返回更长或更短的列表。 62.0.8.0.5 应用： # # CSS 选择器---------------------------------------------- # ateam &lt;- read_html(&quot;http://www.boxofficemojo.com/movies/?id=ateam.htm&quot;) # html_nodes(ateam, &quot;center&quot;) # html_nodes(ateam, &quot;center font&quot;) # html_nodes(ateam, &quot;center font b&quot;) # # 但是html_node与 magrittr 中的%&gt;% 一起用是最好的 # # 你可以连续细分子集 # ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;td&quot;) # ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;font&quot;) # td &lt;- ateam %&gt;% html_nodes(&quot;center&quot;) %&gt;% html_nodes(&quot;td&quot;) # td # # 当你输入一个节点列表时，html_nodes() 返回所有节点 # # 把结果合并成为一个新的节点列表 # td %&gt;% html_nodes(&quot;font&quot;) # # html_node() 返回第一个符合的节点，如果没有符合的节点就返回&quot;missing&quot;节点 # if (utils::packageVersion(&quot;xml2&quot;) &gt; &quot;0.1.2&quot;) { # td %&gt;% html_node(&quot;font&quot;) # } # # 可以用magrittr::extract2选出指定位置的项目，和[[作用相似 # library(magrittr) # ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% extract2(1) %&gt;% html_nodes(&quot;img&quot;) # ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% &#39;[[&#39;(1) %&gt;% html_nodes(&quot;img&quot;) # # 找到在前两个表格中包含的所有图片 # ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% &#39;[[&#39;1:2) %&gt;% html_nodes(&quot;img&quot;) # ateam %&gt;% html_nodes(&quot;table&quot;) %&gt;% extract(1:2) %&gt;% html_nodes(&quot;img&quot;) # # XPath 选择器--------------------------------------------- # # 用XPath连续选择有一些困难，你需要改变使用的前缀。 # # 不管你现在在文档的哪里，需要一直从最前段的节点选择 # ateam %&gt;% # html_nodes(xpath = &quot;//center//font//b&quot;) %&gt;% # html_nodes(xpath = &quot;//b&quot;) 62.0.9 html_session 62.0.9.0.1 介绍： 模拟HTML游览器中的对话 62.0.9.0.2 函数定义： html_session(url, …) is.session(x) 62.0.9.0.3 参数列表： url 开始对话的位置 … 整个对话中需要的任何其他配置 x 测试是否是一个对话的项目 62.0.9.0.4 方法： 一个对话结构可以同时响应httr和html方法的操作：可以用httr::cookies()，httr::headers()和httr::status_code()得到访问请求的属性；以及，可以使用html_nodes访问html。 62.0.9.0.5 应用： # # http://stackoverflow.com/questions/15853204 # s &lt;- html_session(&quot;http://hadley.nz&quot;) # s %&gt;% jump_to(&quot;hadley-wickham.jpg&quot;) %&gt;% jump_to(&quot;/&quot;) %&gt;% session_history() # s %&gt;% jump_to(&quot;hadley-wickham.jpg&quot;) %&gt;% back() %&gt;% session_history() # s %&gt;% follow_link(css = &quot;p a&quot;) 62.0.10 html_table （提取网页数据表） 62.0.10.0.1 介绍： 把网页数据表格解析成R的数据框架。 62.0.10.0.2 函数定义： html_table(x, header = NA, trim = TRUE, fill = FALSE, dec = “.”) 62.0.10.0.3 参数列表： x 可以是单个节点，节点集合，或者文档 header 如果为TRUE则使用第一行为列名；如果为NA，当有 标签时使用第一行为列名 trim 如果为TRUE则过滤单元格前后的空格 fill 如果TRUE则自动填充缺失为NA dec 字符转换为10进制 62.0.10.0.4 前提条件： html_table 目前的前提条件是：没有单元格占了多行，标题在第一行。 62.0.10.0.5 应用： # sample1 &lt;- minimal_html(&quot;&lt;table&gt; # &lt;tr&gt;&lt;th&gt;Col A&lt;/th&gt;&lt;th&gt;Col B&lt;/th&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;z&lt;/td&gt;&lt;/tr&gt; # &lt;/table&gt;&quot;) # sample1 %&gt;% # html_node(&quot;table&quot;) %&gt;% # html_table() # # 合并单元格里的数据会被复制 # sample2 &lt;- minimal_html(&quot;&lt;table&gt; # &lt;tr&gt;&lt;th&gt;A&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;th&gt;C&lt;/th&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td colspan=&#39;2&#39;&gt;7&lt;/td&gt;&lt;/tr&gt; # &lt;/table&gt;&quot;) # sample2 %&gt;% # html_node(&quot;table&quot;) %&gt;% # html_table() # # 如果数据表格格式很差且每行的列数不同，用fill = TRUE来填补缺失的数据 # sample3 &lt;- minimal_html(&quot;&lt;table&gt; # &lt;tr&gt;&lt;th&gt;A&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;th&gt;C&lt;/th&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td colspan=&#39;2&#39;&gt;3&lt;/td&gt;&lt;/tr&gt; # &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt; # &lt;/table&gt;&quot;) # sample3 %&gt;% # html_node(&quot;table&quot;) %&gt;% # html_table(fill = TRUE) 62.0.11 html_text 62.0.11.0.1 介绍： 提取网页中的属性，文档和标签名称。 62.0.11.0.2 函数定义： html_text(x, trim = FALSE) html_name(x) html_children(x) html_attrs(x) html_attr(x, name, default = NA_character_) 62.0.11.0.3 参数列表： x 可以是文档，节点，或者节点合集 trim 如果是TRUE可以过滤前后的空格 name 需要获取的属性名字 default 如果属性不在每个节点都存在，用作设定值的字符 62.0.11.0.4 返回数据： html_attr, html_tag 和html_text：字符向量；html_attrs：一个列表。 62.0.11.0.5 应用： # movie &lt;- read_html(&quot;http://www.imdb.com/title/tt1490017/&quot;) # cast &lt;- html_nodes(movie, &quot;#titleCast span.itemprop&quot;) # html_text(cast) # html_name(cast) # html_attrs(cast) # html_attr(cast, &quot;class&quot;) 62.0.12 jump_to （提取相对或绝对链接） 62.0.12.0.1 介绍： jump_to() 读取一个（相对或绝对）链接； follow_link读取一个代表当前页面上的指向链接（a 标签）的表达 62.0.12.0.2 函数定义： jump_to(x, url, …) follow_link(x, i, css, xpath, …) 62.0.12.0.3 参数列表： x 一个会话 url 要访问的地址 … 其他需要的httr配置 62.0.12.0.4 应用： # s &lt;- html_session(&quot;http://hadley.nz&quot;) # s &lt;- s %&gt;% follow_link(&quot;github&quot;) # s &lt;- s %&gt;% back() # s %&gt;% follow_link(&quot;readr&quot;) 62.0.13 pluck 62.0.13.0.1 介绍： 用位置提取列表中的项目 62.0.13.0.2 函数定义： pluck(x, i, type) 62.0.13.0.3 参数列表： x 一个列表 i 一个字符串或整数 type 如果知道的输出类型 62.0.14 session_history 62.0.14.0.1 介绍： 历史记录导航工具 62.0.14.0.2 函数定义： session_history(x) back(x) 62.0.14.0.3 参数列表： x 一个对话 62.0.15 set_values （修改表单） 62.0.15.0.1 介绍： 设定格单里数据值 62.0.15.0.2 函数定义： set_values(form, …) 62.0.15.0.3 参数列表： form 要修改的表单 … 针对要修改控件的的名-值对 62.0.15.0.4 应用： # search &lt;- html_form(read_html(&quot;http://www.google.com&quot;))[[1]] # set_values(search, q = &quot;My little pony&quot;) # set_values(search, hl = &quot;fr&quot;) # ## 不要运行：set_values(search, btnI = &quot;blah&quot;) 62.0.16 submit_form 62.0.16.0.1 介绍： 向服务器提交回表格 62.0.16.0.2 函数定义： submit_form(session, form, submit = NULL, …) 62.0.16.0.3 参数列表： session 要提交表单的会话 form 要提交的表格 submit 上传使用的button名。如果没有设置，默认为form第一个上传的button（会有消息弹出） … httr::GET()和httr::POST()需要的附加参数 62.0.16.0.4 返回数据： 如果成功则返回解析出的网页回复。如果没有成功会弹出错误提示。如果需要回复中的其他项目，可以用submit_request 返回的项目自己设定。 62.0.16.0.5 应用： # test &lt;- google_form(&quot;1M9B8DsYNFyDjpwSK6ur_bZf8Rv_04ma3rmaaBiveoUI&quot;) # f0 &lt;- html_form(test)[[1]] # f1 &lt;- set_values(f0, entry.564397473 = &quot;abc&quot;) "],
["translation-of-parcoords-introduction.html", "Chapter 63 Translation of ‘parcoords’ Introduction 63.1 1. ‘parcoords’包使用说明 - 中文翻译 63.2 2. ‘parcoords’使用教程 - 中文翻译", " Chapter 63 Translation of ‘parcoords’ Introduction Yichi Liu and Michi Liu This is a Chinese translation version of the introduction and examples to the R package “parcoords”. 这是R包“parcoords”的使用说明和示例教程的中文翻译。 63.1 1. ‘parcoords’包使用说明 - 中文翻译 翻译自英文原版说明：https://cran.r-project.org/web/packages/parcoords/parcoords.pdf 63.1.1 parcoords 基于“d3.js”的交互式平行坐标图。 63.1.1.1 描述 创建由d3.js parallel-coordinates的htmlwidget外包的交互式平行坐标图。 63.1.1.2 使用方法 parcoords(data = NULL, rownames = TRUE, color = NULL, brushMode = NULL, brushPredicate = &quot;and&quot;, alphaOnBrushed = NULL, reorderable = FALSE, axisDots = NULL, margin = NULL, composite = NULL, alpha = NULL, queue = FALSE, mode = FALSE, rate = NULL, dimensions = NULL, bundleDimension = NULL, bundlingStrength = 0.5, smoothness = 0, tasks = NULL, autoresize = FALSE, withD3 = FALSE, width = NULL, height = NULL, elementId = NULL) 63.1.1.3 参数 参数 描述 data 用来画图的数据集，格式为data.frame。 rownames 逻辑值，代表是否显示data.frame里的行名称。不论此处是什么值，我们都会将带有行名称的数据发送给JavaScript。如果此处是False（否），则在平行坐标中不显示行名称。 color 颜色可以是单一的rgb或hex。对于一个颜色函数，需要提供一个list( colorScale = , colorBy = , colorScheme =, colorInterpolator = , colorDomain =)，其中colorScale是d3-scale的名字，例如scaleOrdinal或 scaleSequential。colorBy是数据中用来决定颜色的列的名称。如果将一个离散型或序数型变量用于color，则请提供colorScheme，例如schemCategory10。如果将一个连续性变量用于color，则请提供colorInterpolator，作为d3内插器的名称，例如interpolateViridis。如果用了d3的色阶，请确保使用参数withD3 = TRUE。 brushMode 字符串格式。根据需要的呈现效果选择刷子的模式，可以选择“1D-axes”或“1D-axes-multi”或“2D-strums”。 brushPredicate 字符串格式。可以选择“and”或者“or”，用来表示多个刷子的逻辑关系。 alphaOnBrushed 取值范围为0到1，表示被刷后的不透明度（缺省值为0）。 reorderable 逻辑值，表示是否对坐标重新排序。 axisDots 逻辑值，表示是否在多段线与坐标轴相交处用点标记。 margin list格式，列出边界空白的大小（单位为像素）。现阶段如果brushMode = “2D-strums”，需要左边边界空白为0, 因此可能会由于自动更正而导致异常结果。 composite 前景的合成类别。 alpha 取值范围是0到1，表示多段线的不透明度。 queue 逻辑值（缺省值为否：FALSE），用来表示是否将渐进式呈现的模式改为队列。如果数据集很大，此处通常为TRUE。 rate integer格式，表示队列渐进式呈现速率。 dimensions list格式，用来自定义坐标的维度。 bundleDimension 字符格式，表示需要捆绑的列/变量的名称。 bundlingStrength 数值格式，取值范围是0到1，表示捆绑的力度。如果bundleDimension是空的，那么这个参数将被忽略，不会对平行坐标产生影响。 smoothness 数值格式，取值范围是0到1，表示平滑度或曲率。如果bundleDimension是空的，那么这个参数将被忽略，不会对平行坐标产生影响。 tasks 可以是一个字符串，或者一个JS，或者一列字符串，或者一列JS。表示在parcoords呈现图像后运行的JavaScript函数。这为高级定制化图像提供了可能。请注意，函数运行的机制是JavaScript call。对于在函数内部的对象this来说，this.el代表了parcoords包含的元素，this.parcoords代表了parcoords的实体。 autoresize 逻辑值（缺省值是FALSE），代表在容器大小变化时是否自动改变parcoords的大小。在使用rmarkdown幻灯片展示或者flexdashboard的情境中，这将非常有用。但如果数据集很大或者在一个典型的html环境中则不太有用。 withD3 逻辑值，代表是否要包含d3r中的从属附件。parcoords htmlwidget独立使用了JavaScript，因此在global/window命名空间中不会包含完整的d3。若要包含d3.js，则应设置withD3 = TRUE。 width 整数值（单位为像素），代表了宽度。如果此处width = NULL，则会自动拉宽到100。 height 整数值（单位为像素），代表了高度。如果此处height = NULL，则高度会自动设置为400px。 elementId CSS选择器的ID。 63.1.1.4 返回值 一个属于htmlwidget类型的对象，可以作为HTML自动在不同环境中显示，包括R控制台里，Markdown文件内，和Shiny的输出文件内等。 63.1.1.5 使用示例 if(interactive()) { # simple example using the mtcars dataset data(mtcars) parcoords(mtcars) # various ways to change color # in these all lines are the specified color parcoords(mtcars, color = &quot;green&quot;) parcoords(mtcars, color = &quot;#f0c&quot;) # in these we supply a function for our color parcoords( mtcars ,color = list( colorBy = &quot;cyl&quot; ,colorScale = &quot;scaleOrdinal&quot; ,colorScheme = &quot;schemeCategory10&quot; ) , withD3 = TRUE ) if (require(&#39;ggplot2&#39;, quietly = TRUE)) { parcoords( diamonds ,rownames = FALSE ,brushMode = &quot;1d-axes&quot; ,reorderable = TRUE ,queue = TRUE ,color = list( colorBy = &quot;cut&quot; ,colorScale = &quot;scaleOrdinal&quot; ,colorScheme = &quot;schemeCategory10&quot; ) , withD3 = TRUE ) } } library(parcoords) parcoords(mtcars, dimensions = list(cyl = list( title = &quot;cylinder&quot;, tickValues = unique(mtcars$cyl) ))) parcoords( mtcars ,rownames = FALSE ,brushMode = &quot;1d-multi&quot; ,brushPredicate = &quot;OR&quot; ,dimensions = list(cyl = list( title = &quot;cylinder&quot;, tickValues = unique(mtcars$cyl) )) ) 63.1.2 parcoords-shiny 在Shiny中使用parcoords。 63.1.2.1 描述 用以在Shiny应用程序中使用sunburst和在交互式Rmd文件中输出并显示的函数。 63.1.2.2 使用方法 parcoordsOutput(outputId, width = &quot;100%&quot;, height = &quot;400px&quot;) renderParcoords(expr, env = parent.frame(), quoted = FALSE) 63.1.2.3 参数 参数 描述 outputId 返回的变量名称（用于读取）。 width, height 必须是有效的CSS单位（例如’100%‘, ’400px’, ‘auto’）或者一个数字。若是数字，则会转换成字符并在后方加上“px”。 expr 用来生成sunburst的表达式。 env 运行表达式expr的环境。 quoted expr是否是一个被引用的表达式(用quote())。若是想将表达式存在一个变量之中，这个参数将非常有用。 63.1.2.4 使用示例 if(interactive()) { #### filter proxy example ---- library(parcoords) library(shiny) ui &lt;- tagList( textOutput(&quot;filteredstate&quot;, container=h3), parcoordsOutput(&quot;pc&quot;) ) server &lt;- function(input, output, session) { rv &lt;- reactiveValues(filtered = FALSE) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) observe({ # toggle between filtered and unfiltered every 2.5 seconds invalidateLater(2500) rv$filtered &lt;- !isolate(rv$filtered) }) observeEvent(rv$filtered, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp &lt;- parcoordsProxy(&quot;pc&quot;) if(rv$filtered) { pcFilter( pcp, list( cyl = c(6,8), hp = list(gt = 200) ) ) } else { pcFilter(pcp, list()) } }) output$filteredstate &lt;- renderText({ paste0(&quot;Filtered:&quot;, rv$filtered) }) } shinyApp(ui = ui, server = server) ### center proxy example ---- library(shiny) library(parcoords) ui &lt;- tags$div( parcoordsOutput(&quot;pc&quot;, width = 2500), style=&quot;width:2500px;&quot; ) server &lt;- function(input, output, session) { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp &lt;- parcoordsProxy(&quot;pc&quot;) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) pcCenter(pcp, &#39;drat&#39;) } shinyApp(ui=ui, server=server) ### hide/unhide proxy example ---- library(parcoords) library(shiny) ui &lt;- tagList( selectizeInput( inputId = &quot;columns&quot;, label = &quot;Columns to Hide&quot;, choices = c(&quot;names&quot;,colnames(mtcars)), selected = &quot;names&quot;, multiple = TRUE ), parcoordsOutput(&quot;pc&quot;), checkboxInput(&quot;hidenames&quot;, label=&quot;Hide Row Names&quot;, value=TRUE), parcoordsOutput(&quot;pc2&quot;) ) server &lt;- function(input, output, session) { output$pc &lt;- renderParcoords({ parcoords(mtcars, rownames = FALSE, brushMode = &quot;1d&quot;) }) output$pc2 &lt;- renderParcoords({ parcoords(mtcars, rownames = FALSE) }) pcUnhide observeEvent(input$columns, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp &lt;- parcoordsProxy(&quot;pc&quot;) pcHide(pcp, input$columns) }, ignoreInit = TRUE, ignoreNULL = FALSE) observeEvent(input$hidenames, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp2 &lt;- parcoordsProxy(&quot;pc2&quot;) if(input$hidenames) { pcHide(pcp2, &quot;names&quot;) } else { pcUnhide(pcp2, &quot;names&quot;) } }) } shinyApp(ui = ui, server = server) ### snapshot example ---- library(shiny) library(parcoords) ui &lt;- tags$div( actionButton(inputId = &quot;snapBtn&quot;, label = &quot;snapshot&quot;), parcoordsOutput(&quot;pc&quot;, height=400) ) server &lt;- function(input, output, session) { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp &lt;- parcoordsProxy(&quot;pc&quot;) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) observeEvent(input$snapBtn, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render pcp &lt;- parcoordsProxy(&quot;pc&quot;) pcSnapshot(pcp) }) } shinyApp(ui=ui, server=server) } 63.1.3 ParcoordsProxy 用于发送指令至Shiny 应用程序的代理实例中。 63.1.3.1 描述 创建一个parcoords 形式的对象，该对象可用于控制和自定义化已经渲染的平行坐标图。该函数只能在Shiny 应用程序和 Shiny 文档中使用。 63.1.3.2 使用方法 parcoordsProxy(parcoordsId, session = shiny::getDefaultReactiveDomain(), deferUntilFlush = TRUE) 63.1.3.3 参数 参数 描述 parcoordsID parcoordsID 该参数是一个单元素字符向量，代表需要更改的平行坐标图的输出ID。（注意：如果函数被Shiny模块调用，该命名空间会被自动添加） session 该参数指向该图所属的Shiny会话控制对象，通常情况下使用默认值即可。 DeferUntilFlush 该参数用于指示是否立刻执行针对该实例的操作。若否，则保留该操作，直到下次所有输出更新后再执行。默认值为TRUE。 63.1.3.4 细节 通常情况下，我们可以直接使用parcoords函数创建平行坐标图。通过此，我们可以自定义一个存储在内存中的平行坐标图。这个平行坐标图可以在R控制台、R markdown 文档以及Shiny输出中显示。而在使用Shiny时，有时在输出/渲染完平行坐标图后，我们可能会想要进一步自定义化我们的平行坐标图。在这种情况下，之前存在内存的平行坐标图早已被清除，我们也早已在网页浏览器上完成了实例的实现。 因此我们需要使用ParcoordsProxy函数。该函数可以返回一个对象，用于代替普通parcoord（平行坐标图）对象。同时可以调用我们平时所用的绘制平行坐标图的函数，实现在动态实例中执行更改和自定义化的指令，而不是对存储在内存中的平行坐标图直接进行更改。 63.1.4 pcCenter 通过parcoordsProxy，实现基于列或者某一变量，将平行坐标图水平居中。 63.1.4.1 使用方法 pcCenter(pc = NULL, dim = NULL) 63.1.4.2 参数 参数 描述 Pc parcoordsProxy对象 dim 字符串向量，选择你想要作为居中基准的列或变量。 63.1.4.3 返回值 parcoordsProxy对象。 63.1.5 pcFilter 通过parcoordsProxy过滤不需要的平行坐标图。 63.1.5.1 使用方法 pcFilter(pc = NULL, filters = NULL) 63.1.5.2 参数 参数 描述 pc parcoordsProxy filters 应用于parcoordsProxy实现过滤的过滤器列表。可查询参阅search.js中的示例。 63.1.5.3 返回值 parcoordsProxy对象。 63.1.6 pcHide 通过parcoordsProxy，隐藏平行坐标图的某些列。 63.1.6.1 用法 pcHide(pc = NULL, dim = NULL) 63.1.6.2 参数 参数 描述 pc parcoordsProxy对象。 dim 字符串向量，想要隐藏的列名。 63.1.6.3 返回值 parcoordsProxy对象。 63.1.7 pcSnapshot 通过parcoordsProxy获取平行坐标图的图像。 63.1.7.1 用法 pcSnapshot(pc = NULL) 63.1.7.2 参数 参数 描述 pc parcoordsProxy对象。 63.1.7.3 返回值 parcoordsProxy对象。 63.1.8 pcUnhide 通过parcoordsProxy解除平行坐标图的某些列的隐藏状态。 63.1.8.1 用法 pcUnHide(pc = NULL, dim = NULL) 63.1.8.2 参数 参数 描述 pc parcoordsProxy对象。 dim 字符串向量，想要解除隐藏的列。 63.1.8.3 返回值 parcoordsProxy对象。 63.2 2. ‘parcoords’使用教程 - 中文翻译 翻译自英文原版教程： ftp://cran.r-project.org/pub/R/web/packages/parcoords/vignettes/introduction-to-parcoords-.html 肯顿·罗素 2019-05-21 Parcoords 提供shiny和crosstalk完全集成的多元数据集的交互性视图（parcoords-es）。 63.2.1 范例 绘制平行坐标图时，默认视图仅提供有限的交互性。parcoords的多种参数可提供一个更有身临其境感的自定义化过程。 library(parcoords) parcoords(mtcars, height = 450) 63.2.2 选项 63.2.2.1 刷子模式 通过brushMode参数，我们可以使用多种笔刷，通过该功能用户可以自由过滤/选择数据。 library(parcoords) parcoords( mtcars, brushMode = &#39;1D-axes&#39;, # &quot;1D-axes&quot;, &quot;1D-axes-multi&quot;, or &quot;2D-strums&quot; height = 500 ) 除了brushMode外，parcoords还有其他参数支持进一步自定义化。例如brushPredicate和alphaOnBrushed。可通过两个示例图的对比，查看它们的功能。 library(parcoords) parcoords( mtcars, brushMode = &#39;1D-axes&#39;, brushPredicate = &quot;or&quot;, # &quot;and&quot; &quot;or&quot; alphaOnBrushed = 0.3, height = 500 ) 63.2.2.2 颜色 可以用rgb或hex值给平行坐标图上单一颜色。 library(parcoords) parcoords( mtcars, color = &quot;#3e3&quot;, height = 500 ) 我们还可以通过提供以下参数list( colorScale = , colorBy = , colorScheme = , colorInterpolator =, colorDomain =)给颜色函数，以对颜色进行更改。其中，colorScale 是d3-scale中的参数名字，例如scaleOrdinal和scaleSequential。colorBy是原数据中决定颜色的列名。如果根据离散变量或者顺序变量对平行坐标图上色，请同时提供colorScheme参数，如schemCategory10，以保证上色能正常进行。如果通过连续变量对其上色，请同时提供colorInterpolator，该参数为d3插值器的名称，如interpolateViridis。如果使用d3提供的色标，请务必把参数withD3=TRUE包含在函数中。以下示例有助于阐明以上概念。 library(parcoords) parcoords( mtcars, color = list( # discrete or categorical column colorScale = &quot;scaleOrdinal&quot;, colorBy = &quot;cyl&quot;, colorScheme = &quot;schemeCategory10&quot; ), withD3 = TRUE, height = 500 ) 对使用连续变量上色，list在参数colorScale = ‘scaleSequential’上可能与其他情况下的略有不同。该情况下，默认的插值器是interpolateViridis，而在下面的例子中，我们使用interpolateMagma作为插值器。 library(parcoords) parcoords( mtcars, color = list( # continuous variable colorScale = &quot;scaleSequential&quot;, colorBy = &quot;mpg&quot;, colorInterpolator = &quot;interpolateMagma&quot; ), withD3 = TRUE, height = 500 ) 63.2.2.3 捆绑 当我们的数据集更大时，使用捆绑功能会很有帮助。为了数据集的大小起见，我们将继续使用mtcars数据。要查看效果，可以尝试使用survival::colon或ggplot2::diamonds。 library(parcoords) parcoords( mtcars, bundleDimension = &quot;cyl&quot;, bundlingStrength = 0.5, smoothness = 0.2, height = 500 ) 63.2.2.4 队列和速率 对于较大的数据集（&gt; 1000行），除非设置queue = TRUE与rate，否则图的交互反应速度会大大降低。这里可能需要进行一些试验才能选择正确的参数达到好的效果。如上所述，我们使用mtcars来示范。而实际上，只有当数据集真的很大时才需要使用这些功能。 library(parcoords) parcoords( mtcars, brushMode = &quot;1D-axes&quot;, queue = TRUE, rate = 2, # probably will be bigger (15 - 100) than this in real use height = 500 ) 63.2.2.5 贴砖模式 我添加了参数mode = 'tiled'，以此来使用下述方法 &quot;基于贴砖模式的平行坐标图及其在财务数据可视化中的应用&quot; Jamal Alsakran, Ye Zhao, and Xinlei Zhao 我很乐意接受关于此方法的反馈或建议。尽管该技术是为较大的数据集设计的，但是此代码尚未经过优化，并且未使用缓存，因此随着数据的增大，它实际上会变慢。对于使用数据集mtcars来说，此方法并没有意义。但出于示例目的，我们将继续使用这个较小的数据集。 library(parcoords) parcoords( mtcars, mode = &quot;tiled&quot;, brushMode = &quot;1D-axes&quot;, height = 500 ) 63.2.3 方法 该包提供了一些独立使用或在Shiny中使用的辅助方法。例如，我们可以使用快照功能来得到一个png格式的文件，作为对平行坐标图的当前状态的导出。 63.2.3.1 截图快照 先前版本的平行坐标中，提供了一些可以将图表捕获为静态图像的基本支持。但是，功能尚不完善，并且实施存在问题。现在，可以通过JavaScript和R的快照功能来达到目的。生成的图像还将记录刷子的当前状态。 library(parcoords) pc &lt;- parcoords( data = mtcars, color = list( colorBy = &quot;hp&quot;, colorScale = &quot;scaleSequential&quot; ), alpha = 0.5, brushMode = &quot;1d&quot;, # requires withD3 for now but will change so this is not necessary # after some iteration since this will pollute global namespace # and potenially conflict with other htmlwidgets using a different version of d3 withD3 = TRUE, elementId = &quot;parcoords-snapshot-example&quot; ) htmltools::tagList( htmltools::tags$script( &quot; function snapshotPC() { var pc = HTMLWidgets.find(&#39;#parcoords-snapshot-example&#39;).instance.parcoords; pc.snapshot(); } &quot; ), htmltools::tags$button( &quot;snapshot&quot;, onclick = &quot;snapshotPC()&quot; ), pc ) snapshot 63.2.3.2 使用Shiny的代理方法 与leaflet和plotly类似，parcoords中提供了代理方法，来与Shiny中的平行坐标进行交互，并且无需完全重新渲染。当前，下列功能（名称由pc*分隔）可以使用。 pcFilter pcCenter pcSnapshot pcHide pcUnhide pcSnapshot 可以输入?parcoords-shiny来查看相关使用示例。 "],
["chinese-translation-of-r-packages-for-interactie-plots-交互式数据可视化包-plotly-parcoords.html", "Chapter 64 Chinese Translation of R Packages for Interactie Plots 交互式数据可视化包: plotly &amp; parcoords 64.1 R 交互式数据可视化包 ‘plotly’ 64.2 R 主题/函数目录： 64.3 add_annotations 64.4 add_data 64.5 add_fun 64.6 add_trace 64.7 animation_opts 64.8 colorbar 64.9 embed_notebook 64.10 ggplotly 64.11 group2NA 64.12 R 交互式数据可视化包 ‘parcoords’ 64.13 R 主题/函数目录： 64.14 parcoords 64.15 parcoords-shiny 64.16 parcoordsProxy 64.17 pcCenter 64.18 parcoords_proxy 64.19 pcFilter 64.20 pcHide 64.21 pcSnapshot 64.22 pcUnhide", " Chapter 64 Chinese Translation of R Packages for Interactie Plots 交互式数据可视化包: plotly &amp; parcoords Han Xu and Lin Jiang 64.0.0.1 原文件链接: plotly: https://cran.r-project.org/web/packages/plotly/plotly.pdf parcoords: https://cran.r-project.org/web/packages/parcoords/parcoords.pdf 64.1 R 交互式数据可视化包 ‘plotly’ 名称：通过‘plotly.js’ 来制作交互式网络图表 版本：4.9.0 版权：MIT + 文件 LICENSE 介绍：从ggplot2制作交互式图标和/或者运用图标语言标准（grammer of graphics) 新定制一个连接界面接口的（MIT版权所有的）Javascript包’plotly.js’ 网站：https://plotly-r.com, https://github.com/ropensci/plotly#readme, https://plot.ly/r 修复历史文件：https://github.com/ropensci/plotly/issues 所需运行环境：R（版本&gt;= 3.2.0），ggplot2（&gt;= 3.0.0） 嵌入包：tools, scales, httr, jsonlite (&gt;= 1.6), magrittr, digest, viridisLite, base64enc, htmltools (&gt;= 0.3.6), htmlwidgets (&gt;= 1.3), tidyr, hexbin, RColorBrewer, dplyr, tibble, lazyeval (&gt;= 0.2.0), rlang, crosstalk, purrr, data.table, promises 建议搭配包：MASS, maps, ggthemes, GGally, testthat, knitr, devtools, shiny (&gt;= 1.1.0), shinytest (&gt;= 1.3.0), curl, rmarkdown, vdiffr, Cairo, broom, webshot, listviewer, dendextend, sf, maptools, rgeos, png, IRdisplay, processx, plotlyGeoAssets, forcats 惰性存储数据功能：开启 Roxygen版本：6.1.1 解码标准：UTF-8 是否需要编制：不需要 作者：Carson Sievert [aut, cre] (https://orcid.org/0000-0002-4958-2844), Chris Parmer [aut], Toby Hocking [aut], Scott Chamberlain [aut], Karthik Ram [aut], Marianne Corvellec [aut] (https://orcid.org/0000-0002-1994-3581), Pedro Despouy [aut], Plotly Technologies Inc. [cph] 维护者：Carson Sievert cpsievert1@gmail.com 文件版本库：CRAN 发布时间：2019-04-10 19:33:05 UTC 64.2 R 主题/函数目录： 64.3 add_annotations 64.3.0.1 描述： 给图表加备注 64.3.0.2 用法： add_annotations(p, text = NULL, ..., data = NULL, inherit = TRUE) 64.3.0.3 参数： p：一个交互式图表 text：需要加上的备注（必须加上的） …：这些参数被记录在此网站https://github.com/plotly/plotly.js/ blob/master/src/components/annotations/attributes.js data: 一个数据表格（DataFrame格式） inherit：是否要沿袭plot_ly()里面设置的属性？ 64.3.0.4 作者： Carson Sievert 64.4 add_data 64.4.0.1 描述： 给图表加数据 64.4.0.2 用法： add_data(p, data = NULL) 64.4.0.3 参数： p：一个交互式图表 data：一个Dataframe格式的数据表格 64.4.0.4 例子： library(plotly) plot_ly() %&gt;% add_data(economics) %&gt;% add_trace(x = ~date, y = ~pce) 64.5 add_fun 64.5.0.1 描述： 当需要加多层数据信息总结的时候可以用此函数 64.5.0.2 用法： add_fun(p, fun, ...) 64.5.0.3 参数： p：一个交互式图表 fun：一个函数，可以用来根据输入的图表为基础并且输出一个新的图表 …：给以上函数fun的参数 64.6 add_trace 64.6.0.1 描述： 给图表加各种图表标记 64.6.0.2 用法： add_trace(p, ..., data = NULL, inherit = TRUE) add_markers(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_text(p, x = NULL, y = NULL, z = NULL, text = NULL, ..., data = NULL, inherit = TRUE) add_paths(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_lines(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_segments(p, x = NULL, y = NULL, xend = NULL, yend = NULL, ..., data = NULL, inherit = TRUE) add_polygons(p, x = NULL, y = NULL, ..., data = NULL, inherit = TRUE) add_sf(p, ..., x = ~x, y = ~y, data = NULL, inherit = TRUE) add_table(p, ..., rownames = TRUE, data = NULL, inherit = TRUE) add_ribbons(p, x = NULL, ymin = NULL, ymax = NULL, ..., data = NULL, inherit = TRUE) add_area(p, r = NULL, t = NULL, ..., data = NULL, inherit = TRUE) add_pie(p, values = NULL, labels = NULL, ..., data = NULL, inherit = TRUE) add_bars(p, x = NULL, y = NULL, ..., data = NULL, inherit = TRUE) add_histogram(p, x = NULL, y = NULL, ..., data = NULL, inherit = TRUE) add_histogram2d(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_histogram2dcontour(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_heatmap(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_contour(p, z = NULL, ..., data = NULL, inherit = TRUE) add_boxplot(p, x = NULL, y = NULL, ..., data = NULL, inherit = TRUE) add_surface(p, z = NULL, ..., data = NULL, inherit = TRUE) add_mesh(p, x = NULL, y = NULL, z = NULL, ..., data = NULL, inherit = TRUE) add_scattergeo(p, ...) add_choropleth(p, z = NULL, ..., data = NULL, inherit = TRUE) 64.6.0.3 参数： p：一个交互式图表 …：这些属性是赋予trace type的。想要知道所有的属性可以看schema()，在trace -&gt; type -&gt; attributes 里面可以找到 data：一个DataFrame格式的表格或者crosstalk包里的sharedData表格 inherit：是否要沿袭plot_ly()里设置的属性？ x：x坐标的取值（开始x的值） y：y坐标的取值（开始y的值） z：一个数字的矩阵 text：文字标签 xend：最终x的位置 yend：最终y的位置 rownames：是否要展示每行的名字 ymin：一个变量用来规定多边形的底部取值 ymax：一个标量用来规定多边形的顶部取值 r：只有极坐标需要此参数 t：只有极坐标需要此参数 values：每个圆形分格表的分格大小 labels：每个圆形分格表的分格标注 64.7 animation_opts 64.7.0.1 描述： 可用plot_ly()里面的frame参数实现动画化，或者（非正式）的ggplot2里面的美学设置来实现动画化。默认情况下，动画会配有一个播放按钮，以及一个进度条来控制播放进度。播放按钮和进度条都是根据animation_opts()里设置的参数来运作的 64.7.0.2 用法： animation_opts(p, frame = 500, transition = frame, easing = &quot;linear&quot;, redraw = TRUE, mode = &quot;immediate&quot;) animation_slider(p, hide = FALSE, ...) animation_button(p, ..., label) 64.7.0.3 参数： p：一个互动式图表 frame：两帧之间停留的时间（包括过度时间） transition：两帧之间过度时间 easing：过度类别，可以在https://github.com/ plotly/plotly.js/blob/master/src/plots/animation_attributes.js中选择 redraw：是否在过度之后需要重新绘图？重新绘图可能会很大程度上影响速度，但有时候是必要的 made：描述想要新的动画模式怎么和已有的动画兼容 hide：是否要移除进度条？ label：给动画播放按钮的标注 64.7.0.4 作者： Carson Sievert 64.7.0.5 例子： df &lt;- data.frame( x = c(1, 2, 2, 1, 1, 2), y = c(1, 2, 2, 1, 1, 2), z = c(1, 1, 2, 2, 3, 3) ) plot_ly(df) %&gt;% add_markers(x = 1.5, y = 1.5) %&gt;% add_markers(x = ~x, y = ~y, frame = ~z) # it&#39;s a good idea to remove smooth transitions when there is # no relationship between objects in each view # 在目标相互没有联系的时候取消平滑过渡效果是个好主意 plot_ly(mtcars, x = ~wt, y = ~mpg, frame = ~cyl) %&gt;% animation_opts(transition = 0) # works the same way with ggplotly # 这个和ggplotly是同样的用法 if (interactive()) { p &lt;- ggplot(txhousing, aes(month, median)) + geom_line(aes(group = year), alpha = 0.3) + geom_smooth() + geom_line(aes(frame = year, ids = month), color = &quot;red&quot;) + facet_wrap(~ city) ggplotly(p, width = 1200, height = 900) %&gt;% animation_opts(1000) } 64.8 colorbar 64.8.0.1 描述： 改变图表的颜色条 64.8.0.2 用法： colorbar(p, ..., limits = NULL, which = 1) 64.8.0.3 参数： p：一个交互式图表 …：这些参数可以在这里找到https://plot.ly/r/reference/#scatter-marker-colorbar. limits：一个长度为2的数字向量 which：表明哪一个是想要更改的颜色条 64.8.0.4 作者： Carson Sievert 64.8.0.5 例子： p &lt;- plot_ly(mtcars, x = ~wt, y = ~mpg, color = ~cyl) # pass any colorbar attribute -- # https://plot.ly/r/reference/#scatter-marker-colorbar # 可以从以上的网站找到想要的colorbar属性 colorbar(p, len = 0.5) # Expand the limits of the colorbar # 增大colorbar的取值范围 colorbar(p, limits = c(0, 20)) # values outside the colorbar limits are considered &quot;missing&quot; # 在colorbar取值范围之外的值都被看成是极端值 colorbar(p, limits = c(5, 6)) # also works on colorbars generated via a z value # 这个函数也可以用在z生成的数值上 corr &lt;- cor(diamonds[vapply(diamonds, is.numeric, logical(1))]) plot_ly(x = rownames(corr), y = colnames(corr), z = corr) %&gt;% add_heatmap() %&gt;% colorbar(limits = c(-1, 1)) 64.9 embed_notebook 64.9.0.1 描述： 把图表嵌入python语言的编辑器里 64.9.0.2 用法： embed_notebook(x, width = NULL, height = NULL, file = NULL) 64.9.0.3 参数： x：一个交互式图表 width：嵌入图表的宽度。如果是NULL，则沿袭x的宽度；如果不是NULL，则采用系统默认为100% height：嵌入图标的高度。如果是NULL，则沿袭x的高度；如果不是NULL，则采用系统默认为400px file：已被移除 64.9.0.4 作者： Carson Sievert 64.10 ggplotly 64.10.0.1 描述： 把ggplot2做的ggplot()图表转换成交互式图表 64.10.0.2 用法： ggplotly(p = ggplot2::last_plot(), width = NULL, height = NULL, tooltip = &quot;all&quot;, dynamicTicks = FALSE, layerData = 1, originalData = TRUE, source = &quot;A&quot;, ...) 64.10.0.3 参数： p：一个交互式图表 width：图表的宽度 height：图表的高度 tooltip：一个用来表明用哪种美学函数的向量。向量里的顺序会决定图标里变量的顺序 dynamicTicks：决定是否要动态的输出坐标轴标注 layerData：决定要返回输出哪一层的图像 originalData：决定应该输出原始数据还是标准化过的数据 source：一个长度为1字符。可以用event_data()里的参数来提取里面特定的事件信息 64.10.0.4 详细信息： 转换的大小会根据正在运行的图表设备来决定（如果没有正在运行的设备，系统默认值是640/480）。 换句话来说，高度和宽度一定要在运行程序的时候设定来保证大小的正确性 64.10.0.5 作者： Carson Sievert 64.10.0.6 借鉴网站： https://plot.ly/ggplot2 以及：plot_ly() 64.10.0.7 例子： # simple example # 简单的例子 ggiris &lt;- qplot(Petal.Width, Sepal.Length, data = iris, color = Species) ggplotly(ggiris) # linked scatterplot brushing # 连着抛光几个散点图 d &lt;- highlight_key(mtcars) qplot(data = d, x = mpg, y = wt) %&gt;% subplot(qplot(data = d, x = mpg, y = vs)) %&gt;% layout(title = &quot;Click and drag to select points&quot;) %&gt;% highlight(&quot;plotly_selected&quot;) # more brushing (i.e. highlighting) examples # 再有几个抛光（重点强调）的例子 demo(&quot;crosstalk-highlight-ggplotly&quot;, package = &quot;plotly&quot;) ## ## ## demo(crosstalk-highlight-ggplotly) ## ---- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## ## &gt; library(plotly) ## ## &gt; # see https://vimeo.com/202647310 ## &gt; d &lt;- highlight_key(txhousing, ~city, &quot;Select a city&quot;) ## ## &gt; p &lt;- ggplot(d, aes(date, median, group = city)) + geom_line() ## ## &gt; ggplotly(p, tooltip = &quot;city&quot;) %&gt;% ## + layout(title = &quot;Click on a line to highlight a year&quot;) %&gt;% ## + highlight(dynamic = TRUE, selectize = TRUE) ## ## &gt; # crosstalk keys are automatically added to the group aesthetic... ## &gt; # if you want to avoid adding the key to group for a layer, ## &gt; # use the original data ## &gt; p &lt;- ggplot(d, aes(month, median)) + ## + geom_line(aes(group = city)) + ## + geom_smooth(data = txhousing, method = &quot;gam&quot;) + ## + facet_wrap(~ year) ## ## &gt; ggplotly(p) %&gt;% ## + layout(title = &quot;Click on a line to highlight a year&quot;) ## ## &gt; # perhaps a more useful example ## &gt; sd &lt;- highlight_key(txhousing, ~year) ## ## &gt; p &lt;- ggplot(sd, aes(month, median)) + ## + geom_line(aes(group = year)) + ## + geom_smooth(data = txhousing, method = &quot;gam&quot;) + ## + facet_wrap(~ city) ## ## &gt; ggplotly(p, height = 800, width = 1600) %&gt;% ## + layout(title = &quot;Click on a line to highlight a year&quot;) # client-side linked brushing in a scatterplot matrix # 客户端的抛光散点图矩阵的例子 highlight_key(iris) %&gt;% GGally::ggpairs(aes(colour = Species), columns = 1:4) %&gt;% ggplotly(tooltip = c(&quot;x&quot;, &quot;y&quot;, &quot;colour&quot;)) %&gt;% highlight(&quot;plotly_selected&quot;) 64.11 group2NA 64.11.0.1 描述： 这个函数主要是plotly内部调试用的，但也对熟练的用户有帮助 64.11.0.2 用法： group2NA(data, groupNames = &quot;group&quot;, nested = NULL, ordered = NULL, retrace.first = inherits(data, &quot;GeomPolygon&quot;)) 64.11.0.3 参数： data：一个DataFrame格式的数据表格 groupName：一个用来分组的向量 nested：其他的分组函数 ordered：一个用来排序的向量 retrace.first：是否将每组的最后一行和组的第一行贴在一起？ 64.11.0.4 详细信息： 如果一个组的分布趋势有共同的非位置特性（例如颜色等等），把他们用同一个特性整合在一起，然后用每组缺失的数据来区分效率会更高（要注意connectgaps是设定成False的） 价值：一个表格首先按照行来排序，然后每组的名字，最后再排序。只要每组名字有正确的变量的名字，新的行会被正确输入来区分各个组 64.11.0.5 例子： # note the insertion of new rows with missing values # 注意看新插入的行里有缺失数据 group2NA(mtcars, &quot;vs&quot;, &quot;cyl&quot;) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## 2 NA 4 NA NA NA NA NA 0 NA NA NA ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 5 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 6 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 7 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 8 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## 9 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 10 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 11 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## 12 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 13 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 14 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 15 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## 16 NA 6 NA NA NA NA NA 0 NA NA NA ## 17 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 18 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 19 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 20 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 21 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 22 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 23 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 24 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 25 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 26 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 27 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 28 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 29 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 30 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 31 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 32 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 33 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 34 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 # need to group lines by city somehow! # 需要把依据city的线分组！ plot_ly(txhousing, x = ~date, y = ~median) %&gt;% add_lines() # instead of using group_by(), you could use group2NA() # 你可以用group2NA()来代替group_by() tx &lt;- group2NA(txhousing, &quot;city&quot;) plot_ly(tx, x = ~date, y = ~median) %&gt;% add_lines() # add_lines() will ensure paths are sorted by x, but this is equivalent # add_lines()会保证出来的图样轨迹是按照x来排序的，但这和之前是一样的 tx &lt;- group2NA(txhousing, &quot;city&quot;, ordered = &quot;date&quot;) plot_ly(tx, x = ~date, y = ~median) %&gt;% add_paths() 64.12 R 交互式数据可视化包 ‘parcoords’ 标题：‘Htmlwidget’ 框架下交互式平行坐标图（适用于d3.js数据可视化库) 版本： 1.0.0 日期：2019-05-13 维护者： Kenton Russell kent.russell@timelyportfolio.com 简介：使用‘htmlwidget’生成适用于‘d3.js’的交互式平行坐标图 https://github.com/BigFatDog/parcoords-es {‘parallel-coordinates’}. URL链接： https://github.com/timelyportfolio/parcoords 乱码汇报：https://github.com/timelyportfolio/parcoords/issues 适用版本： R (&gt;= 3.2.0) 许可：MIT + file LICENSE 是否为LazyData: 是（意为此包在未使用情况下不会占据内存） 此包使用时需同步使用但不需加载的其他包：crosstalk, htmlwidgets (&gt;= 0.6.0), utils 在实例中使用的其他包： d3r, ggplot2, htmltools, knitr, shiny, testthat, rmarkdown RoxygenNote 6.1.1 VignetteBuilder：knitr 是否需要Compilation：否（意为下载时不需要其他工具） 作者： Mike Bostock [aut, cph] (d3.js library in htmlwidgets/lib, http://d3js.org), Kai Chang [aut, cph] (parallel coordinates reusable chart, https://github.com/syntagmatic/parallel-coordinates), Xing Yun [aut, cph] (‘es6’ parallel coordinates, https://github.com/BigFatDog/parcoords-es), Kenton Russell [aut, cre] (R interface), Anobel Odisho [aut] (R interface guidance, suggestions, testing, review, and feedback), Mark Albrecht [ctb] (Shiny output and example) 版本库： CRAN 发行日期： 2019-05-24 13:10:03 UTC 64.13 R 主题/函数目录： 64.14 parcoords 64.14.0.1 描述： 使用‘htmlwidget’生成适用于‘d3.js’的交互式平行坐标图 64.14.0.2 用法： parcoords(data = NULL, rownames = TRUE, color = NULL, brushMode = NULL, brushPredicate = &quot;and&quot;, alphaOnBrushed = NULL, reorderable = FALSE, axisDots = NULL, margin = NULL, composite = NULL, alpha = NULL, queue = FALSE, mode = FALSE, rate = NULL, dimensions = NULL, bundleDimension = NULL, bundlingStrength = 0.5, smoothness = 0, tasks = NULL, autoresize = FALSE, withD3 = FALSE, width = NULL, height = NULL, elementId = NULL) 64.14.0.3 参数： data: 一个data.frame形式的数据，将会用在表格中 rownames：逻辑使用表中data.frame的行名。如果不管此参数，我们会把我们发给JavaScript 的信息赋予到数据的行名上。如果rownames设置为等于FALSE, 我们会使用平行坐标图去隐藏它。 color：颜色可以设置为rgb或者hex的单色。对于一个颜色公式，提供一个以下列表： list( colorScale = , colorBy = , colorScheme =, colorInterpolator = , colorDomain =)。 其中，colorScale是d3-scale的名字，比如scaleOrdinal或scaleSequential。colorBy指出用来决定 颜色的列。如果将颜色赋予离散的或是顺序的变量，那请务必使用colorScheme, 比如schemCategory10。 如果将颜色赋予连续的变量，那请务必使用colorInterpolator以及colorInterpolator作为 d3 interpolator， 比如interpolateViridis. 如果使用d3的设定颜色， 请记得设定 withD3 = TRUE. brushmode：字符串，可设定为 “1D-axes”，或者 “1D-axes-multi”, 或者 “2D-strums”。此 参数给出想要的给表格的brush设定 brushPredicate：字符串，可设定为“and”或者“or”。此参数给出逻辑判断是否结合多种brush。 alphaOnBrushed：不透明度设定，可以设定为0到1（默认设定为0）。 reorderable：逻辑判定可以改变坐标轴的排序 axisDots：逻辑判定可以标亮出多线与坐标走交汇的点 margin：关于像素所需的页边差距的列表。现在brushMode = “2D-strums”需要左边margin = 0， 所以这个会自动改变且有可能得出不期望的结果 composite：前景内容的混合了 icing alpha：多线的不透明度设定，可以设定为0到1 queue：逻辑判断（默认设定为FALSE)，用来改变队列逐步的实现模式。一般当数据集非常大时， 设置queue = T。 mode: 字符串，与以上queue参数的设定有关。当queue = T时，会设定mode = “queue”。 rate: 排序所依据的整数比率 dimension: 用来自定义坐标轴维度的列表 bundleDimension: 列或者变量的字符串，用来定义哪些列或者变量捆绑在一起。 bundlingStrength: 0到1之间的数值， 用来设定捆绑的强度。这个数值不会影响平行坐标图， 如果bundleDimension没有设定，此时本变量会被忽略。 smoothness: 0到1之间的数值，用来设定使线条变曲线的强度。这个数值不会影响平行坐标图， 如果bundleDimension没有设定，此时本变量会被忽略。 tasks: 一个字符串，或者JS, 或者一个字符串列表，或者一个表现JavaScript公式的JS。这个参数 会在parcoords展示后执行。这个参数提供了一个更高级自定义的机会。注意，function（公式）会使用 JavaScript call的机制，所以在公式中，this会成为一个含有this.el的对象，this.el会展示 parcoords含有的元素 并且this.parcoords展示parcoords实例。 autoresize: 逻辑判断（默认设定为FALSE)，用来自用调整parcoords的大小当容器的大小改变。 这个在r markdown 演讲（presentation）以及flexdashboard中非常有用。然而此参数对于 更大的数据集或是典型的html环境下不太有用。 withD3: 逻辑判断是否包含d3 dependency (来自于当到d3r)。‘parcoords’ 建立在独立的JavaScript 上，将不会包含全部的d3。如果想包含d3.js, 设定 withD3 = TRUE。 width: 数值，用来设定widget的宽度。 如果设定width = NULL, 会自动设定widget容器为100。 height: 数值，用来设定widget的高度。 如果设定height = NULL, 会自动设定widget容器为400px。 elementId: 独有的给widget的CSS selector id。 64.14.0.4 产出： 此公式将会生成一个htmlwidget格式的对象，此对象对自动以html格式产出。此对象可以使用在 多种背景中，包括R console, 或是在R Markdown 文件中, 以及在 Shiny 的产出语法设定中。 64.14.0.5 例子： library(parcoords) if(interactive()) { # simple example using the mtcars dataset # 一个用mtcars数据集的简单例子 data( mtcars ) parcoords( mtcars ) # various ways to change color # 有多种方法去改变颜色 # in these all lines are the specified color # 此方法下每条线都有一个特殊的颜色 parcoords( mtcars, color = &quot;green&quot; ) parcoords( mtcars, color = &quot;#f0c&quot; ) # in these we supply a function for our color # 此方法下我们通过公式来赋予颜色 parcoords( mtcars, color = list(colorBy = &quot;cyl&quot;, colorScale = &quot;scaleOrdinal&quot;, colorScheme = &quot;schemeCategory10&quot;), withD3 = TRUE) if(require(&#39;ggplot2&#39;, quietly = TRUE)) { parcoords(diamonds,rownames = FALSE,brushMode = &quot;1d-axes&quot;, reorderable = TRUE,queue = TRUE, color= list(colorBy=&quot;cut&quot;, colorScale = &quot;scaleOrdinal&quot;, colorScheme = &quot;schemeCategory10&quot;), withD3 = TRUE) } } parcoords( mtcars, dimensions = list( cyl = list( title = &quot;cylinder&quot;, tickValues = unique(mtcars$cyl) ) )) parcoords( mtcars ,rownames = FALSE ,brushMode = &quot;1d-multi&quot; ,brushPredicate = &quot;OR&quot; ,dimensions = list( cyl = list( title = &quot;cylinder&quot;, tickValues = unique(mtcars$cyl) ) ) ) 64.15 parcoords-shiny 64.15.0.1 描述： 像在Rmd文件中，产出并使用Shiny中的交互式旭日形图（sunburst）。 64.15.0.2 用法： parcoordsOutput(outputId, width = &quot;100%&quot;, height = &quot;400px&quot;) renderParcoords(expr, env = parent.frame(), quoted = FALSE) 64.15.0.3 参数： outputId: 读入的产出变量 width, height: 必须是有效的CSS单位（如“100%”， “400px”， “auto）或是数值，此数值 将会生成一个含有‘px’的字符串 expr: 一个用来生成旭日形图的表达式 env: 用来评估expr的环境 quoted: 是否expr是一个引用的表达式（含有quote（））。当你想要保存表达式为一个变量时， 此变量十分有用。 64.15.0.4 例子： if (interactive()) { #### filter proxy example ---- #### 筛选proxy的例子 library(parcoords) library(shiny) ui &lt;- tagList( textOutput(&quot;filteredstate&quot;, container=h3), parcoordsOutput(&quot;pc&quot;) ) server &lt;- function(input, output, session) { rv &lt;- reactiveValues(filtered = FALSE) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) observe({ # toggle between filtered and unfiltered every 2.5 seconds # 每2.5秒在筛选与为筛选中切换 invalidateLater(2500) rv$filtered &lt;- !isolate(rv$filtered) }) observeEvent(rv$filtered, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render # 生成一个代理人用于在Shiny和平行坐标图之间交流 pcp &lt;- parcoordsProxy(&quot;pc&quot;) if(rv$filtered) { pcFilter( pcp, list( cyl = c(6,8), hp = list(gt = 200) ) ) } else { pcFilter(pcp, list()) } }) output$filteredstate &lt;- renderText({ paste0(&quot;Filtered: &quot;, rv$filtered) }) } shinyApp(ui = ui, server = server) ### center proxy example ---- ### 中心化proxy的例子 library(shiny) library(parcoords) ui &lt;- tags$div( parcoordsOutput(&quot;pc&quot;, width = 2500), style=&quot;width: 2500px;&quot; ) server &lt;- function(input, output, session) { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render # 生成一个代理人用于在Shiny和平行坐标图之间交流 pcp &lt;- parcoordsProxy(&quot;pc&quot;) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) pcCenter(pcp, &#39;drat&#39;) } shinyApp(ui = ui, server = server) ### snapshot example ---- ### snapshot的例子 library(shiny) library(parcoords) ui &lt;- tags$div( actionButton(inputId = &quot;snapBtn&quot;, label = &quot;snapshot&quot;), parcoordsOutput(&quot;pc&quot;, height=400) ) server &lt;- function(input, output, session) { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render # 生成一个代理人用于在Shiny和平行坐标图之间交流 pcp &lt;- parcoordsProxy(&quot;pc&quot;) output$pc &lt;- renderParcoords({ parcoords(mtcars) }) observeEvent(input$snapBtn, { # create a proxy with which we will communicate between # Shiny and the parallel coordinates without a re-render # 生成一个代理人用于在Shiny和平行坐标图之间交流 pcp &lt;- parcoordsProxy(&quot;pc&quot;) pcSnapshot(pcp) }) } shinyApp(ui=ui, server=server) } 64.16 parcoordsProxy 64.16.0.1 描述： 生成一个类似parcoords的对象，此对象可用来自定义并且控制一个已经生成的parcoords。 只能用于Shiny apps和Shiny docs。 64.16.0.2 用法： parcoordsProxy(parcoordsId, session = shiny::getDefaultReactiveDomain(), deferUntilFlush = TRUE) 64.16.0.3 参数： parcoordsId：一个单个特征矢量，此矢量指出修改的parcoords的产出id（output id）。如果 在一个Shiny模块中唤起，命名空间会自动加入。 session：反对地图所归属的Shiny session。一般情况下，默认设定可以满足要求。 deferUntilFlush：指出反对实例的行动是否需要马上执行，或者他们应该在下一次所有产出更新是 执行。默认设定为TRUE。 64.16.0.4 详细描述： 通常，你生成一个parcoords表会使用parcoords公式。这个公式会生成一个内存（in-memory） 的你自定义的parcoords表现。这样的parcoords可以在R console 打印出，包阔在R Markdown 文件中, 或是作为一个Shiny的产出。 对于在Shiny中，你想要更多的自定义一个parcoords，即使它已经生成一个产出。在这个时候， 内存呈现下的parcoords已经过去很久，用户的页面已经实现了parcoords的实例。 在这个时候，就应该使用parcoordsProxy。 它将会得到一个代替寻常parcoords的对象。寻常的 parcoords公式会被唤起，并且要求会在实况的parcoords实例上执行，而不是自定义那个内存呈现。 64.17 pcCenter 64.17.0.1 描述： 基于列/变量，通过parcoordsProxy， 使得parcoords位于水平面的中心。 64.17.0.2 用法： pcCenter(pc = NULL, dim = NULL) 64.17.0.3 参数： pc： 给出parcoordsProxy dim：字符串，给出中心化的列/变量 64.17.0.4 产出： 64.18 parcoords_proxy 64.19 pcFilter 64.19.0.1 描述： 通过parcoordsProxy筛选parcoords。 64.19.0.2 用法： pcFilter(pc = NULL, filters = NULL) 64.19.0.3 参数： pc：给出parcoordsProxy filters: 一个用于parcoords proxy的筛选的标准列表。请看search.js（https://github.com/deitch/searchjs） 得到一些作为筛选标准 的例子。 64.19.0.4 产出： parcoords_proxy 64.20 pcHide 64.20.0.1 描述： 通过parcoordsProxy隐藏 parcoords 的列。 64.20.0.2 用法 pcHide(pc = NULL, dim = NULL) 64.20.0.3 参数： pc: 给出parcoordsProxy dim: 字符串，给出想要隐藏的列的名字。 64.20.0.4 产出： parcoords_proxy 64.21 pcSnapshot 64.21.0.1 描述： 通过parcoordsProxy下载parcoords的图像 64.21.0.2 用法： pcSnapshot(pc = NULL) 64.21.0.3 参数： pc: 给出parcoordsProxy 64.21.0.4 产出： parcoords_proxy 64.22 pcUnhide 64.22.0.1 描述： 通过parcoordsProxy取消隐藏parcoords的列 64.22.0.2 用法： pcUnhide(pc = NULL, dim = NULL) 64.22.0.3 参数： pc: 给出parcoordsProxy dim: 字符串，给出想要隐藏的列的名字。 64.22.0.4 产出： parcoords_proxy "],
["translation-of-lattice-package.html", "Chapter 65 Translation of Lattice Package 65.1 Lattice 画图包的使用介绍 65.2 例子引入 65.3 主要思想 65.4 设计目标 65.5 常见的高级功能 65.6 更多资源", " Chapter 65 Translation of Lattice Package Rui Bai and Congcheng Yan 65.1 Lattice 画图包的使用介绍 作者：Deepayan Sarkar lattice是用来在R中实现格状图形（trellis graph）（最初为S和S-PLUS开发的）的附加程序包。它是功能强大且优雅的高级数据可视化系统，着重于多元数据，可以满足一些典型图形的需求，并且足够灵活以处理大多数非标准要求。 本教程涵盖了lattice的基础知识，并提供了指向更多资源的指南。 65.2 例子引入 为了更好地理解该软件思路，我们先从一些简单的例子开始。在此，我们使用的是mlmRev包中的chem97作为数据集。 data(Chem97, package = &quot;mlmRev&quot;) head(Chem97) ## lea school student score gender age gcsescore gcsecnt ## 1 1 1 1 4 F 3 6.625 0.3393157 ## 2 1 1 2 10 F -3 7.625 1.3393157 ## 3 1 1 3 10 F -4 7.250 0.9643157 ## 4 1 1 4 10 F -2 7.500 1.2143157 ## 5 1 1 5 8 F -1 6.444 0.1583157 ## 6 1 1 6 10 F 4 7.750 1.4643157 该数据集记录了有关在1997年英国A级化学考试中出现的学生信息。 我们只对以下变量感兴趣： score：A级考试中的分数，有六个可能值（0、2、4、6、8）。 gcsescore：GCSE考试的平均分数。 这是一个连续的分数，可以用作A级分数的一种预测。 gender：学生的性别。 我们可以用lattice包画全部 gcsescore 的直方图： histogram(~ gcsescore, data = Chem97) 这个图只显示了一个合理对称的单峰分布，但除此之外没有别的有趣的信息。 一种更有趣的展示是比较gcsescore在不同子组中的分布情况，例如以A级考试成绩划分的那些子组。代码如下： histogram(~ gcsescore | factor(score), data = Chem97) 一种更有效的比较手法是直接叠加直方图。传统的直方图很难做到这一点，但是使用密度分布直方图就可实现叠加了。 在以下示例中，我们使用与以前相同的子组，但另外在每个子组中按性别进行了细分。 densityplot(~ gcsescore | factor(score), Chem97, groups = gender, plot.points = FALSE, auto.key = TRUE) 练习1 如果省略了多余的参数plot.points和auto.key会发生什么？如果省略对factor()的调用会发生什么？ ?panel.densityplot的帮助文档中说明了plot.points参数，而?xyplot的帮助文档中则说明了auto.key参数。在不调用factor()的情况下，score会被视为数字变量，并转换为shingle，详情见?shingle的帮助文档。 练习2 lattice使用可自定义设置的系统来导出默认图形参数。 请注意，两种性别的密度曲线可以通过不同的线型来区分。 如果在交互式图形中运行相同的命令，这是否也是正确的？ 你认为哪个更有效？ 详情见?trellis.device说明文档。 65.3 主要思想 lattice为统计图形提供了一个高级的系统，它是独立于原本的R图形的。 它以S-PLUS中的Trellis套件为模型，并实现了大多数功能。 实际上，lattice可以认为是Trellis 图形基础原理的一种运用。 它使用grid作为基础实现引擎，因此默认参数延续了它的特征。 网格状显示由图形的类型以及不同变量在其中扮演的角色定义。 每种显示类型都与相应的高级功能（直方图，密度图等）相关联。 可能的角色取决于图像的类型，但也有一些特点如下： 主变量：定义主要显示的变量（例如，前面示例中的gcsescore）。 条件变量：将数据分为子组，每个子组显示在不同的panel中（例如，最后两个示例中的得分）。 分组变量：通过叠加展示在panel内对子组进行对比（例如，最后一个示例中的性别）。 以下图像类型可在lattice中显示。 函数 显示图像 histogram() 直方图 densityplot() 密度曲线图 qqmath() 理论分位数图 qq() 两样本分位数图 stripplot() 带状图（比较性一维散点图） bwplot() 比较型箱形图 dotplot() 克利夫兰点图 barchart() 条形图 xyplot() 散点图 splom() 散点图矩阵 contourplot() 等高线图 levelplot() 色阶图线框 wireframe() 曲面的三维透视图 cloud() 三维散点图 parallel() 平行坐标图 我们可以编写新的高级功能来表示进一步的可视化类型。以ecdfplot()和gridptra包中的mapplot()为例。 65.4 设计目标 可视化是一门艺术，但是它可以从系统的科学方法中受益匪浅。我们已经可以证明：可以提出一些通用规则，这些规则可以用于设计更有效的图形。 格状图形（Trellis graph）的主要目标之一是提供使这些规则易于应用的工具，因此使用的难度已从用户转移到了软件。此类规则的示例包括： 尽可能利用可用空间 尽可能通过叠加（分组）强制直接比较 鼓励在并置（调节）时进行比较：使用公共轴，添加公共参考网格等对象。 这些设计目标存在一些技术缺陷。例如，不浪费空间就需要完整绘图开始时便会显示出来，因此，传统R绘图中常见的增量方法（例如，在完成主图完成后添加主标题）不适合。lattice使用基于对象的范例：图以常规R对象表示，其余的更新由修改此类对象并重新绘制而实现。 尽管这些规则很有用，但任何严肃的图形系统也必须具有灵活性。lattice设计的理念之一即为灵活性，但显然对于较常见的任务，需要在灵活性和易用性之间进行权衡。lattice使用以下模型实现平衡： 显示屏由各种元素组成 默认值可提供有意义的结果 每个部分都可以由用户独立控制 主要元素是： – 主（panel）显示 – 坐标轴注释 – 带状注释（描述变化过程） – 图例（通常描述分组过程） 在每种情况下，都可以使用附加参数来改变常见变量。不仅如此，通过任意用户定义的函数可以实现全部变量的变动，灵活性极高。lattice的大多数使用涉及改变这些元素中的一个或多个。不是全部图形设计被整齐地划分为这些元素；lattice可能并不是用于此类显示的好工具。 65.5 常见的高级功能 65.5.1 可视化单变量分布 几种标准统计图形旨在可视化连续随机分布变量。我们已经学习了直方图和密度图，它们都是概率密度函数的一种估计。另一个有用的显示图形是正态QQ图，它与分布函数\\(F（x）=P（X≤x）\\)有关。正态QQ图可以由lattice函数qqmath()生成。 qqmath(~ gcsescore | factor(score), Chem97, groups = gender, f.value = ppoints(100), auto.key = TRUE,type = c(&quot;p&quot;, &quot;g&quot;), aspect = &quot;xy&quot;) 正态QQ图对数据的实际分位数与正态分布的分位数（或某些其他理论分布）。它们可以看作是分布函数\\(F\\)的估计，其中概率轴是由正常分位数函数转换的。它们旨在检测与正态分布的偏离程度。 若是近乎正态，这些点将近似沿直线分布。在上图中，凸度表明分布是左偏的，而斜率的变化表明变化的方差。type参数为每个格子添加一个公共的参考网格，从而更容易看gcsescore向上跨格的变化。aspect参数会自动计算长宽比。 两样本QQ图比较了两个样本的分位数（而不是一个样本和一个理论分配）。它们可以由lattice函数qq()生成，它具有两个主变量。在y〜x中，y必须是一个具有两个水平的因子，并且比较的样本是y的两个级别的x的子集。例如，我们可以比较男性和女性的gcsescore分布，以A级成绩为条件。 qq(gender ~ gcsescore | factor(score), Chem97, f.value = ppoints(100), type = c(&quot;p&quot;, &quot;g&quot;), aspect = 1) 该图表明，在给定的A级考试分数下，女性在GCSE考试中的表现要好于男性。换句话说，从GCSE考试到A级考试，男性倾向于进步更多，而方差则较小（第一格中的除外）。 两样本QQ图仅允许每次比较两个样本。一个著名的允许任意数量样本之间进行比较的图形设计是比较形箱形图。它们与QQ图有关：比较的值是五个“特殊”分位数，中位数，第一和第三四分位数，以及极值。详情参见?boxplot.stats。 箱须图可以通过lattice函数bwplot()生成。 bwplot(factor(score) ~ gcsescore | gender, Chem97) 盒和直线的长度减小表明方差减小，并且大量的一侧的离群值表示较重的左尾（左偏分布的特征）。 相同的箱须图可以稍有不同的布局显示，以强调更细微的变化数据中的效果：例如，gcsescore的中位数不会如人们所期望的那样单调地从左到右增长。 bwplot(factor(score) ~ gcsescore | gender, Chem97) layout参数控制列，行和页面中面板的布局（默认参数不会在此示例中有意义）。请注意，箱须图现在是垂直的，因为已经在参数中按变量顺序切换。 练习3 我们看到的所有图都表明gcsescore的分布略有倾斜，并且在每个子组中具有不同的方差。使用Box-Cox转换通常可以成功。MASS软件包中的boxcox()函数可用于查找“最佳” Box-Cox变换，在这种情况下约为2.34。重现以前的情节通过gcsescore~ 2.34来替换gcsescore重现之前的图片。你觉得这样的转换成功吗？ 练习4 并非所有工具对所有问题都有用。箱形图，以及较小程度的QQ分布图在对称和单峰分布时最有用，但其他时候会有误导性。例如，考虑由下列代码生成的图 data(gvhd10, package = &quot;latticeExtra&quot;) bwplot(Days ~ log(FSC.H), data = gvhd10) 你能从该图中得出\\(log(FSC.H)\\)的分布是什么？现在按天画\\(log(FSC.H)\\)的密度图。你还能得出与以前相同的结论吗？ 对于小样本，获取它的统计星系通常是不必要的，只需绘制所有数据即可发现有趣的分布特征。以下示例使用quakes数据集，绘制了地震震中的数量级。 stripplot(depth ~ factor(mag), data = quakes, jitter.data = TRUE, alpha = 0.6, main = &quot;Depth of earthquake epicenters by magnitude&quot;, xlab = &quot;Magnitude (Richter)&quot;, ylab = &quot;Depth (km)&quot;) 这被称为一维散点图的带状图。请注意使用抖动和部分透明性来减轻过度绘图。参数xlab，ylab和main用于添加信息标签; 这在所有lattice中都是存在的。 65.5.2 可视化表格 数据表格是一类重要的统计数据展示方式。条形图和克利夫兰点图是为表格设计的一种可视化方法。我们使用VADeaths数据集，该数据集给出了1941年在美国弗吉尼亚州的不同人群中的死亡人数。VADeaths是一个矩阵。 VADeaths ## Rural Male Rural Female Urban Male Urban Female ## 50-54 11.7 8.7 15.4 8.4 ## 55-59 18.1 11.7 24.3 13.6 ## 60-64 26.9 20.3 37.0 19.3 ## 65-69 41.0 30.9 54.6 35.1 ## 70-74 66.0 54.3 71.1 50.0 要使用lattice包，我们首先需要将其转换为data frame。 VADeathsDF &lt;-as.data.frame.table(VADeaths,responseName =&quot;Rate&quot;) VADeathsDF ## Var1 Var2 Rate ## 1 50-54 Rural Male 11.7 ## 2 55-59 Rural Male 18.1 ## 3 60-64 Rural Male 26.9 ## 4 65-69 Rural Male 41.0 ## 5 70-74 Rural Male 66.0 ## 6 50-54 Rural Female 8.7 ## 7 55-59 Rural Female 11.7 ## 8 60-64 Rural Female 20.3 ## 9 65-69 Rural Female 30.9 ## 10 70-74 Rural Female 54.3 ## 11 50-54 Urban Male 15.4 ## 12 55-59 Urban Male 24.3 ## 13 60-64 Urban Male 37.0 ## 14 65-69 Urban Male 54.6 ## 15 70-74 Urban Male 71.1 ## 16 50-54 Urban Female 8.4 ## 17 55-59 Urban Female 13.6 ## 18 60-64 Urban Female 19.3 ## 19 65-69 Urban Female 35.1 ## 20 70-74 Urban Female 50.0 条形图由barchart()函数生成，克利夫兰点图由dotplot()函数生成。两者的输入形式为y〜x（加上其他条件和分组变量），其中x和y之一应该是一个影响因素。 VADeaths条形图由以下代码生成 barchart(Var1 ~ Rate | Var2, VADeathsDF, layout = c(4, 1)) 该图可能会引起误解，因为图中最明显的视觉效果是面积的比较，但它没有任何意义。这个问题可以通过使区域与它们代表的数值成比例来解决。 barchart(Var1 ~ Rate | Var2, VADeathsDF, layout = c(4, 1), origin = 0) 更好的方法是完全放弃这些干扰端点位置区分的条形，而是使用点图。 dotplot(Var1 ~ Rate | Var2, VADeathsDF, layout = c(4, 1)) 在这个例子中，如果我们根据Var2将点分组合并，将会更有效。 dotplot(Var1 ~ Rate, data = VADeathsDF, groups = Var2, type = &quot;o&quot;, auto.key = list(space = &quot;right&quot;, points = TRUE, lines = TRUE)) 该图清楚地表明，城市女性和农村女性按年龄划分的死亡率特征几乎相同，农村男性的死亡率有所上升，而城市男性的死亡率则进一步上升。 这种情况很难在较早的图中看到。 65.5.3 通用功能和方法 高级lattice函数实际上是通用函数，使用特定方法进行工作。到目前为止，我们看到的所有示例都使用“formula”方法；也就是说，在第一个参数是公式时使用。因为barchart()和dotplot()经常用于把多维表存储为数组，lattice还包括不需要转换到数据集的合适的方法。例如，最后一个示例的替代方法是 dotplot(VADeaths, type = &quot;o&quot;, auto.key = list(points = TRUE, lines = TRUE, space = &quot;right&quot;)) 可以使用以下方法列出特定的通用函数的方法 methods(generic.function = &quot;dotplot&quot;) ## [1] dotplot.array* dotplot.coef.mer* dotplot.default* dotplot.formula* ## [5] dotplot.matrix* dotplot.numeric* dotplot.ranef.mer dotplot.table* ## see &#39;?methods&#39; for accessing help and source code 这些方法的特殊功能（如果有）在各自的帮助页面中进行了介绍；例如, 上面的示例为?dotplot.matrix。 练习5 使用“matrix”方法重现未分组的点图。 65.5.4 散点图和扩展 散点图通常用于连续的双变量数据以及时间序列数据。我们使用Earthquake数据，该数据包含在1940年至1980年间北美西部23次大地震的各个地震仪位置记录的测量结果。我们的第一个示例绘制了关于测量站到震中的距离测得的最大水平加速度。 data(Earthquake, package = &quot;nlme&quot;) xyplot(accel ~ distance, data = Earthquake) 该图显示了典型的右偏分布特征，可以通过以对数刻度绘制来进行改进。通常添加一个参考网格和某种平滑度。例如， xyplot(accel ~ distance, data = Earthquake, scales = list(log = TRUE), type = c(&quot;p&quot;, &quot;g&quot;, &quot;smooth&quot;), xlab = &quot;Distance From Epicenter (km)&quot;, ylab = &quot;Maximum Horizontal Acceleration (g)&quot;) 65.5.5 瓦块数据 像往常一样，可以通过调节因子绘制散点图。也可以在瓦块,即因子的连续分类上进行调节，由可能重叠的区间确定。 再次使用quakes数据集，我们可以通过查看一系列二维散点图来尝试了解地震震中的三维分布。 Depth &lt;- equal.count(quakes$depth, number=8, overlap=.1) summary(Depth) ## ## Intervals: ## min max count ## 1 39.5 63.5 138 ## 2 60.5 102.5 138 ## 3 97.5 175.5 138 ## 4 161.5 249.5 142 ## 5 242.5 460.5 138 ## 6 421.5 543.5 137 ## 7 537.5 590.5 140 ## 8 586.5 680.5 137 ## ## Overlap between adjacent intervals: ## [1] 16 14 19 15 14 15 15 xyplot(lat ~ long | Depth, data = quakes) 65.5.6 三维显示 当然，对于连续三变量数据，使用三维散点图可能更有效。 cloud(depth ~ lat * long, data = quakes, zlim = rev(range(quakes$depth)), screen = list(z = 105, x = -70), panel.aspect = 0.75, xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, zlab = &quot;Depth&quot;) 由于“camera”方向的作用很强，因此静态三维散点图不是很有效。不幸的是，lattice不允许交互操作查看方向。不过，一些此类图可以表明震中位置集中在三维空间中围绕两个平面。 其他三变量函数是wireframe()和levelplot()，它们以三维表面的形式显示数据。我们不会在lattice中详细讨论这些功能以及其他高级功能，但是可以在其帮助页面中找到示例。 练习5 尝试通过修改screen参数来更改上一个图中的查看方向。 65.5.7 网格（trellis）对象 lattice与传统R图形不同的一个重要特征是，高级函数实际上不绘制任何内容。相反，它们返回“trellis”对象，该对象需要print()或plot()。R的自动打印规则意味着在大多数情况下，用户看不到任何行为上的差异。在下面的例子中，我们对“trellis”对象使用plot()方法的可选参数来并排显示两个图。 dp.uspe &lt;- dotplot(t(USPersonalExpenditure), groups = FALSE, layout = c(1, 5), xlab = &quot;Expenditure (billion dollars)&quot;) dp.uspe.log &lt;- dotplot(t(USPersonalExpenditure), groups = FALSE, layout = c(1, 5), scales = list(x = list(log = 2)), xlab = &quot;Expenditure (billion dollars)&quot;) plot(dp.uspe, split = c(1, 1, 2, 1)) plot(dp.uspe.log, split = c(2, 1, 2, 1), newpage = FALSE) 65.6 更多资源 我们所涵盖的材料应足够理解含有lattice的在线文档。其他有用的材料是 Lattice 手册， Springerde的UseR!系列的一部分。网站链接： http://lmdvr.r-forge.r-project.org 它包含了所有手册中出现的图片和代码。 Bell Labs的Trellis网站 http://netlib.bell-labs.com/cm/ms/departments/sia/project/trellis/ 提供了大量关于Trellis的材料，这些材料也同样适用于lattice（当然，不涵盖lattice特有的特征） 65.6.1 版本信息 R版本2.14.0，开发中（不稳定）（2011-05-07 r55801），x86_64-unknown-linux-gnu 区域设置：LC_CTYPE=en_IN, LC_NUMERIC=C, LC_TIME=en_IN, LC_COLLATE=en_IN, LC_MONETARY=en_IN, LC_MESSAGES=en_IN, LC_PAPER=C, LC_NAME=C, LC_ADDRESS=C, LC_TELEPHONE=C,LC_MEASUREMENT=en_IN, LC_IDENTIFICATION=C 基本包：base, datasets, graphics, grDevices, methods, stats, utils 其他包：lattice 0.19-26 加载工具：grid 2.14.0, tools 2.14.0 "],
["ggmosaic-chinese.html", "Chapter 66 ggmosaic 66.1 Chinese Translation: ‘ggmosaic’（马赛克图） 66.2 引言 66.3 简介 66.4 分割的顺序 66.5 根据一个变量分割（分箱数据）： 66.6 根据一个变量分割(非分箱数据): 66.7 根据两个变量分割 66.8 根据三个变量分割 66.9 调整切割的方向 66.10 另外一种方法:条件变量(Conditional) 66.11 另外一种方法:块化(Facet) 66.12 ‘ggmosaic’ vs vcd::‘mosaic’", " Chapter 66 ggmosaic Qiang Zhao Mike Yao-Yi Wang English version 66.1 Chinese Translation: ‘ggmosaic’（马赛克图） 66.2 引言 这个文档是参考edav.info中第十五章节：马赛克图（Mosaic），引用其中的数据和例子。相较于edav.info中vcd包里的mosiac函数，我们准备使用ggmosiac来画马赛克图。 66.3 简介 马赛克图针对的是分类变量 geom_maisc中的变量 weight : 数据中的计数栏 x : product（因变量，自变量2，自变量1） fill : 数据中的自变量栏 conds : 条件变量 66.4 分割的顺序 马赛克图遵循等级分层结构，因此往product里面加变量的顺序极其重要。下面，我们会一步一步的展示如何正确的加入变量。我们首先要安装并且引用ggplot2和ggmosaic包。 library(ggplot2) library(ggmosaic) df_bin=data.frame(Age=c(&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;), Favorite=c(rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2),rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2)), Music=c(rep(c(&#39;classical&#39;,&#39;rock&#39;),4)), Freq=c(1,1,3,1,2,5,1,0)) df_unbin = data.frame(Age =c(rep(&quot;old&quot;,6), rep(&quot;young&quot;, 8)), Favorite = c(rep(&quot;bubble gum&quot;, 2),rep(&quot;coffee&quot;, 4), rep(&quot;bubble gum&quot;, 7), &quot;coffee&quot;), Music = c(&quot;classical&quot;, &quot;rock&quot;, rep(&quot;classical&quot;, 3), &quot;rock&quot;, rep(&quot;classical&quot;, 2), rep(&quot;rock&quot;, 5), &quot;classical&quot;)) 66.5 根据一个变量分割（分箱数据）： df_bin ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 ## 7 young coffee classical 1 ## 8 young coffee rock 0 首先，我们根据年龄（Age）分割： 注意：ggmosaic可以通过weight来处理分箱数据，我们令weight等于数据中的计数栏（Freq）即可。vcd::mosaic也可以做到同样效果，但是计数栏的名称一定要为Freq。 ggplot(data = df_bin)+ geom_mosaic(aes(x = product(Age), fill = Age, weight = Freq))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(binned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 66.6 根据一个变量分割(非分箱数据): 对于非分箱数据，我们应该忽略weight。下面是非分箱数据的读数: df_unbin ## Age Favorite Music ## 1 old bubble gum classical ## 2 old bubble gum rock ## 3 old coffee classical ## 4 old coffee classical ## 5 old coffee classical ## 6 old coffee rock ## 7 young bubble gum classical ## 8 young bubble gum classical ## 9 young bubble gum rock ## 10 young bubble gum rock ## 11 young bubble gum rock ## 12 young bubble gum rock ## 13 young bubble gum rock ## 14 young coffee classical ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age), fill = Age))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(unbinned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：我们接下来的例子都是使用非分箱的数据 66.7 根据两个变量分割 我们首先根据年龄(Age)分割，然后再根据音乐种类(Music)分割: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Music, Age), fill = Music))+ labs(x = &quot;Age&quot;, y = &quot;Music&quot;, title = &quot;Spliting on Age, then Music&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 下面这个例子是先分割音乐种类(Music)，再分割年龄(Age): ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age, Music), fill = Age))+ labs(x= &quot;Music&quot;, y = &quot;Age&quot;, title = &quot;Spliting on Music, then Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 如果我们想画因变量Y关于自变量X的马赛克图，我们应该设aes中的x=prod(Y,X)。我们要保证因变量是最后一个被划分的。而且, 我们也要使得fill=Y，因为我们注重的是因变量Y的分布。 66.8 根据三个变量分割 我们首先划分年龄(Age),然后划分音乐种类(Music)，最后划分喜好(Favorite)。 ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite))+ labs(x = &quot;Favorite:Age&quot;, y = &quot;Music&quot;, title = &quot;Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：在上面的例子当中，系统默认的切割方向以及顺序如下: 年龄(Age): 垂直切割 音乐种类(Music): 横向切割 喜好(Favorite): 垂直切割 66.9 调整切割的方向 我们可以随意改变任意变量的切割方向。比如，我们打算用上面的例子绘画一个双层结构图(DoubleDecker Plot)。 切割顺序为： 年龄(Age): 垂直切割 (‘hspine’) 音乐种类(Music): 垂直切割 (‘hespine’) 喜好(Favorite): 横向切割 (‘vspine’) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：divider中的切割方向分别对应product(Favorite,Music,Age)。但是实际上的切割顺序还是年龄(Age)，音乐种类(Music)，喜好(Favorite)。divider中的’vspine’表示横向切割；’hspine’表示垂直切割。 66.10 另外一种方法:条件变量(Conditional) 我们可以使用条件(conds)的属性来达到与上面例子相同的效果。 geom_mosaic(aes(x = product(最后一次切割), fill = 最后一次切割, conds = product(第二次切割, 第一次切割)). ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite), fill = Favorite, conds = product(Music, Age)), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - (Favorite | Music, Age)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 66.11 另外一种方法:块化(Facet) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music), fill = Favorite))+ facet_grid(. ~Age)+ labs(x=&quot;Music&quot;, y = &quot;favorite&quot;, title = &quot;Favorite ~ Music and facet on Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 66.12 ‘ggmosaic’ vs vcd::‘mosaic’ 我们很容易搞混’ggmosaic’和‘mosaic’的切割顺序和切割方向。 对于‘mosaic’来说，切割顺序服从公式’mosaic(最后一个切割~第一个切割+第二个切割)’，而且切割方向的变量也是对应着direction=(第一个切割的变量，第二个切割的变量，最后一个切割的变量)。其中’v’表示的是垂直切割,’h’表示的是横向切割。 但是这和‘ggmosaic’有着很大的不同。在‘ggmosaic’中，切割应服从’product(最后一个切割，第二个切割，第一个切割)’的一个倒叙顺序。而且切割方向的变量也对应着divider=(最后一个切割的变量，第二个切割的变量，第一个切割的变量)。其中’vspine’表示横向切割，‘hspine’表示垂直切割。 "],
["chinese-translation-links.html", "Chapter 67 Chinese translation links 67.1 R and ggplot2 67.2 forcats package 67.3 Continuous variables with R (Chinese) 67.4 Visualising Spatial Data", " Chapter 67 Chinese translation links Some groups of students have contributed to the community by translating useful resources into another language. 67.1 R and ggplot2 Yuchen Pei and Jiaqi Tang We translated two online tutorials for visualizaiton in R into Chinese. The first one is called A Comprehensive Guide to Data Visualization in R for Beginners and the second one is called ggplot2: Mastering the basics. Our translation files can be found here: https://github.com/Jasmine1231/EDAV-19Fall-Community_Contribution . 67.2 forcats package Xu Xu and Xiaoyun Zhu 引言： 说到数据分析，我们不得不提大神 Hadley Wickham。 Hadley Wickham 是 RStudio 的首席科学家以及 Stanford University, Rice University 统计系的兼职教授。他是著名图形可视化软件包 ggplot2 的开发者，以及其他许多被广泛使用的软件包的作者，代表作品如 plyr、reshape2 等。 Wickham曾说过：“通过数据从根本上了解世界真的是一件非常，非常酷的事情”，而整理数据正是透过数据看世界的第一步。 作为一个多产的R开发者，Wickham乐于给那些喜欢摆弄数据的人提供力量和支持。他就创建了一个便于整理分类数据(categorical data)的包forcats，用于处理因子，可以更高效地对因子进行修改。 下面我们翻译了一篇R-bloggers上讲解forcats用法的文章（作者是S.Richter-Walsh)，希望这个教程可以让大家在整理数据时更得心应手。 这是原文链接，供大家参考：https://www.r-bloggers.com/cats-are-great-and-so-is-the-forcats-r-package/ 翻译正文： Forcats是由Hadley Wickham创建的一个做数据整理时非常好用的包。在进行数据分析和建模之前，我们经常需要花大量的时间来清理数据（或者说预处理数据）。要我估计的话，我认为一个数据科学家会花至少70%-80%的时间来清理数据。这也是学校所教和业界真实项目之间最大的区别。学校教学时所用的数据集经常是预处理过后整齐的数据，但实际工作中的数据集基本不可能是这样的。我很喜欢清理数据，并喜欢分析在清理过程中出现的问题。我发现forcats这个包在整理分类数据时非常有效。 67.2.1 示范数据准备 我们用下面的代码生成一些数据用于示范。这个数据集是关于销售数据的，其中有50个缺失数据(NA), 7个因子。 library(dplyr) # Also load up dplyr so we can use the pipe operator: %&gt;% library(forcats) df &lt;- data_frame(sales = factor(rep(c(&quot;Online&quot;, &quot;Post&quot;, &quot;Web&quot;, &quot;Call Centre&quot;, &quot;Inbound Phone&quot;, &quot;Outbound Phone&quot;, &quot;Field Sales&quot;, NA), 50)), buy = sample(c(0, 1), 400, replace = T)) %&gt;% mutate(sales = sample(sales, size = length(sales), replace = T)) table(df$sales) ## ## Call Centre Field Sales Inbound Phone Online Outbound Phone ## 49 47 57 41 48 ## Post Web ## 61 53 67.2.2 关于缺失数据(NAs)的处理 缺失数据在实际工作中很常见。在R的数据框(dataframe)中计算连续变量的均值(mean)，中位数(median)，方差(variance)和标准差(standard deviation)时，我们需要考虑这些缺失数据。另外，如果我们希望用某个数据集来建模，我们也需要处理缺失数据来保留某些特定的变量和防止数据的缺失。我们常用的策略有用均值或中位数来代替连续型数据中的缺失值或用众数来代替分类数据中的缺失值。但有些情况下这些策略并不可取，我们有时需要将缺失数据设为一个明确的因子水平。我们可以通过forcats::fct_explicit_na()来实现这个目标，而且只需要一行代码。下面就让我们在示范数据上尝试一下： df$sales &lt;- fct_explicit_na(df$sales) table(df$sales) ## ## Call Centre Field Sales Inbound Phone Online Outbound Phone ## 49 47 57 41 48 ## Post Web (Missing) ## 61 53 44 通过上面的处理，现在缺失数据都由一个明确的因子来表示，之前的缺失数值直接由(Missing)来代替。我们还可以用下面的代码来给这个新的因子命名： df$sales &lt;- fct_explicit_na(df$sales, na_level = &quot;My New Level&quot;) 67.2.3 同义因子水平 有时分类变量会包含两个及更多指向同一分组的因子水平。语法表示可能会有细微差别，比如以大写字母开头和以小写字母开头（GroupA vs. groupA）。在这种情况下，我们可以用 forcats::fct_collapse() 来合并多个同义分子水平到一个里。在我们的测试数据中，让我们假设Web和Online指向同一销售渠道。我们想要合并这两个成为一个名为Online的因子水平。 df$sales &lt;- fct_collapse(df$sales, Online=c(&quot;Online&quot;, &quot;Web&quot;)) 67.2.4 混合多个频率低的因子水平成为一个 另一种可能发生的情况是，我们想使用需要大的样本容量来维持统计显著性的数据群组进行分析或者建模。 设想一个因子变量有20个水平， 但是只有其中的5个能用来解释数据集中90%以上的观测值。你可以全部剔除这些观测值，但是如果可能，要尽量避免数据丢失。或者，你可以混合多个频率低的因子水平成为一个覆盖它们全部的水平，在保持其余的和这些观测值相关的属性变量不变的同时整理了群组水平。这个函数混合多个频率低的水平成为一个名为Other的默认水平，并且保持这个水平中的数据数量为全部水平中最小的数量。使用者还可以用n参数来调整混合之后所保留的水平数量。在我们的测试数据中，名为Outbound，Phone 的水平是频率最低的，所以它们被混合进了一个新的名为Other的水平中。 df$sales &lt;- fct_lump(df$sales) 67.2.5 在ggplot2 条形图中改变条的顺序 tidyverse是一系列包含dplyr，ggplot2，和forcats的包。这个包为数据科学家们创造了一个完美的工具生态系统。我想要介绍一下forcats::fct_infreq() 函数。这个函数可以在探索性数据分析和数据展示的阶段与ggplot2一同使用。有时改变条形图中因子的顺序可能有些难办，但是使用forcats包可以使这个任务简单化。 library(ggplot2) ggplot(df, aes(x = fct_infreq(sales))) + geom_bar() 现在因子水平按照频率递减的顺序排列好了，并且包含了（缺失的）和Other。 通过使用forcats来进行一些快速的预处理，我们没有丢失任何原数据。祝大家天天开心！ 67.3 Continuous variables with R (Chinese) Bangwei Zhou and Zhihao Ai We created a tutorial in Chinese on the content of continuous variables with R. We combined (and translated) texts from chapter three of the textbook Graphical Data Analysis with R by Antony Unwin and the Continuous Variables section on edav.info. Additionally, we also included an example to better illustrate how a user can fully utilize R to assess continuous variables from PSET1 problem 1. We hope this document can effectively jumpstart any user (with limited language background to Chinese) with sufficient skills to assess continuous variables with R. Our document can be found here. 67.4 Visualising Spatial Data Mutian Wang and Siyuan Wang We translated a tutorial Introduction to Visualising Spatial Data in R written by Robin Lovelace, James Cheshire, Rachel Oldroyd. The source text can be found here. Our translation can be found on our GitHub repo. In this repo, you can see our translation in the html file, and the source code in the Rmd file. "],
["edav-info.html", "Chapter 68 edav.info", " Chapter 68 edav.info Amaury Sudrie, Maxime Tchibozo, Romane Goldmuntz and Vy Tran The four of us worked on the French translation of the website edav.info/, a useful resource of GR5702 course at Columbia University. The version is the one of October 21st 2019, any update is welcome! Link to the repo: https://github.com/Amelrich/EDAV "],
["heatmaps.html", "Chapter 69 Heatmaps", " Chapter 69 Heatmaps Ji In Choi and Jung Ah Shin Source: https://edav.info/heatmap.html 69.0.1 R Markdown 69.0.2 개요 이번 단원은 히트맵을 만드는 과정을 다룬다. 69.0.3 tl;dr 다음은 미국, 영국 및 일본에 있는 아버지와 아들의 직업에 대한 히트맵이다. 아래의 코드를 참고하면 된다. library(vcdExtra) # dataset library(dplyr) # manipulation library(ggplot2) # plotting library(viridis) # color palette ## format data orderedclasses &lt;- c(&quot;Farm&quot;, &quot;LoM&quot;, &quot;UpM&quot;, &quot;LoNM&quot;, &quot;UpNM&quot;) mydata &lt;- Yamaguchi87 mydata$Son &lt;- factor(mydata$Son, levels = orderedclasses) mydata$Father &lt;- factor(mydata$Father, levels = orderedclasses) japan &lt;- mydata %&gt;% filter(Country == &quot;Japan&quot;) uk &lt;- mydata %&gt;% filter(Country == &quot;UK&quot;) us &lt;- mydata %&gt;% filter(Country == &quot;US&quot;) ### convert to % of country and class total mydata_new &lt;- mydata %&gt;% group_by(Country, Father) %&gt;% mutate(Total = sum(Freq)) %&gt;% ungroup() ### make custom theme theme_heat &lt;- theme_classic() + theme(axis.line = element_blank(), axis.ticks = element_blank()) ### basic plot plot &lt;- ggplot(mydata_new, aes(x = Father, y = Son)) + geom_tile(aes(fill = Freq/Total), color = &quot;white&quot;) + coord_fixed() + facet_wrap(~Country) + theme_heat ### plot with text overlay and viridis color palette plot + geom_text(aes(label = round(Freq/Total, 1)), color = &quot;white&quot;) + scale_fill_viridis() + ## formatting ggtitle(&quot;Like Father, Like Son&quot;, subtitle = &quot;Heatmaps of occupational categories for fathers and sons, by country&quot;) + labs(caption = &quot;Source: vcdExtra::Yamaguchi87&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) 이 데이터에 대해 더 자세한 정보를 얻으려면 콘솔(console)에 ?vcdExtra::Yamaguchi87 을 입력하면 된다. 69.0.4 간단한 예제들 69.0.5 2-차원 빈 카운트를 사용한 히트 맵 이 히트 맵에는 SpeedSki 데이터 셋을 사용할 것이다. 2-차원 빈 카운트 히트 맵은 2 개의 변수 x와 y 만 필요로 한다. 세 번째 변수, 즉 색상은 해당 영역의 빈 포인트 수를 나타낸다. 이 히트맵을 2 차원 히스토그램으로 생각하면 된다. 히트 맵을 만들려면 geom_point () 대신 geom_bin2d ()을 사용한다. library(ggplot2) # plotting library(GDAdata) # data (SpeedSki) ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() 69.0.6 데이터 프레임의 히트 맵 데이터 프레임을 시각적으로 이해하려면 히트 맵을 사용할 수 있다. 열의 크기를 조정하여 공통적인 척도로 데이터를 파악할 수도 있다. 이 예시에서는 geom_tile을 사용하여 데이터 프레임의 모든 셀을 그래프로 표시하고 값으로 색상을 지정한다. library(pgmm) # data library(tidyverse) # processing/graphing library(viridis) # color palette data(wine) ### convert to column, value wine_new &lt;- wine %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_new, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe&quot;) ### 다른 스케일링을 사용한 히트맵 wine_scaled &lt;- data.frame(scale(wine)) %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_scaled, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe, Scaled&quot;) 69.0.7 수정 ggplot 함수 호출 체인에서 색상 팔레트를 지정하여 색상 팔레트를 변경할 수 있다. bin 너비는 geom_bin2d () 함수 내부에 추가 할 수 있다. library(viridis) # viridis color palette library(viridis) # viridis color palette library(GDAdata) # data (SpeedSki) ### create plot g1 &lt;- ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_viridis() # 색상 변경 ### show plot g1 + geom_bin2d(binwidth = c(5, 5)) # bin 너비 변 또 다른 예는 다음과 같다. ### 더 큰 빈 너비를 사용한 히트맵 g1 + geom_bin2d(binwidth = c(10, 10)) ###육각형 빈을 사용한 히트맵 g1 + geom_hex(binwidth = c(5, 5)) ### 육각형 빈과 산포도를 사용한 히트맵 g1 + geom_hex(binwidth = c(5, 5), alpha = .4) + geom_point(size = 2, alpha = 0.8) ### 사용자 정의를 사용한 커스텀 색상 그라디언트와 빈 카운트를 가진 육각형 빈 ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_gradient(low = &quot;#cccccc&quot;, high = &quot;#09005F&quot;) + # color geom_hex(bins = 10) # number of bins horizontally/vertically 69.0.8 이론 히트맵은 산점도 (Scatterplot) 와 히스토그램의 조합과 유사하다.상대 분포를 보는 동시에 여러 모수를 비교할 수 있기 때문이다. - 히트맵이 시각적으로 눈에 띄지만 정보 전달에는 다른 종류의 그래프들이 훨씬 더 효과적일 수 있다. 더 자세한 내용은 아래 링크를 참고하면 된다. https://campus.datacamp.com/courses/data-visualization-with-ggplot2-2/chapter-4-best-practices?ex=10 69.0.9 추가 자료 https://www.r-graph-gallery.com/heatmap/ : heatmap() 함수를 사용해서 히트맵을 만든 예시들이 있다. https://www.r-bloggers.com/how-to-make-a-simple-heatmap-in-ggplot2/ : Geom_title() 을 사용해서 히트맵을 만드는 방법. "],
["nullabor.html", "Chapter 70 nullabor 70.1 nullaobr 패키지 입문 70.2 nullbor의 lineup 예시 70.3 무수(null) 와 데이터 포인츠들간의 거리계산", " Chapter 70 nullabor Jinwoo Jung and Tae Yoon Lim This is a Korean translated version of nullabor packages (i.e. nullabor.html, nullabor-examples.html, and distance.html) 70.1 nullaobr 패키지 입문 nullabor 패키지는 그래프에 보이는 통계적 유의성을 계량화하는 함수를 제공한다. library(nullabor) library(ggplot2) library(dplyr) 패키지내 함수들은 두가지 경우에 대한 방법을 제공한다: lineup 와 rorschach. lineup의 경우 실제 데이터의 그래프를 여러 무수의 데이터 그래프 사이에 끼워넣고, Rorschach 경우, 모든 그래프들이 무수의 데이터 그래프이다. encrypt 는 실제 데이터의 위치를 비밀로 만들수 있고, decrypt 를 사용해 위치를 찾을 수 있다. R내에는 여러 함수들을 이용해 이 무수의 데이터를 만들 수 있고 예시는 다음과 같다: null_permute, null_lm, null_dist. 70.1.1 lineup 방법 이 방법에서는, 실제 데이터 그래프가 무수의 데이터 그래프들 사이에 끼어있다. 매트릭스 형태의 여러 그래프들이 이 &quot;lineup&quot; 으로 알려져 있다. 무수의 데이터 그래프들은 귀무가설에 동의하는 방법으로 생성된다. 이렇게 생성된 &quot;lineup&quot;을 관찰자에게 보여주고, 만약 관찰자가 실제 데이터를 무수의 데이터들 사이에서 고를 수 있다면, 실제 데이터의 통계적 유의성에 무게가 실린다. &quot;lineup&quot; 함수는 무수의 데이터집합과 실제 데이터를 임의적인 순서로 같은 집합의 값을 돌려준다. &quot;lineup&quot; 함수에 실제 데이터만을 입력값으로 넣으면 함수 자체내에서 무수의 데이터들이 생성된다. 사용자들은 또한 자신들의 방법으로 무수의 데이터를 생성후 lineup의 입력값에 넣을 수 있다. 실제 데이터의 위치는 기입 안해도 되고 함수자체가 임의적으로 실제 데이터의 위치를 지정한다. 그 후 함수는 실제 데이터의 위치를 암호화 된 코드로 반환한다. 암호화된 코드는 콘솔에 복사, 붙여넣기를 통하여 실제 위치를 알 수 있다. 밑에 제시된 예시를 참조하길 바란다. d &lt;- lineup(null_permute(&quot;mpg&quot;), mtcars) head(d) ## mpg cyl disp hp drat wt qsec vs am gear carb .sample ## 1 10.4 6 160 110 3.90 2.620 16.46 0 1 4 4 1 ## 2 27.3 6 160 110 3.90 2.875 17.02 0 1 4 4 1 ## 3 21.0 4 108 93 3.85 2.320 18.61 1 1 4 1 1 ## 4 14.7 6 258 110 3.08 3.215 19.44 1 0 3 1 1 ## 5 13.3 8 360 175 3.15 3.440 17.02 0 0 3 2 1 ## 6 15.2 6 225 105 2.76 3.460 20.22 1 0 3 1 1 # 실제 데이터의 위치 attr(d, &quot;pos&quot;) ## [1] 17 &quot;lineup&quot; 데이터는 ggplot2 패키지를 통하여 시각화 할 수 있다. 이렇게 시각화된 데이터를 가지고 많은 사람들에게 실제 데이터를 생선된 무수의 데이터들 사이에서 고를 수 있는지 없는지를 통하여 귀무 가설을 실험할 수 있다. 만약 대중들 중에 실제 데이터 그래프를 정확하게 집어 낼 수 있다면, 귀무 가설을 수용 하지 않고 실제 데이터가 좀 더 강한 분포를 갖고 있다고 결론 낼수 있다. 밑에 제공된 예시는 시각화 하는 방법이다. ggplot(data=d, aes(x=mpg, y=wt)) + geom_point() + facet_wrap(~ .sample) 70.1.2 Rorschach 방법 Rorschach 방법은 표집으로 인한 다양성을 사람의 눈으로 확인 할 수 있는지를 확인할 때 사용한다. 그래프들은 무수의 데이터들로 만들어 졌고, 이 무수의 데이터들은 귀무 가설에 수용한다. &quot;rorschach&quot; 함수는 무수의 데이터로 만들어진 그래프의 집합을 반환한다. &quot;lineup&quot; 함수와 같이 무수를 만드는 방법은 입력값에 실제 데이터와 함께 제공되어야 한다. 실제 데이터를 포함할지 안할지에 대한 확률 또한 입력값으로 넣을 수 있다. 아래 예시를 참조하길 바란다. d &lt;- rorschach(null_permute(&quot;mpg&quot;), mtcars, n = 20, p = 0) ggplot(data=d, aes(x=mpg, y=wt)) + geom_point() + facet_wrap(~ .sample) 70.1.3 특정 분포를 가진 무수의 데이터 생성하기 &quot;null_dist&quot; 함수는 데이터 변수의 이름과 특정 분포를 입력 값으로 받는다. 그러면 이 변수는 임의로 특정 분포를 가진 데이터로 생성된다. 특정 분포 입력값에 들어 갈 수 있는 분포는 다음과 같다: beta(베타), cauchy(카우치), chi-squared(카이제곱), exponential(지수분포), f(F분포), gamma(감마 분포), geometric(기하 분포), log-normal(로그 정규 분포), lognormall(로그 정규 분포), logistic(로그 분포), negative binomial(음수 이항 분포), normal(정규 분포), poisson(푸아송 분포), t(t 분포) and weibull(베이풀 분포). 입력값에 이러한 분포 값을 포함 시킬 수 있다. 만약 분포에 대한 값이 입력값에 포함되지 않을 경우 &quot;fitdistr&quot; 가 사용되어 제공된 데이터를 가늠한다. &quot;null_dist&quot;함수는 무수의 데이터 집합을 생성하는 함수를 반환한다. 아래의 예시를 참조하길 바란다. head(null_dist(&quot;mpg&quot;, dist = &quot;normal&quot;)(mtcars)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 17.711471 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 19.329862 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.891983 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 24.701859 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 6.846055 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.694908 6 225 105 2.76 3.460 20.22 1 0 3 1 70.1.4 순열을 통한 무수의 데이터 생성하기 &quot;null_permute&quot; 함수는 변수의 이름을 입력값으로 받는다. 이 변수를 순열하여 무수의 데이터 집합을 얻는다. &quot;null_permute&quot; 함수는 무수의 데이터 집합을 생성하는 함수를 반환한다. 아래의 예시를 참조하길 바란다. head(null_permute(&quot;mpg&quot;)(mtcars)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 18.1 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 15.5 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 21.5 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 16.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 22.8 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 30.4 6 225 105 2.76 3.460 20.22 1 0 3 1 70.1.5 모델에서의 무수 잔차를 이용해 무수의 데이터 생성하기 &quot;null_lm&quot;함수는 모델에 대한 선형 회귀식 정보&quot;lm&quot;와 무수 잔차 생성에 관한 방법을 입력값으로 받는다. 세가지의 빌트-인 방식은 다음과 같다: &quot;resid_rotate&quot;, &quot;resid_pboot&quot;,&quot;resid_boot&quot; 함수는 무수의 데이터 집합을 생성하는 함수를 반환한다. 아래의 예시를 참조하길 바란다. head(null_lm(wt~mpg, method = &#39;rotate&#39;)(mtcars)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 3.551389 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 3.162937 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 3.283246 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.043491 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 4.449837 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.821975 20.22 1 0 3 1 ## .resid .fitted ## Mazda RX4 0.46223502 3.089154 ## Mazda RX4 Wag 0.07378361 3.089154 ## Datsun 710 0.44764432 2.835602 ## Hornet 4 Drive 0.01068180 3.032809 ## Hornet Sportabout 1.03670071 3.413136 ## Valiant 0.32432142 3.497653 70.1.6 nullabor 밖의 데이터 생성하기 만약 무수 생성 매커니즘이 특별하다면 이 패키지 외에서 이러한 방법을 만드는 것은 가치가 있다. lineup 함수는 임의로 데이터 그래프의 포지션을 지정후 암호화 하는데 사용 할 수 있다. 좋은 예시로는 다음이 있고, Roy Chowdhury, N. et al (2015), 이 논문의 경우 데이터는 고차원으로 이루어 져 있고 종속변수는 정량형 변수 이다. 우리는 선형 차원 감소와 같은 차원 감소 방법을 통해 저차원의 방식으로 그룹을 나눈다. 무수 생성의 방법으로는 그룹들의 레이블을 순열하고 차원 감소를 다시 한번 실행한다. 아래의 예시를 참조하길 바란다. library(MASS) data(wasps) wasp.lda &lt;- lda(Group~., data=wasps[,-1]) wasp.ld &lt;- predict(wasp.lda, dimen=2)$x true &lt;- data.frame(wasp.ld, Group=wasps$Group) wasp.sim &lt;- data.frame(LD1=NULL, LD2=NULL, Group=NULL, .n=NULL) for (i in 1:19) { x &lt;- wasps x$Group &lt;- sample(x$Group) x.lda &lt;- lda(Group~., data=x[,-1]) x.ld &lt;- predict(x.lda, dimen=2)$x sim &lt;- data.frame(x.ld, Group=x$Group, .n=i) wasp.sim &lt;- rbind(wasp.sim, sim) } pos &lt;- sample(1:20, 1) d &lt;- lineup(true=true, samples=wasp.sim, pos=pos) ggplot(d, aes(x=LD1, y=LD2, colour=Group)) + facet_wrap(~.sample, ncol=5) + geom_point() + theme(aspect.ratio=1) attr(d, &quot;pos&quot;) ## [1] 11 70.1.7 유의확률 계산하기 pvisual 함수는 라인업의 결과를 독립적인 관찰자들에게 제시한 시각화된 유의확률을 계산할 수 있다. 입력값으로는 (1) 라인업에 포함된 그래프 수, m(디폴트= 20) (2) 그래프를 읽는 관찰자 수, K (3) 실제 데이터를 뽑은 관찰자 수가 있다. 아래의 예시로는 10명의 관찰자가 있었고 그 중 4명이 맞았다고 가정해보자. 그렇다면 시각화 유의확률 이항을 통해 계산하고 모의로 종속 여부에 관하여 계산한다. pvisual(4, 10) ## x simulated binom ## [1,] 4 0.002 0.001028498 70.1.8 검정력 계산하기 visual_power 함수는 각 라인업의 검정력을 계산하는데 사용된다. 아래 예시를 참조하길 바란다. data(turk_results) visual_power(turk_results) ## # A tibble: 6 x 3 ## pic_id power n ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 36 0 18 ## 2 105 0.746 17 ## 3 116 0.125 16 ## 4 131 0.842 14 ## 5 159 0.656 15 ## 6 225 0.130 15 70.2 nullbor의 lineup 예시 70.2.1 선거 개찰 이 예시의 주제는 각 주의 표차를 여론 조사와 정규 분포를 통한 표집으로 실제 선거날의 잠재적 표차를 벡터화 시켜 알아보는 것이다. 이 예시에서 사용된 데이터는 2012년 미국 대선이다. . simPoll &lt;- function(trueData) { simMargin &lt;- rnorm(nrow(trueData), mean=trueData$Margin, sd=2.5) simDemocrat &lt;- ((simMargin&gt;0) &amp; trueData$Democrat) | ((simMargin&lt;0) &amp; !trueData$Democrat) simMargin &lt;- abs(simMargin) res &lt;- trueData res$Democrat &lt;- simDemocrat res$Margin &lt;- simMargin res } simPoll 함수는 각주로 부터의 개표결과를 받은 후 임의 값을 평균 값을 개표결과로 둔 정규 분포를 통해 만드는 데에 상대적으로 특화 되어 있다. 지금부터 각 주의 표준편차 (또는 정확성) 을 2.5라고 가정하겠다. sim1 첫번째 모의의 예시이다 - 이 모의에 의하면 우리는 민주당의 대선 표를 모의 개표날의 결과로 구할 수 있다. 아래의 코드를 참조 하길 바란다. data(electoral, package=&quot;nullabor&quot;) margins &lt;- electoral$polls sim1 &lt;- simPoll(margins) sum(sim1$Electoral.Votes[sim1$Democrat]) ## [1] 337 simPoll 함수가 실제 데이터와 똑같은 형식으로 데이터 집합을 반환하기에, 우리는 이 함수를 이용하여 lineup 방법으로 개표 모의에 관한 집합을 얻을 수 있다. 우리는 실제 데이터의 위치를 계속 알고 싶기에 라인업 방법에 입력값을 지정해야한다. (하지만 아래 예시에서는 일단 위치를 숨기기에 입력값에 포함시키지 않는다). pos &lt;- sample(20,1) lpdata &lt;- nullabor::lineup(method = simPoll, true=margins, n=20, pos=pos) dim(lpdata) ## [1] 1020 5 summary(lpdata) ## State Electoral.Votes Margin Democrat ## Alabama : 20 Min. : 3.00 Min. : 0.00119 Mode :logical ## Alaska : 20 1st Qu.: 4.00 1st Qu.: 6.89044 FALSE:523 ## Arizona : 20 Median : 8.00 Median :13.61451 TRUE :497 ## Arkansas : 20 Mean :10.55 Mean :15.98530 ## California: 20 3rd Qu.:12.00 3rd Qu.:22.00660 ## Colorado : 20 Max. :55.00 Max. :90.18677 ## (Other) :900 ## .sample ## Min. : 1.00 ## 1st Qu.: 5.75 ## Median :10.50 ## Mean :10.50 ## 3rd Qu.:15.25 ## Max. :20.00 ## 우리는 개표 결과를 실제 대선 개표 결과와 바꾸어 주어야 한다. election &lt;- electoral$election idx &lt;- which(lpdata$.sample==pos) lpdata$Margin[idx] &lt;- election$Margin 그리고 이제 실제 데이터를 이용한 그래프를 그리겠다. 이 경과에서 우리는 데이터를 재구조해야한다: library(dplyr) lpdata &lt;- lpdata %&gt;% arrange(desc(Margin)) lpdata &lt;- lpdata %&gt;% group_by(.sample, Democrat) %&gt;% mutate( tower=cumsum(Electoral.Votes[order(Margin, decreasing=TRUE)]) ) lpdata$diff &lt;- with(lpdata, Margin*c(1,-1)[as.numeric(Democrat)+1]) 이제 우리는 그래프의 매트릭스를 얻을 수 있다: library(ggplot2) dframe &lt;- lpdata dframe$diff &lt;- with(dframe, diff+sign(diff)*0.075) dframe$diff &lt;- pmin(50, dframe$diff) ggplot(aes(x=diff, y=tower, colour = factor(Democrat)), data=dframe) + scale_colour_manual(values=c(&quot;red&quot;, &quot;blue&quot;), guide=&quot;none&quot;) + scale_fill_manual(values=c(&quot;red&quot;, &quot;blue&quot;), guide=&quot;none&quot;) + scale_x_continuous(breaks=c(-25,0,25), labels=c(&quot;25&quot;, &quot;0&quot;, &quot;25&quot;), limits=c(-50,50)) + geom_rect(aes(xmin=pmin(0, diff), xmax=pmax(0,diff), ymin=0, ymax=tower, fill=Democrat), size=0) + geom_vline(xintercept=0, colour=&quot;white&quot;) + facet_wrap(~.sample) + theme(axis.text=element_blank(), axis.ticks=element_blank(), axis.title=element_blank(), plot.margin=unit(c(0.1,0.1,0,0), &quot;cm&quot;)) 스스로 눈으로 보고 실제 데이터 값을 구할수 있는지 알아보길 바란다. 이 라인업 중 어떤것이 다른거 같나? 스스로 답을 내보고 밑에 값과 비교해보자. pos ## [1] 10 70.3 무수(null) 와 데이터 포인츠들간의 거리계산 70.3.1 소개 데이터 포인트들을 계수적으로 설명하는것은 무척이 어렵습니다. 이러한 함수들은 나열해져잇는 상황속에서 특수한 상황을 측정할때 필요한데, 실제 데이터 포인트들이 무수(null)로 부터 어떻게 다른지 찾는데에 쓰입니다. 운율학(Metrics)은 나열되있는 그래프들관의 상관관계에 있어 사람들이 데이터포인트들을 이해하는데 계산되며, 또한 나열되어있는 그래프들의 질에대해서도 쓰이는데, 이것은 관찰자들에게 있어 나열되어 있는 그래프들을 어떻게 바라볼지에 대해 영향을 줍니다. nullabor 페키치에서 쓰이는 그 거리계산 운율법/학은 첫 10개의 Turk 스터디에서 보이는 몇 종류의 그래프 종류를 다루고 있습니다. 그 종류에는 산점도(scaterplot), 면/면 상자그림(boxplot), 산점도에 올려져있는 회기 라인들, 그리고 색깔별 산점도 정도가 있습니다. 그래프 공간과 그래프 밀도를 나누는(binning) 일반적인 방법은 일반적으로 2차원적 화면으로사용합니다. 무수를 만드는법에 전제되는 것은 함수들이 무수의 그래프에서 측정한 거리의 실증적인 분포도를 계속해서 시뮬레이션을 할수있다는것에 둡니다. 이것은 데이터 그래프가 무수의 그래프에서 얼마나 떨어져있는지에 알려줄수 있습니다. 이것은 관찰자 들에 의해서 나열되있는 데이터 그래프들을 판별하는데에 있어 사용됩니다. reg_dist, bin_dist, uni_dist, box_dist, sep_dist 함수들은 한 그래프가 다른 그래프에 비해 얼마나 다른지를 측정하는 방법들입니다. 이 함수들은 이것은 우리가 계수적으로 알수있듯이 실질적인 데이터로 그려진 그래프가 무수로 그려진 그래프보다 다른지에 알수 있습니다. distmet and distplot 이 함수들은 데이터와 무수를 만드는 장치들의 거리 측정값들의 분포들을 계락적으로 계산할수 있으며, 나중에 나열될 실질적인 데이터 그래프와 무수의 데이터 그래프들의 값들이 보이게 그래프를 만들수 있습니다. 이렇게 함으로써 사람들이 쉽게 나열되있는 그래프들중에서 데이터 그래프를 찾을수잇게 조금이나마 도움을 주며 그리하여 사람들에게 아마존 Turks 실험에서 결과들을 세팅하고 분석하는데에있어 정리를하는데 도움을 줍니다http://www.public.iastate.edu/~hofmann/experiments.html/. library(nullabor) library(ggplot2) library(dplyr) 70.3.2 거리 운율학 nullabor 페케지에는 bin_dist, box_dist, reg_dist, sep_dist 그리고 uni_dist 이러한 5가지의 거리 운율학이 잇습니다. 이 각기다른 거리 운율학들은 데이터의 다른 성직들을 파악하게끔 구성되어 있습니다. uni_dist 는 한가지의 변수로 만들어진 데이터를 사용할때 이용며 다른 운율학들은 두가지변수일때도 다 사용가능합니다. 구간화된 거리는 어떠한 상황에서도 사용할수 잇는 일반적인 거리 계산법인것에 비해, 다른 거리 운율법들은 그래프에 있는 그래픽적인 요소들 (산점도 위에 올려져잇는 회귀나 무리지어 있는경우등) 을 발견하기위에 구성되어 잇습니다. 이러한 운율법들을 계산하기 위해서는 class 변수들이나 가간화에 사용할 숫자들이 제공되어야 합니다 70.3.3 단일변수 데이터에서의 거리 uni_dist 는 단일변수 데이타의 첫 4개의 중심적인 moments 들간의 유클리디안 거기를 측정하는데 사양되는 운율법입니다. 일반적인 사용법은 두개의 다른 데이터세트에서 그려진 두개의 히스토그램의 거리를 측정하는데 사용됩니다. uni_dist(rnorm(100), rpois(100, 2)) ## [1] 1.953109 70.3.4 회귀 매개변수들의 거리 reg_dist 한 그래프와 다른 그래프로 그려진 모델의 회귀 매개변수들의 유클리디언 거리들을 계산하는 운율법입니다. 회귀 라인이 산점도 위에 그려져잇는 상황에서 이 우율법을 사용하는것을 추천드립니다. with(mtcars, reg_dist(data.frame(wt, mpg), data.frame(sample(wt), mpg))) ## [1] 0.3877867 70.3.5 박스플랏에서의 거리 box_dist 는 양옆에 잇는 두가지 다른 레벨의 박스플랏들이 잇을때 사용되는 거리 운율법입니다. 첫 사분위수, 중간값, 3번째의 사분위수는 가각의 박스들에 계산되며 2개의 박스들에서는 이것들의 절대적인 값이 계산됩니다. box_dist는 두개의 그래프들간의 절대적인 길이를 유클리디언 거리계산법으로 계산합니다. 이 박스플랏 거리는 양옆으로 구성된 박스플랏이 두가지의 레벨을 가지고 있는 변수의 분포들을 구분하는 상황에서 사용됩니다. with(mtcars, box_dist(data.frame(as.factor(am), mpg), data.frame(as.factor(sample(am)), mpg))) ## [1] 13.04042 70.3.6 구분된 상황에서의 거리 sep_dist는 뭉쳐져잇는 데이터 포인트들이 구분되어잇을때 사용되는 거리 운율법입니다. 뭉쳐잇는 데이터들중에서의 구분은 한 결집에서의 점에서의 거리와 다른 결집에서의 점과의 최소한의 거리로 정의된다. 한가지의 데이터셋에서의 결집들의 거리는 계산되며. 유클리디언 거리는 주어진 데이터셋과 다른 데이터셋의 구분 거리를 사용해 계산된다. 그 데이터셋에서의 결집의 숫자는 미리 제공되어야하며, 제공이 안되어있다면, 결집의 계급을 나누는 방법을 통해 결집들을 얻어낸다. with(mtcars, sep_dist(data.frame(wt, mpg, as.numeric(as.factor(mtcars$cyl))), data.frame(sample(wt), mpg, as.numeric(as.factor(mtcars$cyl))), nclust = 3)) ## [1] 0.4067679 70.3.7 구간화 거리 bin_dist 어느 데이터셋에서의 상황에서도 사용되는 일반화적인 거리 측정법이다. 두가지 변수로 이루어진 데이터셋이라면, X와 Y의 변수들을 p와 q구간들로 나누어져 pq 셀을 얻을수 있다. 한가지 셀에 들어갈 포인트들ㅇ의 숫자는 주어진 데이터셋을 통해 계산된다. bin_dist 두개의 데이터 셋들에서 두 데이터간의 셀 카운트들을 유클리디언 거리르 계산한다. p 와 q의 값들은 The values of p and q 인수로 제공되어야 한다. with(mtcars, bin_dist(data.frame(wt, mpg), data.frame(sample(wt), mpg), lineup.dat = NULL, X.bin = 5, Y.bin = 5)) ## [1] 10.29563 70.3.8 정렬에서의 그래프들간의 평균 거리 계산 정렬되어잇는 그래프들에서 실질적인 그래프가 다른 무수의 그래프와 구별되는것은 흥미롭습니다. 이것의 구별하는데에 있어 실질적인 그래프와 무수의 그래프들의 거리들은 계산되며 이 거리들의 평균값이 계산됩니다. 비슷하게, 각각의 무수의 그래프에서, 하나의 무수의 그래프와 나머지 무수의 그래프들간의 거리가 계산되며, 각각의 무수의 그래프들의 평균값을 구하는데 이 평균값이 사용됩니다. calc_mean_dist은 정렬(나열) 되어잇는 각각의 그래프들의 평균 계산값들을 계산합니다. 만약 실질적인 그래프의 평균값이 무수의 그래프들의 평균 거리값보다 클때, 이 정렬은 쉽다고 여겨집니다. 또 만얀 무수의 그래프들중 하나가 실질적인 그래프보다 평균 거리값이 크면, 이 정렬은 어렵다고 여겨집니다. calc_mean_dist(lineup(null_permute(&#39;mpg&#39;), mtcars, pos = 10), var = c(&#39;mpg&#39;, &#39;wt&#39;), met = &#39;reg_dist&#39;, pos = 10) ## # A tibble: 20 x 2 ## plotno mean.dist ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0946 ## 2 2 0.0808 ## 3 3 0.0492 ## 4 4 0.0459 ## 5 5 0.0925 ## 6 6 0.0459 ## 7 7 0.0475 ## 8 8 0.0879 ## 9 9 0.0478 ## 10 10 0.868 ## 11 11 0.0772 ## 12 12 0.0977 ## 13 13 0.0479 ## 14 14 0.0998 ## 15 15 0.128 ## 16 16 0.112 ## 17 17 0.0808 ## 18 18 0.367 ## 19 19 0.0513 ## 20 20 0.0887 70.3.9 여러가지의 정렬들의 차이 측정법 정렬에서의 각각의 그래프들의 평균 거리값들은 calc_mean_dist 함수에 의해서 계산할수 있다. calc_diff 함수는 실질적인 그래프와 허수의 그래프들의 최대치의 평균 거리 계산값의 차이점을 계산한다. calc_diff(lineup(null_permute(&#39;mpg&#39;), mtcars, pos = 10), var = c(&#39;mpg&#39;, &#39;wt&#39;), met = &#39;reg_dist&#39;, dist.arg = NULL, pos = 10) ## [1] 0.5452839 70.3.10 최적의 구간화 수 구간화 거리는 구간화 수의 큰 영향을 받는다. 구간화의 수는 사용자에 의해 제공되며 이는 주관적인 부분 이다. 그러기에 최적의 구간화 수를 구하는 방법을 알아내는 것이 중요하다. opt_diff 함수는 최적의 구간화 수를 x와 y방향으로 알아내는 데 사용된다. 구간화 거리는 x와y 방향의 다양한 조합을 통해 계산되고 calc_diff 함수를 통해 각 조합의 차를 계산 한다. 조합 중에서 차가 최고값인 조합을 사용하여야 한다. opt.diff &lt;- opt_bin_diff(lineup(null_permute(&#39;mpg&#39;), mtcars, pos = 10), var = c(&#39;mpg&#39;, &#39;wt&#39;), 2, 4, 2, 4, pos = 10, plot = TRUE) opt.diff$p 70.3.11 거리 운율법의 분포도 정렬의 질을 측정하기는 흥미롭습니다. 하지만 몇개의 정렬들을 미교해보는것또한 중요합니다. The distmet 함수는 실질적인 그래프의 평균 거리 와 무수들의 평균거리값을 전제한 거리 운율학들의 경험적 분포들을 제공하는 함수입니다. 정렬된 데이터와 무수들을 만드는 장치, 그리고 거리 운율학의 선택이 선 제공되어야 합니다. 사용자들은 그들이 선택한 운율법들을 선택할수 있습니다. 만약 거리 운율법이 추가적인 인수가 요구될땐, 그것들도 제공 되어야 합니다. lineup.dat &lt;- lineup(null_permute(&#39;mpg&#39;), mtcars, pos = 10) qplot(mpg, wt, data = lineup.dat, geom = &#39;point&#39;) + facet_wrap(~ .sample) 실질적인 그래프의 위치를 얻기위해 lineup.dat의 값의 복사, 붙이기 #decrypt(&#39;...&#39;) #[1] &#39;True data in position 10&#39; # Use pos = 10 dist.vals &lt;- distmet(lineup.dat, var = c(&#39;mpg&#39;, &#39;wt&#39;),&#39;reg_dist&#39;, null_permute(&#39;mpg&#39;), pos = 10, repl = 100, dist.arg = NULL) head(dist.vals$lineup) ## # A tibble: 6 x 2 ## plotno mean.dist ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0326 ## 2 2 0.0777 ## 3 3 0.0328 ## 4 4 0.0321 ## 5 5 0.108 ## 6 6 0.0301 dist.vals$diff ## [1] 0.6767334 head(dist.vals$closest) ## [1] 5 19 18 13 15 head(dist.vals$null_values) ## [1] 0.15415695 0.12378307 0.02839558 0.09112972 0.35517664 0.02878199 dist.vals$pos ## [1] 10 dist.vals &lt;- distmet(lineup.dat, var = c(&#39;mpg&#39;, &#39;wt&#39;),&#39;bin_dist&#39;, null_permute(&#39;mpg&#39;), pos = 10, repl = 100, dist.arg = list(lineup.dat = lineup.dat, X.bin = 5, Y.bin = 5)) 70.3.12 거리 운율법의 경험적 분포도를 그리기 distplot함수는 distmet 함수의 아우풋을 가지고 거리 운율법의 경험적 분포돌을 그리게 합니다. 분포도는 회색, 실질적인 그래프의 거리는 주황색, 그리고 무수의 그래픋들의 거리들은 검은색으로 표기됩니다. distplot(dist.vals) 70.3.13 참조 Buja, A., Cook, D., Hofmann, H., Lawrence, M., Lee, E.-K., Swayne, D. F, Wickham, H. (2009) Statistical Inference for Exploratory Data Analysis and Model Diagnostics, Royal Society Philosophical Transactions A, 367:4361–4383. Wickham, H., Cook, D., Hofmann, H. and Buja, A. (2010) Graphical Inference for Infovis, IEEE Transactions on Visualization and Computer Graphics, 16(6):973–979, http://doi.ieeecomputersociety.org/10.1109/TVCG.2010.161. Best paper award. Hofmann, H., Follett, L., Majumder, M. and Cook, D. (2012) Graphical Tests for Power Comparison of Competing Designs, IEEE Transactions on Visualization and Computer Graphics, 18(12):2441–2448, http://doi.ieeecomputersociety.org/10.1109/TVCG.2012.230. Majumder, M., Hofmann, H. and Cook, D. (2013) Validation of Visual Statistical Inference, Applied to Linear Models, Journal of the American Statistical Association, 108(503):942–956. Featured Article http://amstat.tandfonline.com/doi/pdf/10.1080/01621459.2013.808157. Roy Chowdhury, N., Cook, D., Hofmann, H., Majumder, M., Lee, E. K., &amp; Toth, A. L. (2014). Using visual statistical inference to better understand random class separations in high dimension, low sample size data. Computational Statistics, 30(2), 293-316. 10.1007/s00180-014-0534-x Majumder, M. and Hofmann, H. and Cook, D. (2014) Human Factors Influencing Visual Statistical Inference, http://arxiv.org/abs/1408.1974. Roy Chowdhury, N. and Cook, D. and Hofmann, H. and Majumder, M. and Zhao, Y. (2014) Utilizing Distance Metrics on Lineups to Examine What People Read From Data Plots, http://arxiv.org/abs/1408.1889. "],
["hex-sticker.html", "Chapter 71 Hex Sticker", " Chapter 71 Hex Sticker Priyadharshini Rajbabu and Suman Tripathy Our group was tasked with designing a hex sticker for the EDAV course, under the advisement of Professor Robbins. We were initially provided with a folder of various logos used on the edav.info webpage. They can be found at this link https://github.com/jtr13/EDAV/tree/master/images/icons. We first considered using the hexSticker package in R to create a hex sticker design, but ran into issues with image quality on the sticker and also felt there were limited resources for design with this package. A better alternative was to use Adobe Photoshop to design the sticker with the help of a blank psd template from Sticker Mule, to set up the proper sizing and to allow for easy printing. Some things we took into consideration in detail while designing the sticker were: Color scheme of the sticker: Decided to stick to the shade of blue widely used on edav.info webpage. After much experimentation, we converted the icons and logos to an all-black theme, which better complemented the blue. Cropping the edav.info logo: We cut out the “.info” half of the initial logo for this sticker. The technique to doing this in photoshop was to erase this half and set it to a transparent background, allowing the blue background to take its place. Ordering of the three elements of the sticker: We experimented with quite a few variations of the sticker to see what layout/ordering of the edav icon, logo, and “fall 2019” title would be most compatible. The sticker has been saved in a photoshop (psd) file, as that is where we designed it. PSD FILE -&gt; https://github.com/prajbabu/edavhex/blob/master/edavhexsticker.psd For readability, and for those who don’t have Adobe Photoshop installed, the file has also been saved as a: PDF -&gt; https://github.com/prajbabu/edavhex/blob/master/edavhexstickerFINAL.pdf PNG -&gt; https://github.com/prajbabu/edavhex/blob/master/hexsticker_image.png. If hyperlinks don’t work, please copy link into a browser.* "],
["midsemester-review.html", "Chapter 72 Midsemester Review 72.1 Lecuture 1: Introduction 72.2 Lecture 2: Histograms 72.3 Lecture 3: Grammar of Graphics 72.4 Lecture 4: Common ggplot2 Problems 72.5 Lecture 5: Boxplots &amp; Continuous Variables 72.6 Lecture 6: Rounding Normal (Continuous Variables Wrap-up) 72.7 Lecture 7: Graphical Perception 72.8 Lecture 8: Categorical Variables (Textbook: Chapter 04) 72.9 Lecture 9: Web Scraping &amp; rvest package 72.10 Lecture 10: Scatterplots - 2 Continuous Variables (Textbook: Chapter 05) 72.11 Lecture 11: Parallel Coordinates 72.12 Lecture 12: Interactive Parallel Coordinates (Htmlwidget: parcoords) 72.13 Lecture 13: Git - Workflow 72.14 Lecture 14: Multivariate Categorical Variables (e.g. Mosaic Plots) 72.15 Lecture 15: Transforming Data 72.16 Lecture 16: Likert 72.17 Lecture 17: Git - Branching 72.18 Lecturee 18: Simpson’s Paradox 72.19 Lecture 19: Heatmaps (Textbook: Chapter 8) 72.20 Lecture 20: Time Series (Textbook: Chapter 11)", " Chapter 72 Midsemester Review Tiffany Zhu tz2196 and Olivia Wang yw3324 72.1 Lecuture 1: Introduction Exploratory Data Aanlysis (EDA) detecting patterns, finding outliers, making comparisons, identifying clusters Data science pipeline Exploration vs visualization Exploratory vs explanatory Not mutually exclusive 72.2 Lecture 2: Histograms x &lt;- rnorm(N) df &lt;- data.frame(x) ggplot(df, aes(x)) + geom_histogram() + ggtitle(&quot;Histogram of Random Data&quot;) Summary: Primary tool for continuous variables Different types: Count (frequency)/relative frequency/cumulative frequency/density Boundaries default in R: right closed (e.g. (55,60]) Bin width Sometimes multimodality might disappear with change in binwidth using non-integer binwidth can conceal useful empirical info unequal bin width should be avoided Relative frequency histograms: Area under curve is 1 \\(Relative Frequency = \\frac{count}{total}\\) \\(Density = \\frac{Relative Frequency}{BinWidth}\\) Uneven binwidth histogram: Density histogram should be used Uneven binwidth histogram: Density histogram should be used Cumulative frequency: e.g. how many ppl can have weight less than X Good for: emphasizing features of the raw data Bad for: density estimates 72.3 Lecture 3: Grammar of Graphics Why use a grammar? Becuase More flexible, more room for growth Building blocks Layers (many) geom → aesthetic mapping, stat, position coord (1) facet (1) scales (1 per mapping) x → scale_x_date(), y → scale_y_continuous(), color → scale_color_manual() +theme (1) Example mnemonic to remember order: Geometry Class Feels So Lame Today 72.4 Lecture 4: Common ggplot2 Problems aes() not needed for constant values correct example – color varies with z ggplot(df, aes(x, y, color = z)) + geom_point() if missing legend, data is not tidy (use gather()) 72.5 Lecture 5: Boxplots &amp; Continuous Variables 72.5.0.1 Boxplot Overview * Boxplot components + minimum + quartile 1 (lower hinge) + quartile 2 (median) + quartile 3 (upper hinge) + maximum * interquartile range (hinge spread) + \\(IQR = Q3 – Q1\\) * Can easily see outliers + Point is outlier if it falls outside of fences + Upper Fence: 1.5hinge + upper hinge (Q3) + 1.5IQR + Q3 + Lower Fence: 1.5hinge - lower hinge (Q1) + 1.5IQR - Q1 * Best for comparing the distribution of a variable across the groups (textbook-ch 3) * If multiple boxplots, should reorder by something (e.g. median, max value, standard deviation) 72.5.0.2 Comparing Histograms to Boxplots par(mfrow=c(3,2)) hist(x, main=&quot;Histogram of Normal Data&quot;) boxplot(x, horizontal=T, main=&#39;Boxplot of Normal Data&#39;) x &lt;- c(0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 5, 7, 10) hist(x, main=&quot;Histogram of Right Skewed Data&quot;) boxplot(x, horizontal=T, main=&#39;Boxplot of Right Skewed Data&#39;) x &lt;- c(0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 5, 7, 8, 9, 9, 9, 10, 10) hist(x, main=&quot;Histogram of Bimodal Data&quot;) boxplot(x, horizontal=T, main=&#39;Boxplot of Bimodal Data&#39;) violin plots density curves–&gt; rotated bandwidth really matters box plot vs. violin: skinny quartile: lots of data fat one: little data Ridgeline plots: density curve plots shifted - good for multimodality 72.6 Lecture 6: Rounding Normal (Continuous Variables Wrap-up) You can tell if the data is rounded or not by: changing the bin width (to see gaps) Stem and leaf plot Q-Q plot (quantile-quantile) If distribution is normal, you will get a straight line x &lt;- rnorm(N, 50, 10) qqnorm(x) qqline(x, col = &quot;red&quot;) Other ways to test normality is by: Putting density curve &amp; normal curve on top of the histogram Shapiro-Wilk test Null Hypothesis: Data is normally distributed Alternative Hypothesis: Data is NOT normally distributed Check the p-value 72.7 Lecture 7: Graphical Perception Gets harder and harder to perceive: Position along a common scale Position along identical, nonaligned scales Length Angle / Slope Area Volume Color hue / Color saturation / Density 72.8 Lecture 8: Categorical Variables (Textbook: Chapter 04) Summary: https://github.com/jtr13/codehelp/blob/master/R/reorder.md Hard to work with Not a lot of options Choice about which categories to display Choice of the order of categories Data cleaning takes more time Types of data Nominal – no fixed category order -&gt; order by frequency Bar charts Order by frequency: Sort from highest to lowest count (left to right or top to bottom) Natural order Ordinal – fixed category order -&gt; use natural order Bar chart Sort in logical order of categories can’t change binwidth Discrete – small # of possibilities Cleveland dot plot Not always clearcut: nominal vs ordinal, ordinal vs discrete, Sometimes numbers = nominal, not discrete Bar charts how to order: if ordinal: order by natural order If nominal: order by frequency count If data is binned, use geom_col, If data unbinned, use geom_bar unbinned, ordinal, correct level order geom_bar unbinned, ordinal, levels out of order geom_bar, fct_relevel binned, ordinal, correct level order geom_col binned, ordinal, levels out of order geom_col, fct_inorder unbinned, nominal geom_bar, fct_infreq binned, nominal geom_col, fct_reorder example: library(dplyr) colors &lt;- as.data.frame(HairEyeColor) # just female hair color, using dplyr colors_female_hair &lt;- colors %&gt;% filter(Sex == &quot;Female&quot;) %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) 72.9 Lecture 9: Web Scraping &amp; rvest package Web scraping Should be last resort Better to use an API (httr package) or R package Investigate legal issues, think about ethical questions, limit bandwidth use 72.10 Lecture 10: Scatterplots - 2 Continuous Variables (Textbook: Chapter 05) ggplot(cars, aes(speed, dist)) + geom_point() + ggtitle(&#39;Scatter Plot of Cars Data&#39;) Features visible in scatterplots Correlation (Correlation != Causation) =&gt; dependent variable on the y-axis Associations Outliers Clusters Gaps (where particular combinations of values do not occur) Barriers (Boundaries) (where some combinations of values may not be possible. e.g. having more years of experience than their age) Conditional Relationships (different relationships for different intervals of x) Strategies Use techniques to deal with over plotting open circles Alpha blending Plotly (interactive) Don’t plot all points temporarily remove outliers, start off with sampling, subset data on the basis of some variable, such as “freshmen” Heatmaps bin counts or density estimates Density contour lines to see if there’s a clear cluster or not Combination of above Multiple variables: scatterplot matrices 72.11 Lecture 11: Parallel Coordinates Used for multiple (more than 2) continuous variables Where there are a lot of repeat data, splines make PC plots more useful Interpreting Parallel Coordinates Twisting means that the variables are negatively correlated Straigh parallel lines means that the variables are strongly positively correlated Otherwise, variables are not correlated library(datasets) library(GGally) ggparcoord(iris, columns=1:4, title = &quot;Parallel coordinate plot for Iris flowers&quot;) 72.12 Lecture 12: Interactive Parallel Coordinates (Htmlwidget: parcoords) Interactive PC good for recognizing outliers Parallel coordinate plots need to be interactive to be fully effective Alpha blending, color, filter out (certain data) library(parcoords) parcoords(mtcars, brushMode = &quot;1d-axes&quot;, rownames=F, reorderable = TRUE, withD3 = TRUE) 72.13 Lecture 13: Git - Workflow Create local clone of your repository bc hard to write code on GitHub Simple workflow From local: 1. pull, 2. write code, 3. commit/push 72.14 Lecture 14: Multivariate Categorical Variables (e.g. Mosaic Plots) Categorical data It has categories Data formats: Cases COUNTS (tidy data with Freq Column) Contingency or pivot table Multivariate Categorical FREQUENCY Bar charts Grouped vs. stacked Stacked: interested in overall total Grouped: the rest The closer things are, the easier to compare Cleveland dot plot Two dots on the same horizontal level: for simplicity of comparison PROPORTION/ASSOCIATION Mosaic Plots Any filled rectangular plot (no white space) with consistent numbers of rows and columns, in which the area of each small rectangle is PROPORTIONAL to the FREQUENCY count for a UNIQUE combination of levels of the categorical variable displayed library(vcd) library(ucidata) mosaic(cp ~ exang, labeling = labeling_border(rot_labels = c(45, 0, 0, 0), abbreviate_labs = c(6)), main = &#39;Chest Pain vs Exercise Induced Angina&#39;, heart_disease_cl) Best practices Dependent variables is split LAST and split HORIZONTALLY Fill: set to dependent variable Other variables are split vertically Level of dependent variable is closest to the x-axis and darkest Mosaic pairs plot You are looking at 2 at a time Looking for strong linear relationships between variables Strongest: most staggered; we thus decide to zoom in on those variables Disadvantages: Labeling is a nightmare Fluctation Diagrams Shows same info as the mosaic plot Starts off with same sized squares Drawn in proportion to the one with the highest frequency count Everything thus is proportional All have the same aspect ratio Variables having no relationship to each other: boxes of varying shapes Useful for: When there are a lot of variables Mosaic vs. tree map Tree map: each box cannot be in more than one category (based on hierarchical data) Filled rectangular plot representing hierarchical data Chi Square Test of Independence Close to mosaic Tests how different variables are from one another We compare the observed to the expected (under the assumption of the null: assumes that the two variables are independent) Graph shows that there are no interactions Implementation (from textbook) Starts off with an empty rectangle that represents the whole dataset Taking the first variable and dividing the HORIZONTAL axis into sections PROPORTIONAL to the sizes of its categories. Each of the rectangles is then divided along its VERTICAL axis according to the sizes of the second variable categories In theory, you can continue to divide up the rectangles alternately horizontally and vertically for as many variables as you have. (however, having too many categories makes the plot messy) Works well with: Small number of categories Ordinal dependent variables As you can see cumulative patterns if they exist Alternative: Using the pairs function in vcd Produces a matrix display with the bar charts of the individual variables down the diagonal As well as 6 mosaic plots both above and below the diagonal Labeling is kept to a minimum While efficient, plot is hard to read 72.15 Lecture 15: Transforming Data General naming conventions (e.g. for files) – should be machine readable, human readable Data frames – avoid spaces, punctuation, special characters Factor levels – descriptive but not too long when recoding factor levels: leave a papertrail, keep original columns if can Data visualization – human readability, brevity If possible, don’t use column names that have to be altered when plotting Transposing data frames t() will convert numerical to character if there are non numericals in the column works best if data frame has row names gather() then spread() transposing multiple columns mutate_all, mutate_if, mutate_at 72.16 Lecture 16: Likert Likert data is survey data with responses: strongly agree, agree, don’t know, dislike, strong dislike Plots to graph likert data: Stacked bar chart Diverging stacked bar chart - centered at the neutral data Can use HH::likert package 72.17 Lecture 17: Git - Branching pull create new branch, write code, commit/push, submit pull request (so can merge branch into master), merge pull request, delete branch locally 72.18 Lecturee 18: Simpson’s Paradox Different ways of splitting the mosaic plot/faceting the data can show different and sometimes opposite trend– as a result, it is important to note the proportions when comparing groups of different sizes Wikipedia: “Where a trend appears in several different groups of data but disappears/reverses when these groups are combined” Video - https://www.youtube.com/watch?v=ebEkn-BiW5k when looking at just cats or just humans, can see that treatment helps. but when looking at total (cat + human), seems like treatment doesn’t help. 72.19 Lecture 19: Heatmaps (Textbook: Chapter 8) It can show FREQUENCY counts (2 D histogram- with just an x and an y-axis) or value of a third variable (which you will have to manually insert, it will not be generated automatically) When cleaning dataset to be used for Heat maps: We need 1 column that lists all the variables Can be used for continuous or categorical data (both for axes and fill color) However: It is overused Based off of color (hard to decipher) –&gt; not our first choice Categorical axes Frequency count/ time Common use: gene expression Rows: genes Columns: samples COLOR: change in gene expression LEVEL Not a good option for Data with small numbers of categories (small n) Very hard to read (color makes it hard to see patterns) Continuous axes Location Eyetracking Geographic Data Implementation Geom_point() Geom_tile() –&gt; makes rectangles instead of dots Geom_raster() –&gt; similar to but faster than geom_tile() if one variable is categorical, use fill Heat map theme theme(axis.line = element_blank(), axis.ticks = element_blank()) coord_fixed –&gt; squares white borders with geom_tile rescale(value): transforms values to 0 to 1 rescaling: group_by key first and then rescale small sample size and not rescaled? Meaningless unless we look at proportions Can be rescaled so that whole box/column adds to 1 To test that your values are correct: add geom_text() Try changing color scheme to make things clearer Tips regarding heat maps (from textbook) Each case is represented by a row and each variable is represented by a column Individual cells are colored according to the case value relative to the other values in the COLUMN Either with a normal transformation to z score Or adjusted to a scale from min to max It is unwise to color according to all values in the dataset tas that highlights differences between different levels of variables instead of differences between individual cases. Clusters are very important Rather subjective tool Can be effective for particular structures in some datasets, but cannot be relied upon to produce good results in general. Additional related graphs Scatterplot Matrices Shows relationship and association Parallel coordinate plots Great for studying groups of cases Some of the features are already identified in the scatterplot matrix display, at least for those variables with adjacent axes. A lot of information on the distribution of individual variables Skewness Gaps Most effective if used interactively. 72.20 Lecture 20: Time Series (Textbook: Chapter 11) Definition: lines with time on the x axis usually for continuous variables, but could be for nominal (e.g. person’s state of health) or discrete variables main reasons for studying time series: understand patterns of the past to forecast the future dates and times have tricky properties =&gt; best to use package to deal with them Single time series (from textbook) Decisions to be made before plotting: Symbol – use point and/or lines? Scale – what scale should be used for x or y axis? What min to max level to use? Aspect ratio – the length of the y axis to the length of the horizontal time axis. Trend – should a trend estimate in form of a smoother be added to the display? Gaps – how should gaps be represented? Multiple series (from textbook) Related series for the same population Instead of drawing each series individually, can make a new dataset with 1. time values needed values of the different time series all in one variables a grouping variable of the time series labels Same series for different subgroups E.g. time series of the same variable for several countries, so a time series that can be analyzed together on the same scale Series with different scales To deal with different scales could Set value of first time in series to 100. Each value is divided by the first value and multiplied by 100. standardize all series by their respective means and standard deviations Loess smoother (non-parametric - does not assume anything about the underlying distribution) Uses geom_smooth() Increasing smoothing parameter =&gt; smoother =&gt; under fitting Underfitting = over smoothing Overfitting = under smoothing Trends to describe time series graph Cyclical trends – repeated trends facet by variable to look at cyclic pattern more Secular trends – overall trends Could use bars, they are better for individual values. But should ideally use lines. What if you want to observe the frequency of time series data? A simple answer: use geom_point() in addition to geom_line(). Gaps Add points to lines to show the frequency of the data A straight line is suspicious in the real world, it is usually because there’s missing values. It’s very difficult to deal with NAs when using time series Could just leave gaps by setting missing values to NA If don’t want gaps, remove NAs "],
["list-of-community-contribution.html", "Chapter 73 List of Community Contribution", " Chapter 73 List of Community Contribution Kevin Gao (wg2311) and Haibo Yu (hy2628) We have tried many different things from the list, including: 73.0.1 * A lighting talk in class Title: Experiments Tool and Visualization Description: The “Virtual Lab” refers to using software controlled experiments with internet participants to overcome many of the limitations of traditional lab experiments. To help researchers study more complex tasks and set up interactions that happen over longer periods of time or among larger numbers of people. This allows us to design behavioral experiments and visualize data that would have been very hard to do in the past. I would like to share some of the work and how we visualize things to make a difference. 73.0.2 * A cheatsheet About mapping grammar for ggplot2,pyplot,d3 73.0.3 * A series of tutorials About how to share your R work (4 ways): Publish and share your R plots, hosted free in github website (Quick, Simple and Free) Release R Package, through cran or github Deploy R model to Azure ML Studio (The Simplest Way) Deploy R model as Web Service (The Structured Way) 73.0.4 * A workshop - “ShareYouRWork” Give a walkthough demo on how to deploy R model as web service. (The Structed Way) For more information, please visit our github main repo here "]
]
